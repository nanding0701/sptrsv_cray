./AWPM_CombBLAS.hpp:5:#include "ApproxWeightPerfectMatching.h"
./AWPM_CombBLAS.hpp:11: * <pre>
./AWPM_CombBLAS.hpp:30: *        SuperLU's 2D process mesh.
./AWPM_CombBLAS.hpp:37: * </pre>
./AWPM_CombBLAS.hpp:47:    int    iam, p, procs;
./AWPM_CombBLAS.hpp:49:    procs = grid->nprow * grid->npcol;
./AWPM_CombBLAS.hpp:51:    if(grid->nprow != grid->npcol)
./AWPM_CombBLAS.hpp:53:        printf("AWPM only supports square process grid. Retuning without a permutation.\n");
./AWPM_CombBLAS.hpp:56:    std::vector< std::vector < std::tuple<int_t,int_t,double> > > data(procs);
./AWPM_CombBLAS.hpp:73:     COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
./AWPM_CombBLAS.hpp:104:    int * rdispls = new int[procs];
./AWPM_CombBLAS.hpp:106:    int * recvcnt = new int[procs];
./AWPM_CombBLAS.hpp:109:    for(int i=0; i<procs-1; ++i)
./CMakeLists.txt:22:# first: precision-independent files
./CMakeLists.txt:57:set_source_files_properties(superlu_timer.c PROPERTIES COMPILE_FLAGS -O0)
./CMakeLists.txt:162:        set_target_properties(${target} PROPERTIES
./CMakeLists.txt:164:                              VERSION ${PROJECT_VERSION}
./CMakeLists.txt:169:        set_target_properties(${target} PROPERTIES
./CMakeLists.txt:171:                              VERSION ${PROJECT_VERSION}
./CMakeLists.txt:177:target_compile_definitions(superlu_dist PRIVATE SUPERLU_DIST_EXPORTS)
./CMakeLists.txt:179:  set_target_properties(superlu_dist PROPERTIES
./CMakeLists.txt:194:# DESTINATION ${CMAKE_INSTALL_PREFIX}/include)
./Cnames.h:4:approvals from U.S. Dept. of Energy) 
./Cnames.h:14: * <pre>
./Cnames.h:18: * </pre>
./Cnames.h:83:#define f_get_CompRowLoc_Matrix          f_get_comprowloc_matrix_ 
./Cnames.h:84:#define f_set_CompRowLoc_Matrix          f_set_comprowloc_matrix_
./Cnames.h:98:#define f_dCreate_CompRowLoc_Mat_dist    f_dcreate_comprowloc_mat_dist_
./Cnames.h:99:#define f_zCreate_CompRowLoc_Mat_dist    f_zcreate_comprowloc_mat_dist_
./Cnames.h:100:#define f_Destroy_CompRowLoc_Mat_dist    f_destroy_comprowloc_mat_dist_
./Cnames.h:211:#define f_get_CompRowLoc_Matrix          F_GET_COMPROWLOC_MATRIX
./Cnames.h:212:#define f_set_CompRowLoc_Matrix          F_SET_COMPROWLOC_MATRIX
./Cnames.h:226:#define f_dCreate_CompRowLoc_Mat_dist    F_DCREATE_COMPROWLOC_MAT_DIST
./Cnames.h:227:#define f_zCreate_CompRowLoc_Mat_dist    F_ZCREATE_COMPROWLOC_MAT_DIST
./Cnames.h:228:#define f_Destroy_CompRowLoc_Mat_dist    F_DESTROY_COMPROWLOC_MAT_DIST
./Cnames.h:340:#define f_get_CompRowLoc_Matrix          f_get_comprowloc_matrix 
./Cnames.h:341:#define f_set_CompRowLoc_Matrix          f_set_comprowloc_matrix
./Cnames.h:355:#define f_dCreate_CompRowLoc_Mat_dist    f_dcreate_comprowloc_mat_dist
./Cnames.h:356:#define f_Destroy_CompRowLoc_Mat_dist    f_destroy_comprowloc_mat_dist
./Makefile:6:#       ALLAUX  -- Auxiliary routines called from all precisions
./Makefile:7:#       DSLUSRC -- Double precision real serial SuperLU routines
./Makefile:8:#       DPLUSRC -- Double precision real parallel SuperLU routines
./Makefile:9:#       ZSLUSRC -- Double precision complex serial SuperLU routines
./Makefile:10:#       ZPLUSRC -- Double precision complex parallel SuperLU routines
./Makefile:13:#  of the two precisions.  To create or add to the library, enter make
./Makefile:14:#  followed by one or more of the precisions desired.  Some examples:
./Makefile:19:#  without any arguments creates a library of all two precisions.
./Makefile:30:# Precision independent routines
./Makefile:57:# Routines for double precision parallel SuperLU
./Makefile:81:		printf "#define XSDK_INDEX_SIZE 64\n" >> superlu_dist_config.h
./Makefile:83:		printf "/* #define XSDK_INDEX_SIZE 64 */\n" >> superlu_dist_config.h
./Makefile:86:		printf "#define HAVE_LAPACK TRUE\n" >> superlu_dist_config.h
./Makefile:88:		printf "/* #define HAVE_LAPACK TRUE */\n" >> superlu_dist_config.h
./Makefile:91:		printf "#define HAVE_PARMETIS TRUE\n" >> superlu_dist_config.h
./Makefile:93:		printf "/* #define HAVE_PARMETIS TRUE */\n" >> superlu_dist_config.h
./Makefile:96:		printf "#define HAVE_COMBBLAS TRUE\n" >> superlu_dist_config.h
./Makefile:98:		printf "/* #define HAVE_COMBBLAS TRUE */\n" >> superlu_dist_config.h
./Makefile:100:	printf "#if (XSDK_INDEX_SIZE == 64)\n#define _LONGINT 1\n#endif\n" >> superlu_dist_config.h
./TreeBcast_slu.hpp:38:      protected:
./TreeBcast_slu.hpp:72:      protected:
./TreeBcast_slu.hpp:121:      protected:
./TreeBcast_slu.hpp:131:      protected:
./TreeBcast_slu.hpp:141:      protected:
./TreeBcast_slu_impl.hpp:179:                Int iProc = this->myDests_[idxRecv];
./TreeBcast_slu_impl.hpp:180: printf("I am %d, will send total %d ranks.---Now is rank %d.\n",dn_rank, this->myDests_.size(), iProc);
./TreeBcast_slu_impl.hpp:181:	offset = dn_rank*maxrecvsz*BCsendoffset[iProc]+1;
./TreeBcast_slu_impl.hpp:182:	MPI_Accumulate(locBuffer, msgSize, this->type_, iProc, offset, msgSize, this->type_, MPI_REPLACE, winl);		  
./TreeBcast_slu_impl.hpp:183:	MPI_Accumulate(&my_BCtasktail, 1, this->type_, iProc, 0, 1, this->type_, MPI_SUM, winl);		
./TreeBcast_slu_impl.hpp:184:        MPI_Win_flush(iProc, winl); 
./TreeBcast_slu_impl.hpp:185:        BCsendoffset[iProc] +=1;
./TreeBcast_slu_impl.hpp:186:	} // for (iProc)
./TreeBcast_slu_impl.hpp:196:          Int iProc = this->myDests_[idxRecv];
./TreeBcast_slu_impl.hpp:197: printf("I am %d, will send total %d ranks.---Now is rank %d.\n",dn_rank, this->myDests_.size(), iProc);
./TreeBcast_slu_impl.hpp:200:              iProc, this->tag_,this->comm_, &this->sendRequests_[idxRecv] );
./TreeBcast_slu_impl.hpp:205:			  // std::cout<<this->myRank_<<" FWD to "<<iProc<<" on tag "<<this->tag_<<std::endl;
./TreeBcast_slu_impl.hpp:206:        } // for (iProc)
./TreeBcast_slu_impl.hpp:214:        } // for (iProc)
./TreeBcast_slu_impl.hpp:278:      Int nprocs = 0;
./TreeBcast_slu_impl.hpp:279:      MPI_Comm_size(pComm, &nprocs);
./TreeBcast_slu_impl.hpp:281:      if(nprocs<=FTREE_LIMIT){
./TreeBcast_slu_impl.hpp:405:      Int prevRoot = ranks[0];
./TreeBcast_slu_impl.hpp:412:            this->myRoot_ = prevRoot;
./TreeBcast_slu_impl.hpp:437:            this->myRoot_ = prevRoot;
./TreeBcast_slu_impl.hpp:453:          prevRoot = curRoot;
./TreeInterface.cpp:15:	BcTree BcTree_Create(MPI_Comm comm, Int* ranks, Int rank_cnt, Int msgSize, double rseed, char precision){
./TreeInterface.cpp:17:		if(precision=='d'){
./TreeInterface.cpp:21:		if(precision=='z'){
./TreeInterface.cpp:27:	void BcTree_Destroy(BcTree Tree, char precision){
./TreeInterface.cpp:28:		if(precision=='d'){
./TreeInterface.cpp:32:		if(precision=='z'){
./TreeInterface.cpp:39:	void BcTree_SetTag(BcTree Tree, Int tag, char precision){
./TreeInterface.cpp:40:		if(precision=='d'){
./TreeInterface.cpp:44:		if(precision=='z'){
./TreeInterface.cpp:51:	yes_no_t BcTree_IsRoot(BcTree Tree, char precision){
./TreeInterface.cpp:52:		if(precision=='d'){
./TreeInterface.cpp:56:		if(precision=='z'){
./TreeInterface.cpp:64:	void BcTree_forwardMessageOneSide(BcTree Tree, void* localBuffer, Int msgSize, char precision){
./TreeInterface.cpp:65:		if(precision=='d'){
./TreeInterface.cpp:69:		if(precision=='z'){
./TreeInterface.cpp:75:	void BcTree_forwardMessageSimple(BcTree Tree, void* localBuffer, Int msgSize, char precision){
./TreeInterface.cpp:76:		if(precision=='d'){
./TreeInterface.cpp:80:		if(precision=='z'){
./TreeInterface.cpp:87:	void BcTree_waitSendRequest(BcTree Tree, char precision){
./TreeInterface.cpp:88:		if(precision=='d'){
./TreeInterface.cpp:92:		if(precision=='z'){
./TreeInterface.cpp:100:	void BcTree_allocateRequest(BcTree Tree, char precision){
./TreeInterface.cpp:101:		if(precision=='d'){
./TreeInterface.cpp:105:		if(precision=='z'){
./TreeInterface.cpp:111:	int BcTree_getDestCount(BcTree Tree, char precision){
./TreeInterface.cpp:112:		if(precision=='d'){
./TreeInterface.cpp:116:		if(precision=='z'){
./TreeInterface.cpp:122:	int BcTree_GetMsgSize(BcTree Tree, char precision){
./TreeInterface.cpp:123:		if(precision=='d'){
./TreeInterface.cpp:127:		if(precision=='z'){
./TreeInterface.cpp:179:	RdTree RdTree_Create(MPI_Comm comm, Int* ranks, Int rank_cnt, Int msgSize, double rseed, char precision){
./TreeInterface.cpp:181:		if(precision=='d'){
./TreeInterface.cpp:185:		if(precision=='z'){
./TreeInterface.cpp:191:	void RdTree_Destroy(RdTree Tree, char precision){
./TreeInterface.cpp:192:		if(precision=='d'){
./TreeInterface.cpp:196:		if(precision=='z'){
./TreeInterface.cpp:203:	void RdTree_SetTag(RdTree Tree, Int tag, char precision){
./TreeInterface.cpp:204:		if(precision=='d'){
./TreeInterface.cpp:208:		if(precision=='z'){
./TreeInterface.cpp:214:	int  RdTree_GetDestCount(RdTree Tree, char precision){
./TreeInterface.cpp:215:		if(precision=='d'){
./TreeInterface.cpp:219:		if(precision=='z'){
./TreeInterface.cpp:225:	int  RdTree_GetMsgSize(RdTree Tree, char precision){
./TreeInterface.cpp:226:		if(precision=='d'){
./TreeInterface.cpp:230:		if(precision=='z'){
./TreeInterface.cpp:238:	yes_no_t RdTree_IsRoot(RdTree Tree, char precision){
./TreeInterface.cpp:239:		if(precision=='d'){
./TreeInterface.cpp:243:		if(precision=='z'){
./TreeInterface.cpp:249:	void RdTree_forwardMessageOneSide(RdTree Tree, void* localBuffer, Int msgSize, char precision){
./TreeInterface.cpp:250:		if(precision=='d'){
./TreeInterface.cpp:254:		if(precision=='z'){
./TreeInterface.cpp:260:	void RdTree_forwardMessageSimple(RdTree Tree, void* localBuffer, Int msgSize, char precision){
./TreeInterface.cpp:261:		if(precision=='d'){
./TreeInterface.cpp:265:		if(precision=='z'){
./TreeInterface.cpp:270:	void RdTree_allocateRequest(RdTree Tree, char precision){
./TreeInterface.cpp:271:		if(precision=='d'){
./TreeInterface.cpp:275:		if(precision=='z'){
./TreeInterface.cpp:282:	void RdTree_waitSendRequest(RdTree Tree, char precision){
./TreeInterface.cpp:283:		if(precision=='d'){
./TreeInterface.cpp:287:		if(precision=='z'){
./TreeReduce_slu.hpp:22:      protected:
./TreeReduce_slu.hpp:56:protected:
./TreeReduce_slu.hpp:67:protected:
./TreeReduce_slu.hpp:83:protected:
./TreeReduce_slu_impl.hpp:50:		Int iProc = this->myRoot_;
./TreeReduce_slu_impl.hpp:51: printf("I am %d, will send to %d\n",dn_rank, iProc);
./TreeReduce_slu_impl.hpp:54:	offset = dn_rank*maxrecvsz*RDsendoffset[iProc]+1;
./TreeReduce_slu_impl.hpp:56:	MPI_Accumulate(locBuffer, msgSize, this->type_, iProc, offset, msgSize, this->type_, MPI_REPLACE, winl);		  
./TreeReduce_slu_impl.hpp:57:	MPI_Accumulate(&my_RDtasktail, 1, this->type_, iProc, 0, 1, this->type_, MPI_SUM, winl);		  
./TreeReduce_slu_impl.hpp:58:        MPI_Win_flush(iProc, winl); 
./TreeReduce_slu_impl.hpp:59:	RDsendoffset[iProc] +=1 ;
./TreeReduce_slu_impl.hpp:73:			  Int iProc = this->myRoot_;
./TreeReduce_slu_impl.hpp:74: printf("I am %d, will send to %d\n",dn_rank, iProc);
./TreeReduce_slu_impl.hpp:77:				  iProc, this->tag_,this->comm_, &this->sendRequests_[0] );
./TreeReduce_slu_impl.hpp:81:				  // std::cout<<this->myRank_<<" FWD to "<<iProc<<" on tag "<<this->tag_<<std::endl;
./TreeReduce_slu_impl.hpp:125:      Int nprocs = 0;
./TreeReduce_slu_impl.hpp:126:      MPI_Comm_size(pComm, &nprocs);
./TreeReduce_slu_impl.hpp:128:      if(nprocs<=FTREE_LIMIT){
./TreeReduce_slu_impl.hpp:266:      Int prevRoot = ranks[0];
./TreeReduce_slu_impl.hpp:273:            this->myRoot_ = prevRoot;
./TreeReduce_slu_impl.hpp:298:            this->myRoot_ = prevRoot;
./TreeReduce_slu_impl.hpp:314:          prevRoot = curRoot;
./c2cpp_GetAWPM.cpp:4:approvals from U.S. Dept. of Energy) 
./c2cpp_GetAWPM.cpp:13: * \brief Get approximate weight perfect matching (AWPM).
./c2cpp_GetAWPM.cpp:15: * <pre>
./c2cpp_GetAWPM.cpp:18: * April 1, 2018
./c2cpp_GetAWPM.cpp:19: * </pre>
./c2cpp_GetAWPM.cpp:27: * <pre>
./c2cpp_GetAWPM.cpp:31: * Get approximate weight perfect matching (AWPM).
./c2cpp_GetAWPM.cpp:48: *        SuperLU's 2D process mesh.
./c2cpp_GetAWPM.cpp:55: * </pre>
./colamd.c:4:approvals from U.S. Dept. of Energy) 
./colamd.c:14: <pre>
./colamd.c:20:    colamd:  an approximate minimum degree column ordering algorithm,
./colamd.c:23:	linear programming problems, and other related problems.
./colamd.c:25:    symamd:  an approximate minimum degree ordering algorithm for Cholesky
./colamd.c:32:	than A'A.  This also provides a good ordering for sparse partial
./colamd.c:33:	pivoting methods, P(AQ) = LU, where Q is computed prior to numerical
./colamd.c:69:	THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY
./colamd.c:70:	EXPRESSED OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
./colamd.c:73:	this program, provided that the Copyright, this License, and the
./colamd.c:115:	    expressions as arguments to COLAMD_RECOMMENDED.  Not needed for
./colamd.c:147:		entries are removed prior to ordering.  Columns with more than
./colamd.c:148:		(knobs [COLAMD_DENSE_COL] * n_row) entries are removed prior to
./colamd.c:153:		entries are removed prior to ordering, and placed last in the
./colamd.c:160:		be properly set to their defaults by the future version of
./colamd.c:213:		or equivalently as a C preprocessor macro: 
./colamd.c:231:		duplicate row indices may be be present.  However, colamd will
./colamd.c:271:		Colamd returns FALSE if stats is not present.
./colamd.c:416:		row indices may be present.  However, symamd will run faster
./colamd.c:456:		Symamd returns FALSE if stats is not present.
./colamd.c:536:	    	A pointer to a function providing memory allocation.  The
./colamd.c:561:	    Prints the error status and statistics recorded in the stats
./colamd.c:581:	    Prints the error status and statistics recorded in the stats
./colamd.c:589: </pre>
./colamd.c:651:/* Routines are either PUBLIC (user-callable) or PRIVATE (not user-callable) */
./colamd.c:653:#define PRIVATE static
./colamd.c:681:#define DEAD_PRINCIPAL		(-1)
./colamd.c:682:#define DEAD_NON_PRINCIPAL	(-2)
./colamd.c:690:#define COL_IS_DEAD_PRINCIPAL(c)	(Col [c].start == DEAD_PRINCIPAL)
./colamd.c:692:#define KILL_PRINCIPAL_COL(c)		{ Col [c].start = DEAD_PRINCIPAL ; }
./colamd.c:693:#define KILL_NON_PRINCIPAL_COL(c)	{ Col [c].start = DEAD_NON_PRINCIPAL ; }
./colamd.c:701:/* use mexPrintf in a MATLAB mexFunction, for debugging and statistics output */
./colamd.c:702:#define PRINTF mexPrintf
./colamd.c:709:/* Use printf in standard C environment, for debugging and statistics output. */
./colamd.c:712:#define PRINTF printf
./colamd.c:720:/* === Prototypes of PRIVATE routines ======================================= */
./colamd.c:723:PRIVATE int init_rows_cols
./colamd.c:734:PRIVATE void init_scoring
./colamd.c:748:PRIVATE int find_ordering
./colamd.c:762:PRIVATE void order_children
./colamd.c:769:PRIVATE void detect_super_cols
./colamd.c:784:PRIVATE int garbage_collection
./colamd.c:794:PRIVATE int clear_mark
./colamd.c:800:PRIVATE void print_report
./colamd.c:807:/* === Debugging prototypes and definitions ================================= */
./colamd.c:813:/* present when debugging */
./colamd.c:815:PRIVATE int colamd_debug ;	/* debug print level */
./colamd.c:817:#define DEBUG0(params) { (void) PRINTF params ; }
./colamd.c:818:#define DEBUG1(params) { if (colamd_debug >= 1) (void) PRINTF params ; }
./colamd.c:819:#define DEBUG2(params) { if (colamd_debug >= 2) (void) PRINTF params ; }
./colamd.c:820:#define DEBUG3(params) { if (colamd_debug >= 3) (void) PRINTF params ; }
./colamd.c:821:#define DEBUG4(params) { if (colamd_debug >= 4) (void) PRINTF params ; }
./colamd.c:824:#define ASSERT(expression) (mxAssert ((expression), ""))
./colamd.c:826:#define ASSERT(expression) (assert (expression))
./colamd.c:829:PRIVATE void colamd_get_debug	/* gets the debug print level from getenv */
./colamd.c:834:PRIVATE void debug_deg_lists
./colamd.c:846:PRIVATE void debug_mark
./colamd.c:854:PRIVATE void debug_matrix
./colamd.c:863:PRIVATE void debug_structures
./colamd.c:883:#define ASSERT(expression) ((void) 0)
./colamd.c:902:    value has been determined to provide good balance between the number of
./colamd.c:931:			prior to ordering in colamd.  Rows and columns with
./colamd.c:932:			knobs[0]*n_col entries or more are removed prior to
./colamd.c:937:			prior to ordering in colamd, and placed last in the
./colamd.c:1017:	DEBUG0 (("symamd: stats not present\n")) ;
./colamd.c:1030:    	stats [COLAMD_STATUS] = COLAMD_ERROR_A_not_present ;
./colamd.c:1031:	DEBUG0 (("symamd: A not present\n")) ;
./colamd.c:1035:    if (!p)		/* p is not present */
./colamd.c:1037:	stats [COLAMD_STATUS] = COLAMD_ERROR_p_not_present ;
./colamd.c:1038:	DEBUG0 (("symamd: p not present\n")) ;
./colamd.c:1312:    providing a permutation Q such that the Cholesky factorization
./colamd.c:1352:	DEBUG0 (("colamd: stats not present\n")) ;
./colamd.c:1363:    if (!A)		/* A is not present */
./colamd.c:1365:	stats [COLAMD_STATUS] = COLAMD_ERROR_A_not_present ;
./colamd.c:1366:	DEBUG0 (("colamd: A not present\n")) ;
./colamd.c:1370:    if (!p)		/* p is not present */
./colamd.c:1372:	stats [COLAMD_STATUS] = COLAMD_ERROR_p_not_present ;
./colamd.c:1373:	DEBUG0 (("colamd: p not present\n")) ;
./colamd.c:1457:    /* === Order the non-principal columns ================================== */
./colamd.c:1480:    print_report ("colamd", stats) ;
./colamd.c:1493:    print_report ("symamd", stats) ;
./colamd.c:1518:PRIVATE int init_rows_cols	/* returns TRUE if OK, or FALSE otherwise */
./colamd.c:1539:    int last_row ;		/* previous row */
./colamd.c:1560:	Col [col].shared3.prev = EMPTY ;
./colamd.c:1721:	    /* note that the lengths here are for pruned columns, i.e. */
./colamd.c:1755:PRIVATE void init_scoring
./colamd.c:1779:    int col_length ;		/* length of pruned column */
./colamd.c:1805:    /* factorization can proceed as far as possible. */
./colamd.c:1813:	    KILL_PRINCIPAL_COL (c) ;
./colamd.c:1840:	    KILL_PRINCIPAL_COL (c) ;
./colamd.c:1870:    /* pruned in the code below. */
./colamd.c:1900:	/* determine pruned column length */
./colamd.c:1908:	    KILL_PRINCIPAL_COL (c) ;
./colamd.c:1947:	/* only add principal columns to degree lists */
./colamd.c:1963:	    /* now add this column to dList at proper score location */
./colamd.c:1965:	    Col [c].shared3.prev = EMPTY ;
./colamd.c:1969:	    /* previous pointer to this new column */
./colamd.c:1972:		Col [next_col].shared3.prev = c ;
./colamd.c:1987:    DEBUG1 (("colamd: Live cols %d out of %d, non-princ: %d\n",
./colamd.c:2006:    Order the principal columns of the supercolumn form of the matrix
./colamd.c:2007:    (no supercolumns on input).  Uses a minimum approximate column minimum
./colamd.c:2011:PRIVATE int find_ordering	/* return the number of garbage collections */
./colamd.c:2056:    int pivot_col_thickness ;	/* number of columns represented by pivot col */
./colamd.c:2057:    int prev_col ;		/* Used by Dlist operations. */
./colamd.c:2119:	    Col [next_col].shared3.prev = EMPTY ;
./colamd.c:2237:	/* === Approximate degree computation =============================== */
./colamd.c:2239:	/* Here begins the computation of the approximate degree.  The column */
./colamd.c:2243:	/* excluded from the column score (we thus use an approximate */
./colamd.c:2247:	/* add them up) is proportional to the size of the data structure */
./colamd.c:2250:	/* is proportional to the size of that column (where size, in this */
./colamd.c:2280:	    prev_col = Col [col].shared3.prev ;
./colamd.c:2285:	    if (prev_col == EMPTY)
./colamd.c:2291:		Col [prev_col].shared4.degree_next = next_col ;
./colamd.c:2295:		Col [next_col].shared3.prev = prev_col ;
./colamd.c:2394:		KILL_PRINCIPAL_COL (col) ;
./colamd.c:2404:		/* === Prepare for supercolumn detection ==================== */
./colamd.c:2406:		DEBUG4 (("Preparing supercol detection for Col: %d.\n", col)) ;
./colamd.c:2420:		    /* degree list "hash" is non-empty, use prev (shared3) of */
./colamd.c:2439:	/* The approximate external column degree is now computed.  */
./colamd.c:2455:	KILL_PRINCIPAL_COL (pivot_col) ;
./colamd.c:2521:	    Col [col].shared3.prev = EMPTY ;
./colamd.c:2524:		Col [next_col].shared3.prev = col ;
./colamd.c:2552:    /* === All principal columns have now been ordered ====================== */
./colamd.c:2563:    The find_ordering routine has ordered all of the principal columns (the
./colamd.c:2564:    representatives of the supercolumns).  The non-principal columns have not
./colamd.c:2575:PRIVATE void order_children
./colamd.c:2591:    /* === Order each non-principal column ================================== */
./colamd.c:2595:	/* find an un-ordered non-principal column */
./colamd.c:2597:	if (!COL_IS_DEAD_PRINCIPAL (i) && Col [i].shared2.order == EMPTY)
./colamd.c:2600:	    /* once found, find its principal parent */
./colamd.c:2604:	    } while (!COL_IS_DEAD_PRINCIPAL (parent)) ;
./colamd.c:2606:	    /* now, order all un-ordered non-principal columns along path */
./colamd.c:2665:    For a column c in a hash bucket, Col [c].shared3.prev is NOT a "previous
./colamd.c:2672:    just been computed in the approximate degree computation.
./colamd.c:2676:PRIVATE void detect_super_cols
./colamd.c:2702:    int prev_c ;		/* column preceding c in hash bucket */
./colamd.c:2746:	    /* prev_c is the column preceding column c in the hash bucket */
./colamd.c:2747:	    prev_c = super_c ;
./colamd.c:2762:		    prev_c = c ;
./colamd.c:2786:		    prev_c = c ;
./colamd.c:2796:		KILL_NON_PRINCIPAL_COL (c) ;
./colamd.c:2800:		Col [prev_c].shared4.hash_next = Col [c].shared4.hash_next ;
./colamd.c:2833:PRIVATE int garbage_collection  /* returns the new value of pfree */
./colamd.c:2886:    /* === Prepare to defragment the rows =================================== */
./colamd.c:2969:PRIVATE int clear_mark	/* return the new value for tag_mark */
./colamd.c:2993:/* === print_report ========================================================= */
./colamd.c:2996:PRIVATE void print_report
./colamd.c:3007:    	PRINTF ("%s: No statistics available.\n", method) ;
./colamd.c:3017:    	PRINTF ("%s: OK.  ", method) ;
./colamd.c:3021:    	PRINTF ("%s: ERROR.  ", method) ;
./colamd.c:3029:	    PRINTF ("Matrix has unsorted or duplicate row indices.\n") ;
./colamd.c:3031:	    PRINTF ("%s: number of duplicate or out-of-order row indices: %d\n",
./colamd.c:3034:	    PRINTF ("%s: last seen duplicate or out-of-order row index:   %d\n",
./colamd.c:3037:	    PRINTF ("%s: last seen in column:                             %d",
./colamd.c:3044:	    PRINTF ("\n") ;
./colamd.c:3046: 	    PRINTF ("%s: number of dense or empty rows ignored:           %d\n",
./colamd.c:3049:	    PRINTF ("%s: number of dense or empty columns ignored:        %d\n",
./colamd.c:3052:	    PRINTF ("%s: number of garbage collections performed:         %d\n",
./colamd.c:3056:	case COLAMD_ERROR_A_not_present:
./colamd.c:3058:	    PRINTF ("Array A (row indices of matrix) not present.\n") ;
./colamd.c:3061:	case COLAMD_ERROR_p_not_present:
./colamd.c:3063:	    PRINTF ("Array p (column pointers for matrix) not present.\n") ;
./colamd.c:3068:	    PRINTF ("Invalid number of rows (%d).\n", i1) ;
./colamd.c:3073:	    PRINTF ("Invalid number of columns (%d).\n", i1) ;
./colamd.c:3078:	    PRINTF ("Invalid number of nonzero entries (%d).\n", i1) ;
./colamd.c:3083:	    PRINTF ("Invalid column pointer, p [0] = %d, must be zero.\n", i1) ;
./colamd.c:3088:	    PRINTF ("Array A too small.\n") ;
./colamd.c:3089:	    PRINTF ("        Need Alen >= %d, but given only Alen = %d.\n",
./colamd.c:3095:	    PRINTF
./colamd.c:3102:	    PRINTF
./colamd.c:3109:	    PRINTF ("Out of memory.\n") ;
./colamd.c:3115:	    PRINTF
./colamd.c:3144:PRIVATE void debug_structures
./colamd.c:3226:    Prints the contents of the degree lists.  Counts the number of columns
./colamd.c:3231:PRIVATE void debug_deg_lists
./colamd.c:3305:PRIVATE void debug_mark
./colamd.c:3338:    Prints out the contents of the columns and the rows.
./colamd.c:3341:PRIVATE void debug_matrix
./colamd.c:3406:PRIVATE void colamd_get_debug
./colamd.c:3411:    colamd_debug = 0 ;		/* no debug printing */
./colamd.c:3413:    /* get "D" environment variable, which gives the debug printing level */
./colamd.h:4:approvals from U.S. Dept. of Energy) 
./colamd.h:12:    \brief Colamd prototypes and definitions
./colamd.h:14:	<pre> 
./colamd.h:16:    === colamd/symamd prototypes and definitions =============================
./colamd.h:43:	THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY
./colamd.h:44:	EXPRESSED OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
./colamd.h:47:	this program, provided that the Copyright, this License, and the
./colamd.h:60:	files, and by any C code that calls the routines whose prototypes are
./colamd.h:62: </pre>
./colamd.h:104:#define COLAMD_ERROR_A_not_present		(-1)
./colamd.h:105:#define COLAMD_ERROR_p_not_present		(-2)
./colamd.h:131:	int thickness ;	/* number of original columns represented by this */
./colamd.h:146:	int prev ;	/* previous column in degree list, if col is in a */
./colamd.h:160:    int length ;	/* number of principal columns in this row */
./colamd.h:163:	int degree ;	/* number of principal & non-principal columns in row */
./colamd.h:206:/* === Prototypes of user-callable routines ================================= */
./comm.c:4:approvals from U.S. Dept. of Energy) 
./comm.c:14: * <pre>
./comm.c:18: * </pre>
./comm.c:25: * <pre>
./comm.c:30: *   The process ranks are between 0 and Np-1.
./comm.c:68: * </pre>
./comm.c:102:/*	MPI_Probe( MPI_ANY_SOURCE, tag, scp->comm, &status );*/
./cublas_utils.c:4:approvals from U.S. Dept. of Energy) 
./cublas_utils.c:20:    printf("CUDA version:   v %d\n",CUDART_VERSION);
./cublas_utils.c:25:    printf( "CUDA Devices: \n \n"); 
./cublas_utils.c:29:        struct cudaDeviceProp props;       
./cublas_utils.c:30:        cudaGetDeviceProperties(&props, i);
./cublas_utils.c:31:        printf("%d : %s %d %d\n",i, props.name,props.major,props.minor );
./cublas_utils.c:32:        // cout << i << ": " << props.name << ": " << props.major << "." << props.minor << endl;
./cublas_utils.c:33:        printf("  Global memory:   %ld mb \n", props.totalGlobalMem / mb);
./cublas_utils.c:34:        // cout << "  Global memory:   " << props.totalGlobalMem / mb << "mb" << endl;
./cublas_utils.c:35:        printf("  Shared memory:   %ld kb \n", props.sharedMemPerBlock / kb ); //<<  << "kb" << endl;
./cublas_utils.c:36:        printf("  Constant memory: %ld kb \n", props.totalConstMem / kb );
./cublas_utils.c:37:        printf("  Block registers: %d \n\n", props.regsPerBlock );
./cublas_utils.c:40:        // printf("  Warp size:         %d" << props.warpSize << endl;
./cublas_utils.c:41:        // printf("  Threads per block: %d" << props.maxThreadsPerBlock << endl;
./cublas_utils.c:42:        // printf("  Max block dimensions: [ %d" << props.maxThreadsDim[0] << ", " << props.maxThreadsDim[1]  << ", " << props.maxThreadsDim[2] << " ]" << endl;
./cublas_utils.c:43:        // printf("  Max grid dimensions:  [ %d" << props.maxGridSize[0] << ", " << props.maxGridSize[1]  << ", " << props.maxGridSize[2] << " ]" << endl;
./cublas_utils.c:45:        // cout << "  Shared memory:   " << props.sharedMemPerBlock / kb << "kb" << endl;
./cublas_utils.c:46:        // cout << "  Constant memory: " << props.totalConstMem / kb << "kb" << endl;
./cublas_utils.c:47:        // cout << "  Block registers: " << props.regsPerBlock << endl << endl;
./cublas_utils.c:49:        // cout << "  Warp size:         " << props.warpSize << endl;
./cublas_utils.c:50:        // cout << "  Threads per block: " << props.maxThreadsPerBlock << endl;
./cublas_utils.c:51:        // cout << "  Max block dimensions: [ " << props.maxThreadsDim[0] << ", " << props.maxThreadsDim[1]  << ", " << props.maxThreadsDim[2] << " ]" << endl;
./cublas_utils.c:52:        // cout << "  Max grid dimensions:  [ " << props.maxGridSize[0] << ", " << props.maxGridSize[1]  << ", " << props.maxGridSize[2] << " ]" << endl;
./cublas_utils.c:79:        fprintf(stderr, "CUDA Runtime Error: %s\n", cudaGetErrorString(result));
./cublas_utils.c:90:    fprintf(stderr, "CUDA Blas Runtime Error: %s\n", cublasGetErrorString(result));
./cublas_utils.h:4:approvals from U.S. Dept. of Energy) 
./cublas_utils.h:12: * <pre>
./cublas_utils.h:16: * </pre>
./dSchCompUdt-2Ddynamic.c:4:approvals from U.S. Dept. of Energy) 
./dSchCompUdt-2Ddynamic.c:18: * <pre>
./dSchCompUdt-2Ddynamic.c:153:	 /* if ( iam==0 ) printf("--- k0 %d, k %d, jj0 %d, nub %d\n", k0, k, jj0, nub);*/
./dSchCompUdt-2Ddynamic.c:179:		 printf("j %d: Ublock_info[j].iukp %d, Ublock_info[j].rukp %d,"
./dSchCompUdt-2Ddynamic.c:184:	     /* Prepare to call GEMM. */
./dSchCompUdt-2Ddynamic.c:204:	 /* Now doing prefix sum on full_u_cols.
./dSchCompUdt-2Ddynamic.c:241:         /* Gather U(k,:) into buffer bigU[] to prepare for GEMM */
./dSchCompUdt-2Ddynamic.c:243:#pragma omp parallel for firstprivate(iukp, rukp) \
./dSchCompUdt-2Ddynamic.c:244:    private(j,tempu, jb, nsupc,ljb,segsize, lead_zero, jj, i) \
./dSchCompUdt-2Ddynamic.c:253:            /* == processing each of the remaining columns in parallel == */
./dSchCompUdt-2Ddynamic.c:272:#pragma omp simd
./dSchCompUdt-2Ddynamic.c:283:	if (ldu==0) printf("[%d] .. k0 %d, before updating: ldu %d, Lnbrow %d, Rnbrow %d, ncols %d\n",iam,k0,ldu,Lnbrow,Rnbrow, ncols);
./dSchCompUdt-2Ddynamic.c:302:#pragma omp parallel for private(j,jj,tempu,tempv) default (shared)
./dSchCompUdt-2Ddynamic.c:317:	 /* #pragma omp parallel for (gives slow down) */
./dSchCompUdt-2Ddynamic.c:326:	     tempv = &lusup[luptr+j*nsupr + StRowSource];
./dSchCompUdt-2Ddynamic.c:328:#pragma omp simd
./dSchCompUdt-2Ddynamic.c:334:		    &lusup[luptr+j*nsupr + StRowSource],
./dSchCompUdt-2Ddynamic.c:342:#pragma omp parallel for private(i,j,jj,tempu,tempv) default (shared) \
./dSchCompUdt-2Ddynamic.c:358:	 // #pragma omp parallel for (gives slow down)
./dSchCompUdt-2Ddynamic.c:361:	     // printf("StRowDest %d Rnbrow %d StRowSource %d \n", StRowDest,Rnbrow ,StRowSource);
./dSchCompUdt-2Ddynamic.c:367:	     tempv = &lusup[luptr + j*nsupr + StRowSource];
./dSchCompUdt-2Ddynamic.c:369:#pragma omp simd
./dSchCompUdt-2Ddynamic.c:375:		    &lusup[luptr+j*nsupr + StRowSource],
./dSchCompUdt-2Ddynamic.c:404:#pragma omp parallel default (shared) private(thread_id)
./dSchCompUdt-2Ddynamic.c:423:#pragma omp for \
./dSchCompUdt-2Ddynamic.c:424:    private (nsupc,ljb,lptr,ib,temp_nbrow,cum_nrow)	\
./dSchCompUdt-2Ddynamic.c:470:#if ( PRNTlevel>= 1)
./dSchCompUdt-2Ddynamic.c:489:#if (PRNTlevel>=1 )
./dSchCompUdt-2Ddynamic.c:529:#if ( PRNTlevel>=1 )
./dSchCompUdt-2Ddynamic.c:550:#if ( PRNTlevel>=1 )
./dSchCompUdt-2Ddynamic.c:556:	/* printf("[%d] .. k0 %d, before large GEMM: %d-%d-%d, RemainBlk %d\n",
./dSchCompUdt-2Ddynamic.c:575:#if ( PRNTlevel>=1 )
./dSchCompUdt-2Ddynamic.c:578:#if ( PROFlevel>=1 )
./dSchCompUdt-2Ddynamic.c:579:	//fprintf(fgemm, "%8d%8d%8d %16.8e\n", Rnbrow, ncols, ldu,
./dSchCompUdt-2Ddynamic.c:597:#pragma omp parallel default(shared) private(thread_id)
./dSchCompUdt-2Ddynamic.c:616:#pragma omp for \
./dSchCompUdt-2Ddynamic.c:617:    private (j,lb,rukp,iukp,jb,nsupc,ljb,lptr,ib,temp_nbrow,cum_nrow)	\
./dSchCompUdt-2Ddynamic.c:659:		// printf("[%d] .. before scatter: ib %d, jb %d, temp_nbrow %d, Rnbrow %d\n", iam, ib, jb, temp_nbrow, Rnbrow); fflush(stdout);
./dSchCompUdt-2Ddynamic.c:694:#if ( PRNTlevel>=1 )
./dbinary_io.c:11:    printf("fread n " IFMT "\tnnz " IFMT "\n", *n, *nnz);
./dbinary_io.c:19:    printf("# of doubles fread: %d\n", nnz_read);
./dbinary_io.c:36:      printf("n " IFMT ", # of double: " IFMT "\n", n, nnz);
./dbinary_io.c:37:      printf("dump binary file ... # of double fwrite: %d\n", nnz_written);
./dcomplex.h:4:approvals from U.S. Dept. of Energy) 
./dcomplex.h:14: * <pre>
./dcomplex.h:18: * </pre>
./dcomplex.h:79:/* Prototypes for functions in dcomplex.c */
./dcomplex.h:82:double slud_z_abs1(doublecomplex *);    /* approximate */
./dcomplex_dist.c:4:approvals from U.S. Dept. of Energy) 
./dcomplex_dist.c:14: * <pre>
./dcomplex_dist.c:18: * </pre>
./dcomplex_dist.c:40:	    fprintf(stderr, "slud_z_div.c: division by zero");
./dcomplex_dist.c:80:/* Approximates the abs */
./ddistribute.c:4:approvals from U.S. Dept. of Energy) 
./ddistribute.c:14: * \brief Distribute the matrix onto the 2D process mesh.
./ddistribute.c:16: * <pre>
./ddistribute.c:20: * </pre>
./ddistribute.c:26: * <pre>
./ddistribute.c:29: *   Distribute the matrix onto the 2D process mesh.
./ddistribute.c:52: *        The 2D process mesh.
./ddistribute.c:57: * </pre>
./ddistribute.c:73:    int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
./ddistribute.c:74:    int iam, jbrow, kcol, mycol, myrow, pc, pr;
./ddistribute.c:91:    double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
./ddistribute.c:92:    int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
./ddistribute.c:99:    int_t  **fsendx_plist; /* Column process list to send down Xk.   */
./ddistribute.c:106:    int_t  **bsendx_plist; /* Column process list to send down Xk.   */
./ddistribute.c:113:    int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./ddistribute.c:114:    int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
./ddistribute.c:115:    int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
./ddistribute.c:116:    int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
./ddistribute.c:118:    int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
./ddistribute.c:119:    int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
./ddistribute.c:120:    int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
./ddistribute.c:121:    int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
./ddistribute.c:128:#if ( PRNTlevel>=1 )
./ddistribute.c:131:#if ( PROFlevel>=1 ) 
./ddistribute.c:147:#if ( PRNTlevel>=1 )
./ddistribute.c:158:         * REUSE THE L AND U DATA STRUCTURES FROM A PREVIOUS FACTORIZATION.
./ddistribute.c:161:#if ( PROFlevel>=1 )
./ddistribute.c:164:	/* We can propagate the new values of A into the existing
./ddistribute.c:170:	nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
./ddistribute.c:179:#if ( PRNTlevel>=1 )
./ddistribute.c:182:#if ( PROFlevel>=1 )
./ddistribute.c:199:	    if ( mycol == pc ) { /* Block column jb in my process column */
./ddistribute.c:208:			if ( myrow == PROW( gb, grid ) ) {
./ddistribute.c:240:#if ( PROFlevel>=1 )
./ddistribute.c:271:#if ( PROFlevel>=1 )
./ddistribute.c:280:#if ( PROFlevel>=1 )
./ddistribute.c:281:	if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
./ddistribute.c:290:#if ( PROFlevel>=1 )
./ddistribute.c:294:	   We need to set up the L and U data structures and propagate
./ddistribute.c:296:	lsub = Glu_freeable->lsub;    /* compressed L subscripts */
./ddistribute.c:298:	usub = Glu_freeable->usub;    /* compressed U subscripts */
./ddistribute.c:311:#if ( PRNTlevel>=1 )
./ddistribute.c:316:	k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./ddistribute.c:343:#if ( PRNTlevel>=1 )	
./ddistribute.c:350:	    if ( myrow == PROW( gb, grid ) ) {
./ddistribute.c:361:	   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
./ddistribute.c:378:		    pr = PROW( gb, grid );
./ddistribute.c:381:			if  ( myrow == pr ) {
./ddistribute.c:390:#if ( PRNTlevel>=1 )
./ddistribute.c:402:	nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./ddistribute.c:431:#if ( PROFlevel>=1 )
./ddistribute.c:433:	if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
./ddistribute.c:435:#if ( PRNTlevel>=1 )
./ddistribute.c:458:#if ( PRNTlevel>=1 )	
./ddistribute.c:470:	/* These lists of processes will be used for triangular solves. */
./ddistribute.c:473:	len = k * grid->nprow;
./ddistribute.c:477:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./ddistribute.c:484:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./ddistribute.c:486:#if ( PRNTlevel>=1 )
./ddistribute.c:490:	  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
./ddistribute.c:491:	  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
./ddistribute.c:496:	    if ( mycol == pc ) { /* Block column jb in my process column */
./ddistribute.c:506:			if ( myrow == PROW( gb, grid ) ) {
./ddistribute.c:515:		jbrow = PROW( jb, grid );
./ddistribute.c:517:#if ( PROFlevel>=1 )
./ddistribute.c:533:			pr = PROW( gb, grid );
./ddistribute.c:534:			if ( pr != jbrow &&
./ddistribute.c:535:			     myrow == jbrow &&  /* diag. proc. owning jb */
./ddistribute.c:536:			     bsendx_plist[ljb][pr] == EMPTY ) {
./ddistribute.c:537:			    bsendx_plist[ljb][pr] = YES;
./ddistribute.c:540:			if ( myrow == pr ) {
./ddistribute.c:578:			} /* if myrow == pr ... */
./ddistribute.c:583:#if ( PROFlevel>=1 )
./ddistribute.c:600:		    pr = PROW( gb, grid ); /* Process row owning this block */
./ddistribute.c:601:		    if ( pr != jbrow &&
./ddistribute.c:602:			 myrow == jbrow &&  /* diag. proc. owning jb */
./ddistribute.c:603:			 fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
./ddistribute.c:604:			fsendx_plist[ljb][pr] = YES;
./ddistribute.c:607:		    if ( myrow == pr ) {
./ddistribute.c:619:#if ( PRNTlevel>=1 )
./ddistribute.c:638:			fprintf(stderr, "col block " IFMT " ", jb);
./ddistribute.c:660:		    /* Propagate the compressed row subscripts to Lindex[], and
./ddistribute.c:667:			if ( myrow == PROW( gb, grid ) ) {
./ddistribute.c:685:#if ( PROFlevel>=1 )
./ddistribute.c:710:#if ( PRNTlevel>=1 )
./ddistribute.c:711:	if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
./ddistribute.c:725:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./ddistribute.c:733:#if ( PROFlevel>=1 )
./ddistribute.c:734:	if ( !iam ) printf(".. 1st distribute time:\n "
./dgsequ_dist.c:4:approvals from U.S. Dept. of Energy) 
./dgsequ_dist.c:24:<pre>    
./dgsequ_dist.c:37:    works well in practice.   
./dgsequ_dist.c:80:</pre>
./dlangs_dist.c:4:approvals from U.S. Dept. of Energy) 
./dlangs_dist.c:25:<pre> 
./dlangs_dist.c:60:</pre>
./dlaqgs_dist.c:4:approvals from U.S. Dept. of Energy) 
./dlaqgs_dist.c:23:<pre>
./dlaqgs_dist.c:58:            = 'R':  Row equilibration, i.e., A has been premultiplied by  
./dlaqgs_dist.c:78:</pre>
./dlaqgs_dist.c:104:    small = dmach_dist("Safe minimum") / dmach_dist("Precision");
./dldperm_dist.c:4:approvals from U.S. Dept. of Energy) 
./dldperm_dist.c:16: * <pre>
./dldperm_dist.c:20: * </pre>
./dldperm_dist.c:31: * <pre>
./dldperm_dist.c:55: *        = 5 : Compute a row permutation of the matrix so that the product
./dldperm_dist.c:91: * </pre>
./dldperm_dist.c:115:    printf("LDPERM(): n %d, nnz %d\n", n, nnz);
./dldperm_dist.c:116:    PrintInt10("colptr", n+1, colptr);
./dldperm_dist.c:117:    PrintInt10("adjncy", nnz, adjncy);
./dldperm_dist.c:127:     * Since a symmetric permutation preserves the diagonal entries. Then
./dldperm_dist.c:137:    /* Suppress error and warning messages. */
./dldperm_dist.c:145:    PrintInt10("perm", n, perm);
./dldperm_dist.c:146:    printf(".. After MC64AD info %d\tsize of matching %d\n", info[0], num);
./dldperm_dist.c:149:        printf(".. The last " IFMT " permutations:\n", n-num);
./dldperm_dist.c:150:	PrintInt10("perm", n-num, &perm[num]);
./dlook_ahead_update.c:4:approvals from U.S. Dept. of Energy) 
./dlook_ahead_update.c:17: * <pre>
./dlook_ahead_update.c:89:    printf ("(%d) k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./dlook_ahead_update.c:122:       'firstprivate' ensures that the private variables are initialized
./dlook_ahead_update.c:124:#pragma omp parallel for \
./dlook_ahead_update.c:125:    firstprivate(lptr,luptr,ib,current_b) private(lb) \
./dlook_ahead_update.c:129:        int temp_nbrow; /* automatic variable is private */
./dlook_ahead_update.c:164:                   &lusup[luptr + (knsupc - ldu) * nsupr], &nsupr,
./dlook_ahead_update.c:168:                   &lusup[luptr + (knsupc - ldu) * nsupr], &nsupr,
./dlook_ahead_update.c:236:        /* Multicasts numeric values of L(:,kk) to process rows. */
./dlook_ahead_update.c:251:        scp = &grid->rscp;      /* The scope of process row. */
./dlook_ahead_update.c:254:#if ( PROFlevel>=1 )
./dlook_ahead_update.c:263:#if ( PROFlevel>=1 )
./dlook_ahead_update.c:270:                printf ("[%d] -2- Send L(:,%4d): #lsub %4d, #lusup %4d to Pj %2d, tags %d:%d \n",
./dmach_dist.c:4:approvals from U.S. Dept. of Energy) 
./dmach_dist.c:27:    DMACH returns double precision machine parameters.   
./dmach_dist.c:47:            eps   = relative machine precision   
./dmach_dist.c:50:            prec  = eps*base   
./dmemory_dist.c:4:approvals from U.S. Dept. of Energy) 
./dmemory_dist.c:16: * <pre>
./dmemory_dist.c:20: * </pre>
./dmemory_dist.c:63: * <pre>
./dmemory_dist.c:71: * </pre>
./dmemory_dist.c:106:    nb = CEILING( nsupers, grid->nprow ); /* Number of local row blocks */
./dmemory_dist.c:108:	gb = k * grid->nprow + myrow; /* Global block number. */
./dmemory_dist.c:128:    k = CEILING( nsupers, grid->nprow );
./dmemory_dist.c:134:#if ( PRNTlevel>=1 )
./dmemory_dist.c:135:    if (iam==0) printf(".. dQuerySpace: peak_buffer %.2f (MB)\n",
./dmyblas2_dist.c:4:approvals from U.S. Dept. of Energy) 
./dmyblas2_dist.c:16: * <pre>
./dmyblas2_dist.c:21: * </pre>
./dmyblas2_dist.c:33: * <pre>
./dmyblas2_dist.c:37: * </pre>
./dmyblas2_dist.c:133: * <pre>
./dmyblas2_dist.c:137: * </pre>
./dmyblas2_dist.c:169: * <pre>
./dmyblas2_dist.c:171: * The input matrix is M(1:nrow,1:ncol); The product is returned in Mxvec[].
./dmyblas2_dist.c:172: * </pre>
./dreadMM.c:4:approvals from U.S. Dept. of Energy) 
./dreadMM.c:27: * <pre>
./dreadMM.c:34: * </pre>
./dreadMM.c:63:       printf("Invalid header (first line does not contain 5 tokens)\n");
./dreadMM.c:68:       printf("Invalid header (first token is not \"%%%%MatrixMarket\")\n");
./dreadMM.c:73:       printf("Not a matrix; this driver cannot handle that.\n");
./dreadMM.c:78:       printf("Not in coordinate format; this driver cannot handle that.\n");
./dreadMM.c:84:         printf("Complex matrix; use zreadMM instead!\n");
./dreadMM.c:88:         printf("Pattern matrix; values are needed!\n");
./dreadMM.c:92:         printf("Unknown arithmetic\n");
./dreadMM.c:98:       printf("Symmetric matrix: will be expanded\n");
./dreadMM.c:117:      printf("Rectangular matrix!. Abort\n");
./dreadMM.c:127:    printf("m %lld, n %lld, nonz %lld\n", (long long) *m, (long long) *n, (long long) *nonz);
./dreadMM.c:154:		printf("triplet file: row/col indices are zero-based.\n");
./dreadMM.c:156:		printf("triplet file: row/col indices are one-based.\n");
./dreadMM.c:168:	    fprintf(stderr, "nz " IFMT ", (" IFMT ", " IFMT ") = %e out of bound, removed\n", 
./dreadMM.c:188:      printf("new_nonz after symmetric expansion:\t" IFMT "\n", *nonz);
./dreadMM.c:224:	printf("Col %d, xa %d\n", i, xa[i]);
./dreadMM.c:226:	    printf("%d\t%16.10f\n", asub[k], a[k]);
./dreadMM.c:239:        fprintf(stderr, "dreadrhs: file does not exist\n");
./dreadhb.c:4:approvals from U.S. Dept. of Energy) 
./dreadhb.c:14: * \brief Read a DOUBLE PRECISION matrix stored in Harwell-Boeing format
./dreadhb.c:16: * <pre>
./dreadhb.c:20: * </pre>
./dreadhb.c:28: * Prototypes
./dreadhb.c:39: * <pre>
./dreadhb.c:43: * Read a DOUBLE PRECISION matrix stored in Harwell-Boeing format 
./dreadhb.c:57: *		       if present) 
./dreadhb.c:58: *           	      (zero indicates no right-hand side data is present) 
./dreadhb.c:74: * Line 5 (A3, 11X, 2I14) Only present if there are right-hand sides present 
./dreadhb.c:103: * </pre>
./dreadhb.c:137:    if ( !iam ) printf("Matrix type %s\n", type);
./dreadhb.c:146:	if ( !iam ) printf("This is not an assembled matrix!\n");
./dreadhb.c:148:	if ( !iam ) printf("Matrix is not square.\n");
./dreadhb.c:169:	printf(IFMT " rows, " IFMT " nonzeros\n", *nrow, *nonz);
./dreadhb.c:170:	printf("colnum " IFMT ", colsize " IFMT "\n", colnum, colsize);
./dreadhb.c:171:	printf("rownum " IFMT ", rowsize " IFMT "\n", rownum, rowsize);
./dreadhb.c:172:	printf("valnum " IFMT ", valsize " IFMT "\n", valnum, valsize);
./dreadhb.c:178:    if ( !iam )	printf("read colptr[" IFMT "] = " IFMT "\n", *ncol, (*colptr)[*ncol]);
./dreadhb.c:182:    if ( !iam )	printf("read rowind[" IFMT "] = " IFMT "\n", *nonz-1, (*rowind)[*nonz-1]);
./dreadhb.c:187:	if ( !iam ) printf("read nzval[" IFMT "] = %e\n", *nonz-1, (*nzval)[*nonz-1]);
./dreadhb.c:292: * <pre>
./dreadhb.c:293: * On input, nonz/nzval/rowind/colptr represents lower part of a symmetric
./dreadhb.c:294: * matrix. On exit, it represents the full matrix with lower and upper parts.
./dreadhb.c:295: * </pre>
./dreadhb.c:356:	      printf("%5d: %e\n", k, a_val[k]);
./dreadhb.c:367:	    printf("%5d: %e\n", k, a_val[k]);
./dreadhb.c:375:    printf("FormFullA: new_nnz = " IFMT ", k = " IFMT "\n", new_nnz, k);
./dreadrb.c:4:approvals from U.S. Dept. of Energy) 
./dreadrb.c:16: * <pre>
./dreadrb.c:21: * </pre>
./dreadrb.c:26: * Read a DOUBLE PRECISION matrix stored in Rutherford-Boeing format 
./dreadrb.c:41: *      Col. 15 - 28  Compressed Column: Number of rows (NROW)
./dreadrb.c:43: *      Col. 30 - 42  Compressed Column: Number of columns (NCOL)
./dreadrb.c:45: *      Col. 44 - 56  Compressed Column: Number of entries (NNZERO)
./dreadrb.c:47: *      Col. 58 - 70  Compressed Column: Unused, explicitly zero
./dreadrb.c:78: *      A Compressed column form
./dreadrb.c:81: * </pre>
./dreadrb.c:183: * <pre>
./dreadrb.c:184: * On input, nonz/nzval/rowind/colptr represents lower part of a symmetric
./dreadrb.c:185: * matrix. On exit, it represents the full matrix with lower and upper parts.
./dreadrb.c:186: * </pre>
./dreadrb.c:258:    printf("FormFullA: new_nnz = " IFMT ", k = " IFMT "\n", new_nnz, k);
./dreadrb.c:300:    if ( !iam ) printf("Matrix type %s\n", type);
./dreadrb.c:309:        if ( !iam ) printf("This is not an assembled matrix!\n");
./dreadrb.c:311:        if ( !iam ) printf("Matrix is not square.\n");
./dreadrb.c:328:        printf(IFMT " rows, " IFMT " nonzeros\n", *nrow, *nonz);
./dreadrb.c:329:        printf("colnum " IFMT ", colsize " IFMT "\n", colnum, colsize);
./dreadrb.c:330:        printf("rownum " IFMT ", rowsize " IFMT "\n", rownum, rowsize);
./dreadrb.c:331:        printf("valnum " IFMT ", valsize " IFMT "\n", valnum, valsize);
./dreadtriple.c:4:approvals from U.S. Dept. of Energy) 
./dreadtriple.c:24: * <pre>
./dreadtriple.c:31: * </pre>
./dreadtriple.c:61:    printf("m %lld, n %lld, nonz %lld\n", (long long) *m, (long long) *n, (long long) *nonz);
./dreadtriple.c:87:		printf("triplet file: row/col indices are zero-based.\n");
./dreadtriple.c:89:		printf("triplet file: row/col indices are one-based.\n");
./dreadtriple.c:99:	    fprintf(stderr, "nz " IFMT ", (" IFMT ", " IFMT ") = %e out of bound, removed\n", 
./dreadtriple.c:119:    printf("new_nonz after symmetric expansion:\t%d\n", *nonz);
./dreadtriple.c:154:	printf("Col %d, xa %d\n", i, xa[i]);
./dreadtriple.c:156:	    printf("%d\t%16.10f\n", asub[k], a[k]);
./dreadtriple.c:169:        fprintf(stderr, "dreadrhs: file does not exist\n");
./dreadtriple_noheader.c:4:approvals from U.S. Dept. of Energy) 
./dreadtriple_noheader.c:24: * <pre>
./dreadtriple_noheader.c:31: * </pre>
./dreadtriple_noheader.c:74:	printf("triplet file: row/col indices are zero-based.\n");
./dreadtriple_noheader.c:76:	printf("triplet file: row/col indices are one-based.\n");
./dreadtriple_noheader.c:90:    printf("m %ld, n %ld, nonz %ld\n", *m, *n, *nonz);
./dreadtriple_noheader.c:121:	    fprintf(stderr, "nz %d, (%d, %d) = %e out of bound, removed\n", 
./dreadtriple_noheader.c:141:    printf("new_nonz after symmetric expansion:\t%d\n", *nonz);
./dreadtriple_noheader.c:175:	printf("Col %d, xa %d\n", i, xa[i]);
./dreadtriple_noheader.c:177:	    printf("%d\t%16.10f\n", asub[k], a[k]);
./dreadtriple_noheader.c:190:        fprintf(stderr, "zreadrhs: file does not exist\n");
./dscatter.c:4:approvals from U.S. Dept. of Energy) 
./dscatter.c:16: * <pre>
./dscatter.c:46:    // printf("hello\n");
./dscatter.c:81:        // printf("segsize %d \n",segsize);
./dscatter.c:83:            /*#pragma _CRI cache_bypass nzval,tempv */
./dscatter.c:87:                // printf("i (src) %d, perm (dest) %d  \n",i,indirect_thread[rel]);
./dscatter.c:91:                printf ("(%d %d, %0.3e, %0.3e, %3e ) ", ljb,
./dscatter.c:95:                //printing triplets (location??, old value, new value ) if none of them is zero
./dscatter.c:98:            // printf("\n");
./dscatter.c:101:            // printf("\n");
./dscatter.c:105:        // printf("%d\n",nzval );
./dscatter.c:154:#pragma omp simd
./dscatter.c:163:#pragma omp simd
./dscatter.c:165:    /* can be precalculated? */
./dscatter.c:173:#pragma ivdep
./dscatter.c:179:#pragma omp simd
./dscatter.c:209:    printf ("A(%d,%d) goes to U block \n", ib, jb);
./dscatter.c:232:        // printf("supersize[%ld] \t:%ld \n",ijb,SuperSize( ijb ) );
./dscatter.c:246:            // printf("========Entering loop=========\n");
./dscatter.c:248:#pragma omp simd
./dscatter.c:252:                // printf("%d %d %d %d %d \n",lptr,i,fnz,temp_nbrow,nbrow );
./dscatter.c:253:                // printf("hello   ucol[%d] %d %d : \n",rel,lsub[lptr + i],fnz);
./dscatter.c:259:                    printf ("(%d, %0.3e, %0.3e ) ", rel, ucol[rel] + tempv[i],
./dscatter.c:261:                //printing triplets (location??, old value, new value ) if none of them is zero
./dscatter.c:266:            // printf("\n");
./dscatter.c:274:    // printf("\n");
./dscatter.c:303:    int* full_u_cols,       /*array containing prefix sum of work load */
./dscatter.c:337:        printf ("full_u_cols[num_blks-1] %d  %d \n",
./dscatter.c:339:        printf ("Early return \n");
./dscatter.c:367:    printf ("Remaining cols %d num_blks %d cpu_blks %d \n", cols_remain,
./dscatter.c:375:        printf ("%d %d  %d %d \n", full_u_cols[num_blks - 1],
./dscatter.c:382:        printf ("cols_per_stream :\t%d\n", cols_per_stream);
./dscatter.c:398:                printf ("i %d, j %d, %d  %d ", i, j, full_u_cols[j + 1],
./dscatter.c:404:                    printf ("cutoff met \n");
./dscatter.c:413:                printf ("\n");
./dscatter.c:430:                   Ublock_info_t *Ublock_info,    /*array containing prefix sum of work load */
./dsp_blas2_dist.c:4:approvals from U.S. Dept. of Energy) 
./dsp_blas2_dist.c:14: * <pre>
./dsp_blas2_dist.c:18: * </pre>
./dsp_blas2_dist.c:30: * Function prototypes 
./dsp_blas2_dist.c:40: * <pre>
./dsp_blas2_dist.c:75: *	       The factor L from the factorization Pr*A*Pc=L*U. Use
./dsp_blas2_dist.c:76: *             compressed row subscripts storage for supernodes, i.e.,
./dsp_blas2_dist.c:80: *	        The factor U from the factorization Pr*A*Pc=L*U.
./dsp_blas2_dist.c:90: * <pre>
./dsp_blas2_dist.c:106:    int fsupc, nsupr, nsupc, luptr, istart, irow;
./dsp_blas2_dist.c:145:		nsupr = SuperLU_L_SUB_START(fsupc+1) - istart;
./dsp_blas2_dist.c:148:		nrow = nsupr - nsupc;
./dsp_blas2_dist.c:165:		    STRSV(ftcs1, ftcs2, ftcs3, &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:169:		       	&nsupr, &x[fsupc], &incx, &beta, &work[0], &incy);
./dsp_blas2_dist.c:171:		    dtrsv_("L", "N", "U", &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:175:		       	&nsupr, &x[fsupc], &incx, &beta, &work[0], &incy, 1);
./dsp_blas2_dist.c:178:		    dlsolve ( nsupr, nsupc, &Lval[luptr], &x[fsupc]);
./dsp_blas2_dist.c:180:		    dmatvec ( nsupr, nsupr-nsupc, nsupc, &Lval[luptr+nsupc],
./dsp_blas2_dist.c:201:	    	nsupr = SuperLU_L_SUB_START(fsupc+1) - SuperLU_L_SUB_START(fsupc);
./dsp_blas2_dist.c:218:		    STRSV(ftcs1, ftcs2, ftcs2, &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:221:		    dtrsv_("U", "N", "N", &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:225:		    dusolve ( nsupr, nsupc, &Lval[luptr], &x[fsupc] );
./dsp_blas2_dist.c:249:	    	nsupr = SuperLU_L_SUB_START(fsupc+1) - istart;
./dsp_blas2_dist.c:253:		solve_ops += 2 * (nsupr - nsupc) * nsupc;
./dsp_blas2_dist.c:272:		    STRSV(ftcs1, ftcs2, ftcs3, &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:275:		    dtrsv_("L", "T", "U", &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:279:		    dtrsv_("L", "T", "U", &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:290:	    	nsupr = SuperLU_L_SUB_START(fsupc+1) - SuperLU_L_SUB_START(fsupc);
./dsp_blas2_dist.c:312:		    STRSV( ftcs1, ftcs2, ftcs3, &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:315:		    dtrsv_("U", "T", "N", &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:319:		    dtrsv_("U", "T", "N", &nsupc, &Lval[luptr], &nsupr,
./dsp_blas2_dist.c:335:<pre>
./dsp_blas2_dist.c:391:</pre>
./dsp_blas3_dist.c:4:approvals from U.S. Dept. of Energy) 
./dsp_blas3_dist.c:14: * <pre>
./dsp_blas3_dist.c:18: * </pre>
./dsp_blas3_dist.c:30:<pre>
./dsp_blas3_dist.c:91:    B      - DOUBLE PRECISION array of DIMENSION ( LDB, kb ), where kb is 
./dsp_blas3_dist.c:101:             in the calling (sub) program. LDB must be at least max( 1, n ).  
./dsp_blas3_dist.c:108:    C      - DOUBLE PRECISION array of DIMENSION ( LDC, n ).   
./dsp_blas3_dist.c:117:             in the calling (sub)program. LDC must be at least max(1,m).   
./dsp_blas3_dist.c:121:</pre>
./dutil_dist.c:4:approvals from U.S. Dept. of Energy) 
./dutil_dist.c:16: * <pre>
./dutil_dist.c:48:dCreate_CompRowLoc_Matrix_dist(SuperMatrix *A, int_t m, int_t n,
./dutil_dist.c:71:/*! \brief Convert a row compressed storage into a column compressed storage.
./dutil_dist.c:74:dCompRow_to_CompCol_dist(int_t m, int_t n, int_t nnz, 
./dutil_dist.c:96:    /* Transfer the matrix into the compressed column storage. */
./dutil_dist.c:132:void dPrint_CompCol_Matrix_dist(SuperMatrix *A)
./dutil_dist.c:138:    printf("\nCompCol matrix: ");
./dutil_dist.c:139:    printf("Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./dutil_dist.c:141:    printf("nrow %lld, ncol %lld, nnz %lld\n", (long long) A->nrow,
./dutil_dist.c:144:        printf("nzval:\n");
./dutil_dist.c:145:        for (i = 0; i < Astore->nnz; ++i) printf("%f  ", dp[i]);
./dutil_dist.c:147:    printf("\nrowind:\n");
./dutil_dist.c:149:        printf("%lld  ", (long long) Astore->rowind[i]);
./dutil_dist.c:150:    printf("\ncolptr:\n");
./dutil_dist.c:152:        printf("%lld  ", (long long) Astore->colptr[i]);
./dutil_dist.c:153:    printf("\nend CompCol matrix.\n");
./dutil_dist.c:156:void dPrint_Dense_Matrix_dist(SuperMatrix *A)
./dutil_dist.c:162:    printf("\nDense matrix: ");
./dutil_dist.c:163:    printf("Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./dutil_dist.c:166:    printf("nrow %lld, ncol %lld, lda %lld\n", 
./dutil_dist.c:168:    printf("\nnzval: ");
./dutil_dist.c:169:    for (i = 0; i < A->nrow; ++i) printf("%f  ", dp[i]);
./dutil_dist.c:170:    printf("\nend Dense matrix.\n");
./dutil_dist.c:173:int dPrint_CompRowLoc_Matrix_dist(SuperMatrix *A)
./dutil_dist.c:179:    printf("\n==== CompRowLoc matrix: ");
./dutil_dist.c:180:    printf("Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./dutil_dist.c:182:    printf("nrow %ld, ncol %ld\n", 
./dutil_dist.c:185:    printf("nnz_loc %ld, m_loc %ld, fst_row %ld\n", (long int) nnz_loc, 
./dutil_dist.c:187:    PrintInt10("rowptr", m_loc+1, Astore->rowptr);
./dutil_dist.c:188:    PrintInt10("colind", nnz_loc, Astore->colind);
./dutil_dist.c:190:        PrintDouble5("nzval", nnz_loc, dp);
./dutil_dist.c:191:    printf("==== end CompRowLoc matrix\n");
./dutil_dist.c:195:int file_dPrint_CompRowLoc_Matrix_dist(FILE *fp, SuperMatrix *A)
./dutil_dist.c:201:    fprintf(fp, "\n==== CompRowLoc matrix: ");
./dutil_dist.c:202:    fprintf(fp, "Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./dutil_dist.c:204:    fprintf(fp, "nrow %ld, ncol %ld\n", (long int) A->nrow, (long int) A->ncol);
./dutil_dist.c:206:    fprintf(fp, "nnz_loc %ld, m_loc %ld, fst_row %ld\n", (long int) nnz_loc,
./dutil_dist.c:208:    file_PrintInt10(fp, "rowptr", m_loc+1, Astore->rowptr);
./dutil_dist.c:209:    file_PrintInt10(fp, "colind", nnz_loc, Astore->colind);
./dutil_dist.c:211:        file_PrintDouble5(fp, "nzval", nnz_loc, dp);
./dutil_dist.c:212:    fprintf(fp, "==== end CompRowLoc matrix\n");
./dutil_dist.c:241: * <pre>
./dutil_dist.c:246: * </pre>
./dutil_dist.c:293:void dClone_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B)
./dutil_dist.c:324:void dCopy_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B)
./dutil_dist.c:328:    dClone_CompRowLoc_Matrix_dist(A, B);
./dutil_dist.c:341:void dZero_CompRowLoc_Matrix_dist(SuperMatrix *A)
./dutil_dist.c:358:void dScaleAddId_CompRowLoc_Matrix_dist(SuperMatrix *A, double c)
./dutil_dist.c:383:void dScaleAdd_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B, double c)
./dutil_dist.c:424:/*! \brief Fills a double precision array with a given value.
./dutil_dist.c:454:      printf("\tRHS %2d: ||X-Xtrue||/||X|| = %e\n", j, err);
./dutil_dist.c:458:void PrintDouble5(char *name, int_t len, double *x)
./dutil_dist.c:462:    printf("%10s:", name);
./dutil_dist.c:464:	if ( i % 5 == 0 ) printf("\n[%ld-%ld] ", (long int) i, (long int) i+4);
./dutil_dist.c:465:	printf("%14e", x[i]);
./dutil_dist.c:467:    printf("\n");
./dutil_dist.c:470:int file_PrintDouble5(FILE *fp, char *name, int_t len, double *x)
./dutil_dist.c:474:    fprintf(fp, "%10s:", name);
./dutil_dist.c:476:	if ( i % 5 == 0 ) fprintf(fp, "\n[%ld-%ld] ", (long int) i, (long int) i+4);
./dutil_dist.c:477:	fprintf(fp, "%14e", x[i]);
./dutil_dist.c:479:    fprintf(fp, "\n");
./dutil_dist.c:483:/*! \brief Print the blocks in the factored matrix L.
./dutil_dist.c:485:void dPrintLblocks(int iam, int_t nsupers, gridinfo_t *grid,
./dutil_dist.c:488:    register int c, extra, gb, j, lb, nsupc, nsupr, len, nb, ncb;
./dutil_dist.c:494:    printf("\n[%d] L BLOCKS IN COLUMN-MAJOR ORDER -->\n", iam);
./dutil_dist.c:504:	    nsupr = index[1];
./dutil_dist.c:507:	    printf("[%d] block column %d (local # %d), nsupc %d, # row blocks %d\n",
./dutil_dist.c:511:		printf("[%d] row-block %d: block # " IFMT "\tlength %d\n", 
./dutil_dist.c:513:		PrintInt10("lsub", len, &index[k+LB_DESCRIPTOR]);
./dutil_dist.c:515:		    PrintDouble5("nzval", len, &nzval[r + j*nsupr]);
./dutil_dist.c:521:	printf("(%d)", iam);
./dutil_dist.c:522: 	PrintInt32("ToSendR[]", grid->npcol, Llu->ToSendR[lb]);
./dutil_dist.c:523:	PrintInt10("fsendx_plist[]", grid->nprow, Llu->fsendx_plist[lb]);
./dutil_dist.c:525:    printf("nfrecvx " IFMT "\n", Llu->nfrecvx);
./dutil_dist.c:526:    k = CEILING( nsupers, grid->nprow );
./dutil_dist.c:527:    PrintInt10("fmod", k, Llu->fmod);
./dutil_dist.c:529:} /* DPRINTLBLOCKS */
./dutil_dist.c:538:    register int c, extra, gb, j, i, lb, nsupc, nsupr, len, nb, ncb;
./dutil_dist.c:547:	// assert(grid->npcol*grid->nprow==1);
./dutil_dist.c:561:	    nsupr = index[1];
./dutil_dist.c:586:	snprintf(filename, sizeof(filename), "%s-%d", "L", iam);    
./dutil_dist.c:587:    printf("Dumping L factor to --> %s\n", filename);
./dutil_dist.c:593:		fprintf(fp, "%d %d %d\n", n,n,nnzL);
./dutil_dist.c:605:	    nsupr = index[1];
./dutil_dist.c:613:			fprintf(fp, IFMT IFMT " %e\n", index[k+LB_DESCRIPTOR+i]+1, xsup[gb]+j+1, (double)iam);
./dutil_dist.c:615:			fprintf(fp, IFMT IFMT " %e\n", index[k+LB_DESCRIPTOR+i]+1, xsup[gb]+j+1, nzval[r +i+ j*nsupr]);
./dutil_dist.c:630:/*! \brief Print the blocks in the factored matrix U.
./dutil_dist.c:632:void dPrintUblocks(int iam, int_t nsupers, gridinfo_t *grid, 
./dutil_dist.c:641:    printf("\n[%d] U BLOCKS IN ROW-MAJOR ORDER -->\n", iam);
./dutil_dist.c:642:    nrb = nsupers / grid->nprow;
./dutil_dist.c:643:    extra = nsupers % grid->nprow;
./dutil_dist.c:651:	    printf("[%d] block row " IFMT " (local # %d), # column blocks %d\n",
./dutil_dist.c:652:		   iam, lb*grid->nprow+myrow, lb, nb);
./dutil_dist.c:657:		printf("[%d] col-block %d: block # %d\tlength " IFMT "\n", 
./dutil_dist.c:660:		PrintInt10("fstnz", nsupc, &index[k+UB_DESCRIPTOR]);
./dutil_dist.c:661:		PrintDouble5("nzval", len, &nzval[r]);
./dutil_dist.c:666:	    printf("[%d] ToSendD[] %d\n", iam, Llu->ToSendD[lb]);
./dutil_dist.c:669:} /* DPRINTUBLOCKS */
./dutil_dist.c:672:dprint_gsmv_comm(FILE *fp, int_t m_loc, pdgsmv_comm_t *gsmv_comm,
./dutil_dist.c:675:  int_t procs = grid->nprow*grid->npcol;
./dutil_dist.c:676:  fprintf(fp, "TotalIndSend " IFMT "\tTotalValSend " IFMT "\n", gsmv_comm->TotalIndSend,
./dutil_dist.c:678:  file_PrintInt10(fp, "extern_start", m_loc, gsmv_comm->extern_start);
./dutil_dist.c:679:  file_PrintInt10(fp, "ind_tosend", gsmv_comm->TotalIndSend, gsmv_comm->ind_tosend);
./dutil_dist.c:680:  file_PrintInt10(fp, "ind_torecv", gsmv_comm->TotalValSend, gsmv_comm->ind_torecv);
./dutil_dist.c:681:  file_PrintInt10(fp, "ptr_ind_tosend", procs+1, gsmv_comm->ptr_ind_tosend);
./dutil_dist.c:682:  file_PrintInt10(fp, "ptr_ind_torecv", procs+1, gsmv_comm->ptr_ind_torecv);
./dutil_dist.c:683:  file_PrintInt32(fp, "SendCounts", procs, gsmv_comm->SendCounts);
./dutil_dist.c:684:  file_PrintInt32(fp, "RecvCounts", procs, gsmv_comm->RecvCounts);
./dutil_dist.c:694:          nsupr, nsupers, rel;
./dutil_dist.c:709:    lb = CEILING( nsupers, grid->nprow ) + 1;
./dutil_dist.c:716:	i = PROW( j, grid );
./dutil_dist.c:718:	    nsupr = SuperSize( j );
./dutil_dist.c:719:	    *ldb += nsupr;
./dutil_dist.c:721:	    nlrows += nsupr;
./dutil_dist.c:737:	    gbrow = PROW( gb, grid );
./dutil_dist.c:751:#if ( PRNTlevel>=2 )
./dutil_dist.c:752:    for (i = 0; i < grid->nprow*grid->npcol; ++i) {
./dutil_dist.c:754:	    printf("\n(%d)\n", iam);
./dutil_dist.c:755:	    PrintDouble5("rhs", *ldb, *b);
./etree.c:4:approvals from U.S. Dept. of Energy) 
./etree.c:14: * <pre>
./etree.c:26: *  This implementation uses path compression but not weighted union.
./etree.c:31: * </pre>
./etree.c:106:/* PATH COMPRESSION */
./etree.c:128: * <pre>
./etree.c:140: *        Integer array of parents representing the etree, with n
./etree.c:153: * </pre>
./etree.c:205: * <pre>
./etree.c:214: *        Integer array of parents representing the elimination
./etree.c:215: *        tree of the symbolic product A'*A.  Each vertex is a
./etree.c:220: * </pre>
./etree.c:290: * <pre>
./etree.c:304: *          That is, this is the inverse of the previous q. )
./etree.c:306: *	In the child structure, lower-numbered children are represented
./etree.c:312: * </pre>
./get_perm_c.c:4:approvals from U.S. Dept. of Energy) 
./get_perm_c.c:14: * <pre>
./get_perm_c.c:19: * </pre>
./get_perm_c.c:144: * <pre>
./get_perm_c.c:149: * format represented by (colptr, rowind). The output A'*A is in column
./get_perm_c.c:150: * oriented format (symmetrically, also row oriented), represented by
./get_perm_c.c:161: * </pre>
./get_perm_c.c:251:	    fprintf(stderr, ".. atanz = %lld\n", (long long) *atanz);
./get_perm_c.c:290: * <pre>
./get_perm_c.c:295: * format represented by (colptr, rowind). The output A'+A is in column
./get_perm_c.c:296: * oriented format (symmetrically, also row oriented), represented by
./get_perm_c.c:298: * </pre>
./get_perm_c.c:431: * <pre>
./get_perm_c.c:437: * or using approximate minimum degree column ordering by Davis et. al.
./get_perm_c.c:461: * </pre>
./get_perm_c.c:486:#if ( PRNTlevel>=1 )
./get_perm_c.c:487:	      if ( !pnum ) printf(".. Use natural column ordering\n");
./get_perm_c.c:496:	      /*printf("Form A'+A time = %8.3f\n", t);*/
./get_perm_c.c:497:#if ( PRNTlevel>=1 )
./get_perm_c.c:498:	      if ( !pnum ) printf(".. Use minimum degree ordering on A'+A.\n");
./get_perm_c.c:506:	      /*printf("Form A'*A time = %8.3f\n", t);*/
./get_perm_c.c:507:#if ( PRNTlevel>=1 )
./get_perm_c.c:508:	      if ( !pnum ) printf(".. Use minimum degree ordering on A'*A\n");
./get_perm_c.c:512:        case (COLAMD): /* Approximate minimum degree column ordering. */
./get_perm_c.c:515:#if ( PRNTlevel>=1 )
./get_perm_c.c:516:	      printf(".. Use approximate minimum degree column ordering.\n");
./get_perm_c.c:533:#if ( PRNTlevel>=1 )
./get_perm_c.c:534:	      if ( !pnum ) printf(".. Use METIS ordering on A'+A\n");
./get_perm_c.c:579:	/*    printf("call GENMMD time = %8.3f\n", t);*/
./get_perm_c_parmetis.c:4:approvals from U.S. Dept. of Energy) 
./get_perm_c_parmetis.c:14: * <pre>
./get_perm_c_parmetis.c:21: * </pre>
./get_perm_c_parmetis.c:34: * Internal protypes
./get_perm_c_parmetis.c:38:a_plus_at_CompRow_loc
./get_perm_c_parmetis.c:44: * <pre>
./get_perm_c_parmetis.c:56: * number power of 2 that is smaller than nprocs_i, where nprocs_i = nprow
./get_perm_c_parmetis.c:57: * * npcol is the number of processors used in SuperLU_DIST.
./get_perm_c_parmetis.c:69: *         permutation matrix Pr; perm_r[i] = j means row i of A is in 
./get_perm_c_parmetis.c:70: *         position j in Pr*A.
./get_perm_c_parmetis.c:77: * nprocs_i (input) int*
./get_perm_c_parmetis.c:78: *         Number of processors the input matrix is distributed on in a block
./get_perm_c_parmetis.c:79: *         row format.  It corresponds to number of processors used in
./get_perm_c_parmetis.c:84: *         partitioning algorithm.  ( noDomains <= nprocs_i )
./get_perm_c_parmetis.c:101: * </pre>
./get_perm_c_parmetis.c:105:		     int nprocs_i, int noDomains, 
./get_perm_c_parmetis.c:127:  /* first row index on each processor when the matrix is distributed
./get_perm_c_parmetis.c:128:     on nprocs (vtxdist_i) or noDomains processors (vtxdist_o) */
./get_perm_c_parmetis.c:145:  m_loc = Astore->m_loc;     /* number of rows local to this processor */
./get_perm_c_parmetis.c:150:#if ( PRNTlevel>=1 )
./get_perm_c_parmetis.c:151:  if ( !iam ) printf(".. Use parMETIS ordering on A'+A with %d sub-domains. sizeof(int_t) %lu\n",
./get_perm_c_parmetis.c:156:  /* determine first row on each processor */
./get_perm_c_parmetis.c:157:  vtxdist_i = (int_t *) SUPERLU_MALLOC((nprocs_i+1) * sizeof(int_t));
./get_perm_c_parmetis.c:159:  vtxdist_o = (int_t *) SUPERLU_MALLOC((nprocs_i+1) * sizeof(int_t));
./get_perm_c_parmetis.c:164:  vtxdist_i[nprocs_i] = m;
./get_perm_c_parmetis.c:166:  if (noDomains == nprocs_i) {
./get_perm_c_parmetis.c:168:    for (p = 0; p <= nprocs_i; p++)
./get_perm_c_parmetis.c:179:    /* The remaining non-participating processors get the same 
./get_perm_c_parmetis.c:180:       first-row-number as the last processor.   */
./get_perm_c_parmetis.c:181:    for (p = noDomains; p <= nprocs_i; p++)
./get_perm_c_parmetis.c:187:    PrintInt10 ("vtxdist_o", nprocs_i + 1, vtxdist_o);
./get_perm_c_parmetis.c:192:       a_plus_at_CompRow_loc(iam, perm_r, nprocs_i, vtxdist_i,
./get_perm_c_parmetis.c:214:  /* ParMETIS represents the column pointers and row indices of *
./get_perm_c_parmetis.c:238:	 (int *) SUPERLU_MALLOC((nprocs_i+1) * sizeof(int))))
./get_perm_c_parmetis.c:240:  for (i = 0; i <= nprocs_i; i++)
./get_perm_c_parmetis.c:289:  if (!(displs = (int *) SUPERLU_MALLOC (nprocs_i * sizeof(int))))
./get_perm_c_parmetis.c:291:  if ( !(recvcnts = (int *) SUPERLU_MALLOC (nprocs_i * sizeof(int))))
./get_perm_c_parmetis.c:293:  for (i = 0; i < nprocs_i; i++)
./get_perm_c_parmetis.c:296:  for(i=1; i < nprocs_i; i++) 
./get_perm_c_parmetis.c:310:  /* send l_sizes to every processor p >= noDomains */
./get_perm_c_parmetis.c:312:    for (p = noDomains; p < nprocs_i; p++)
./get_perm_c_parmetis.c:314:  if (noDomains <= iam && iam < nprocs_i)
./get_perm_c_parmetis.c:351:#if ( PRNTlevel>=2 )
./get_perm_c_parmetis.c:353:    PrintInt10 ("Sizes of separators", 2 * noDomains-1, l_sizes);
./get_perm_c_parmetis.c:354:    PrintInt10 ("First Vertex Separator", 2 * noDomains-1, l_fstVtxSep);
./get_perm_c_parmetis.c:368: * <pre>
./get_perm_c_parmetis.c:372: * Form the structure of Pr*A +A'Pr'. A is an n-by-n matrix in
./get_perm_c_parmetis.c:373: * NRformat_loc format, represented by (rowptr, colind). The output
./get_perm_c_parmetis.c:374: * B=Pr*A +A'Pr' is in NRformat_loc format (symmetrically, also row
./get_perm_c_parmetis.c:375: * oriented), represented by (b_rowptr, b_colind).
./get_perm_c_parmetis.c:377: * The input matrix A is distributed in block row format on nprocs_i
./get_perm_c_parmetis.c:378: * processors.  The output matrix B is distributed in block row format
./get_perm_c_parmetis.c:379: * on nprocs_o processors, where nprocs_o <= nprocs_i.  On output, the
./get_perm_c_parmetis.c:385: * Let iam by my process number.  Let fst_row, lst_row = m_loc +
./get_perm_c_parmetis.c:388: * Compute Pr' - the inverse row permutation, stored in iperm_r.
./get_perm_c_parmetis.c:390: * Compute the transpose  of the block row of Pr*A that iam owns:
./get_perm_c_parmetis.c:391: *    T[:,Pr(fst_row:lst_row)] = Pr' * A[:,fst_row:lst_row] * Pr'
./get_perm_c_parmetis.c:394: * All to all communication such that every processor iam receives all
./get_perm_c_parmetis.c:400: * If Pr != I or nprocs_i != nprocs_o then permute the rows of B (that
./get_perm_c_parmetis.c:401: * is compute Pr*B) and redistribute from nprocs_i to nprocs_o
./get_perm_c_parmetis.c:403: * </pre>
./get_perm_c_parmetis.c:407:a_plus_at_CompRow_loc
./get_perm_c_parmetis.c:409: int   iam,         /* Input - my processor number */
./get_perm_c_parmetis.c:410: int_t *perm_r,     /* Input - row permutation vector Pr */
./get_perm_c_parmetis.c:411: int   nprocs_i,    /* Input - number of processors the input matrix
./get_perm_c_parmetis.c:413: int_t *vtxdist_i,  /* Input - index of first row on each processor of the input matrix */
./get_perm_c_parmetis.c:417: int   nprocs_o,    /* Input - number of processors the output matrix
./get_perm_c_parmetis.c:419: int_t *vtxdist_o,  /* Input - index of first row on each processor of the output matrix */
./get_perm_c_parmetis.c:424: gridinfo_t *grid    /* Input - grid of processors information */
./get_perm_c_parmetis.c:428:  int_t i, j, k, col, num_nz, nprocs;
./get_perm_c_parmetis.c:433:  int redist_pra; /* TRUE if Pr != I or nprocs_i != nprocs_o */
./get_perm_c_parmetis.c:446:  CHECK_MALLOC(iam, "Enter a_plus_at_CompRow_loc()");
./get_perm_c_parmetis.c:452:  redist_pra = FALSE;  
./get_perm_c_parmetis.c:453:  nprocs     = SUPERLU_MAX(nprocs_i, nprocs_o);
./get_perm_c_parmetis.c:460:  if (!(sendCnts = (int_t*) SUPERLU_MALLOC(nprocs * sizeof(int_t))))
./get_perm_c_parmetis.c:462:  if (!(recvCnts = (int_t*) SUPERLU_MALLOC(nprocs * sizeof(int_t))))
./get_perm_c_parmetis.c:464:  if (!(sdispls = (int_t*) SUPERLU_MALLOC((nprocs+1) * sizeof(int_t))))
./get_perm_c_parmetis.c:466:  if (!(rdispls = (int_t*) SUPERLU_MALLOC((nprocs+1) * sizeof(int_t))))
./get_perm_c_parmetis.c:468:  apat_mem = 2 * n + 4 * nprocs + 3;
./get_perm_c_parmetis.c:471:  intBuf1 = (int *) SUPERLU_MALLOC(4 * nprocs * sizeof(int));
./get_perm_c_parmetis.c:472:  intBuf2 = intBuf1 + nprocs;
./get_perm_c_parmetis.c:473:  intBuf3 = intBuf1 + 2 * nprocs;
./get_perm_c_parmetis.c:474:  intBuf4 = intBuf1 + 3 * nprocs;
./get_perm_c_parmetis.c:475:  apat_mem += 4*nprocs*sizeof(int) / sizeof(int_t);
./get_perm_c_parmetis.c:482:      redist_pra = TRUE;
./get_perm_c_parmetis.c:486:  /* TRANSPOSE LOCAL ROWS ON MY PROCESSOR iam.         */
./get_perm_c_parmetis.c:496:  /* determine number of elements to be sent to each processor */
./get_perm_c_parmetis.c:497:  for (p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:502:  /* exchange send/receive counts information in between all processors */
./get_perm_c_parmetis.c:507:  for (i = 0, j = 0, p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:524:  /* allocate memory to send blocks of local transpose matrix T to other processors */
./get_perm_c_parmetis.c:536:  for (p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:552:  for (i = 0, p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:571:  for (p=0; p<nprocs; p++) {
./get_perm_c_parmetis.c:609:  for (p = 0; p < nprocs_i; p++)
./get_perm_c_parmetis.c:628:    for (p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:644:  /* Allocate storage for B=Pr*A+A'*Pr' */
./get_perm_c_parmetis.c:658:  for (p = 0; p < nprocs_i; p++)
./get_perm_c_parmetis.c:680:    for (p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:696:  for (p = 0; p <= SUPERLU_MIN(nprocs_i, nprocs_o); p++) 
./get_perm_c_parmetis.c:698:      redist_pra = TRUE;
./get_perm_c_parmetis.c:708:  /* redistribute permuted matrix (by rows) from nproc_i processors
./get_perm_c_parmetis.c:709:     to nproc_o processors */
./get_perm_c_parmetis.c:710:  if (redist_pra) {
./get_perm_c_parmetis.c:721:    for (p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:728:      /* find the processor to which row k belongs */
./get_perm_c_parmetis.c:743:    /* exchange send/receive counts information in between all processors */
./get_perm_c_parmetis.c:747:    for (i = 0, j = 0, p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:781:      /* find the processor to which row k belongs */
./get_perm_c_parmetis.c:800:    for (i = 0, p = 0; p < nprocs_i; p++) {
./get_perm_c_parmetis.c:808:    for (p=0; p<nprocs; p++) {
./get_perm_c_parmetis.c:837:    for (p = 0; p < nprocs; p++) {
./get_perm_c_parmetis.c:869:    for (p = 0; p < nprocs; p++) {
./get_perm_c_parmetis.c:911:  apat_mem -= 4 * nprocs + 2;
./get_perm_c_parmetis.c:914:  apat_mem -= 4*nprocs*sizeof(int) / sizeof(int_t);
./get_perm_c_parmetis.c:918:  CHECK_MALLOC(iam, "Exit a_plus_at_CompRow_loc()");
./get_perm_c_parmetis.c:922:} /* a_plus_at_CompRow_loc */
./html_mainpage.h:4:approvals from U.S. Dept. of Energy) 
./machines.h:4:approvals from U.S. Dept. of Energy) 
./machines.h:14: * <pre>
./machines.h:21: * </pre>
./mc64ad_dist.c:48:/* express prior consent of the authors.  Users wanting to licence their */
./mc64ad_dist.c:54:/* </pre>
./mc64ad_dist.c:68:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:85:/*     is negative, these messages will be suppressed. */
./mc64ad_dist.c:89:/*     If it is negative, these messages are suppressed. */
./mc64ad_dist.c:92:/*     It is the output stream for monitoring printing. */
./mc64ad_dist.c:93:/*     If it is negative, these messages are suppressed. */
./mc64ad_dist.c:98:/*     other will avoid the checks but is likely to cause problems */
./mc64ad_dist.c:99:/*     later if out-of-range indices or duplicates are present. */
./mc64ad_dist.c:101:/*     known to avoid these problems. */
./mc64ad_dist.c:155:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:162: * <pre>
./mc64ad_dist.c:169: * maximizes the product of the diagonal entries of the permuted matrix. 
./mc64ad_dist.c:177: * </pre>
./mc64ad_dist.c:201:/*   5 Compute a column permutation of the matrix so that the product */
./mc64ad_dist.c:231:/* A is a REAL (DOUBLE PRECISION in the D-version) array of length NE. */
./mc64ad_dist.c:267:/* DW is a REAL (DOUBLE PRECISION in the D-version) array of length LDW */
./mc64ad_dist.c:276:/*   error messages. If ICNTL(1) < 0, messages are suppressed. */
./mc64ad_dist.c:280:/*   warning messages. If ICNTL(2) < 0, messages are suppressed. */
./mc64ad_dist.c:284:/*   diagnostic messages. If ICNTL(3) < 0, messages are suppressed. */
./mc64ad_dist.c:331:/*     DOUBLE PRECISION FD05AD */
./mc64ad_dist.c:352:	    printf(" ****** Error in MC64A/AD. INFO(1) = " IFMT 
./mc64ad_dist.c:362:	    printf(" ****** Error in MC64A/AD. INFO(1) = " IFMT 
./mc64ad_dist.c:372:	    printf(" ****** Error in MC64A/AD. INFO(1) = " IFMT
./mc64ad_dist.c:397:	    printf(" ****** Error in MC64A/AD. INFO(1) = " IFMT 
./mc64ad_dist.c:421:		printf(" ****** Error in MC64A/AD. INFO(1) = " IFMT 
./mc64ad_dist.c:444:			printf(" ****** Error in MC64A/AD. INFO(1) = " IFMT 
./mc64ad_dist.c:456:			printf(" ****** Error in MC64A/AD. INFO(1) = " IFMT 
./mc64ad_dist.c:470:/* Print diagnostics on input */
./mc64ad_dist.c:472:	printf("  ****** Input parameters for MC64A/AD: JOB = " IFMT ","
./mc64ad_dist.c:474:	printf(" IP(1:N+1)   = ");
./mc64ad_dist.c:476:	    printf(IFMT, ip[j]);
./mc64ad_dist.c:477:	    if (j%8 == 0) printf("\n");
./mc64ad_dist.c:479:	printf("\n IRN(1:NE) = ");
./mc64ad_dist.c:481:	    printf(IFMT, irn[j]);
./mc64ad_dist.c:482:	    if (j%8 == 0) printf("\n");
./mc64ad_dist.c:484:	printf("\n");
./mc64ad_dist.c:487:	    printf(" A(1:NE)     = ");
./mc64ad_dist.c:489:		printf("%f14.4", a[j]);
./mc64ad_dist.c:490:		if (j%4 == 0) printf("\n");
./mc64ad_dist.c:492:	    printf("\n");
./mc64ad_dist.c:512:	printf(" ****** Warning from MC64A/AD. Need to link mc21ad.\n");
./mc64ad_dist.c:629:	    printf(" ****** Warning from MC64A/AD. INFO(1) = " IFMT
./mc64ad_dist.c:636:	    printf(" ****** Warning from MC64A/AD. INFO(1) = " IFMT "\n"
./mc64ad_dist.c:640:/* Print diagnostics on output */
./mc64ad_dist.c:642:	printf(" ****** Output parameters for MC64A/AD: INFO(1:2)  = " IFMT IFMT "\n",
./mc64ad_dist.c:644:	printf(" NUM        = " IFMT, *num);
./mc64ad_dist.c:645:	printf(" CPERM(1:N) = ");
./mc64ad_dist.c:647:	    printf(IFMT, cperm[j]);
./mc64ad_dist.c:648:	    if (j%8 == 0) printf("\n");
./mc64ad_dist.c:651:	    printf("\n DW(1:N)    = ");
./mc64ad_dist.c:653:		printf("%11.3f", dw[j]);
./mc64ad_dist.c:654:		if (j%5 == 0) printf("\n");
./mc64ad_dist.c:656:	    printf("\n DW(N+1:2N) = ");
./mc64ad_dist.c:658:		printf("%11.3f", dw[*n+j]);
./mc64ad_dist.c:659:		if (j%5 == 0) printf("\n");
./mc64ad_dist.c:661:	    printf("\n");
./mc64ad_dist.c:672:	int_t *pr, int_t *q, int_t *l, double *d__)
./mc64ad_dist.c:705:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:709:/* A is a REAL (DOUBLE PRECISION in the D-version) array of length */
./mc64ad_dist.c:717:/* DW is a REAL (DOUBLE PRECISION in D-version) work array of length N. */
./mc64ad_dist.c:723:/*      DOUBLE PRECISION FD05AD, DMACH */
./mc64ad_dist.c:730:    --pr;
./mc64ad_dist.c:746:	pr[k] = ip[k];
./mc64ad_dist.c:806:/* Rescan unassigned columns; improve initial assignment */
./mc64ad_dist.c:823:	    kk1 = pr[jj];
./mc64ad_dist.c:840:	    pr[jj] = kk2 + 1;
./mc64ad_dist.c:848:	pr[jj] = kk + 1;
./mc64ad_dist.c:853:	pr[j] = k + 1;
./mc64ad_dist.c:860:/* Prepare for main loop */
./mc64ad_dist.c:868:/* algorithm for solving the single source shortest path problem */
./mc64ad_dist.c:883:	pr[j] = -1;
./mc64ad_dist.c:913:		pr[jj] = j;
./mc64ad_dist.c:1001:		    pr[jj] = j;
./mc64ad_dist.c:1015:/* Find augmenting path by tracing backward in PR; update IPERM,JPERM */
./mc64ad_dist.c:1024:	    j = pr[j];
./mc64ad_dist.c:1062:/* JPERM, PR are work arrays */
./mc64ad_dist.c:1073:	    pr[k] = i__;
./mc64ad_dist.c:1087:	jdum = pr[k];
./mc64ad_dist.c:1114:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:1193:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:1229:/* Exchange old last element with larger priority child */
./mc64ad_dist.c:1288:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:1428:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:1460:/* KEY is the smallest of two values present in interval [FIRST,LAST) */
./mc64ad_dist.c:1580:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:1584:/* A is a REAL (DOUBLE PRECISION in the D-version) array of length NE. */
./mc64ad_dist.c:1609:/*      DOUBLE PRECISION FD05AD */
./mc64ad_dist.c:1892:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:1904:/* already present in SPLIT(1:NVAL), insert value such that SPLIT */
./mc64ad_dist.c:1927:/* Check presence of HA in SPLIT */
./mc64ad_dist.c:1969:	iperm, int_t *num, int_t *numx, int_t *pr, int_t *arp, 
./mc64ad_dist.c:1986:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:1989:/* PR(J) is the previous column to J in the depth first search. */
./mc64ad_dist.c:1990:/*   Array PR is used as workspace in the sorting algorithm. */
./mc64ad_dist.c:2005:    --pr;
./mc64ad_dist.c:2057:	pr[j] = -1;
./mc64ad_dist.c:2099:		    pr[j] = j1;
./mc64ad_dist.c:2107:		j1 = pr[j];
./mc64ad_dist.c:2134:	    j = pr[j];
./mc64ad_dist.c:2172:	int_t *out, int_t *pr, int_t *q, int_t *l, double *u, 
./mc64ad_dist.c:2204:/* *** Any problems?   Contact ... */
./mc64ad_dist.c:2208:/* A is a REAL (DOUBLE PRECISION in the D-version) array of length NE. */
./mc64ad_dist.c:2218:/* DW is a REAL (DOUBLE PRECISION in the D-version) array of length 2N. */
./mc64ad_dist.c:2230:/*      DOUBLE PRECISION FD05AD */
./mc64ad_dist.c:2238:    --pr;
./mc64ad_dist.c:2256:	pr[k] = ip[k];
./mc64ad_dist.c:2298:/* Scan unassigned columns; improve assignment */
./mc64ad_dist.c:2346:	    kk1 = pr[jj];
./mc64ad_dist.c:2363:	    pr[jj] = kk2 + 1;
./mc64ad_dist.c:2371:	pr[jj] = kk + 1;
./mc64ad_dist.c:2376:	pr[j] = k + 1;
./mc64ad_dist.c:2383:/* Prepare for main loop */
./mc64ad_dist.c:2391:/* algorithm for solving the single source shortest path problem */
./mc64ad_dist.c:2408:	pr[j] = -1;
./mc64ad_dist.c:2455:	    pr[jj] = j;
./mc64ad_dist.c:2542:		    pr[jj] = j;
./mc64ad_dist.c:2554:/* Find augmenting path by tracing backward in PR; update IPERM,JPERM */
./mc64ad_dist.c:2562:	    jj = pr[j];
./memory.c:4:approvals from U.S. Dept. of Energy) 
./memory.c:14: * <pre>
./memory.c:21: * </pre>
./memory.c:35: * Prototype
./memory.c:42: * Internal prototypes
./memory.c:50:    /*fprintf(stderr, msg);
./memory.c:52:    printf("%s", msg);
./memory.c:71:	printf("(%d) superlu_malloc fails: malloc_total %.0f MB, size %lld\n",
./memory.c:116:#else  /* The production mode. */
./memory.c:223: * <pre>
./memory.c:227: * </pre>
./memory.c:234:	*MemModel = USER;   /* user provided space */
./memory.c:247: * <pre>
./memory.c:249: * routines. For those unpredictable size, make a guess as FILL * nnz(A).
./memory.c:254: * </pre>
./memory.c:295:#if ( PRNTlevel>=2 )
./memory.c:296:	printf(".. symbfact_SubInit(): annz %ld, nzlmax %ld, nzumax %ld\n", 
./memory.c:326:		printf("Not enough memory to perform factorization.\n");
./memory.c:329:#if ( PRNTlevel>=1 )
./memory.c:330:	    printf("(%d).. symbfact_SubInit() reduce size:"
./memory.c:379: * <pre>
./memory.c:383: * </pre>
./memory.c:400:    printf("symbfact_SubXpand(): jcol " IFMT ", next " IFMT ", maxlen " IFMT
./memory.c:410:    	fprintf(stderr, "Can't expand MemType %d: jcol " IFMT "\n", mem_type, jcol);
./memory.c:429: * <pre>
./memory.c:432: * </pre>
./memory.c:459: * <pre>
./memory.c:461: * </pre>
./memory.c:467: int_t *prev_len,   /* length used from previous call */
./memory.c:470: int_t keep_prev,   /* = 1: use prev_len;
./memory.c:483:    if ( no_expand == 0 || keep_prev ) /* First time allocate requested */
./memory.c:484:        new_len = *prev_len;
./memory.c:486:	new_len = alpha * *prev_len;
./memory.c:494:	    if ( keep_prev ) {
./memory.c:500:		    new_len = alpha * *prev_len;
./memory.c:517:	    extra = (new_len - *prev_len) * lword;
./memory.c:518:	    if ( keep_prev ) {
./memory.c:524:		    new_len = alpha * *prev_len;
./memory.c:525:		    extra = (new_len - *prev_len) * lword;	    
./memory.c:552:    *prev_len = new_len;
./memory.c:562: * <pre>
./memory.c:570: * </pre>
./mmd.c:4:approvals from U.S. Dept. of Energy) 
./mmd.c:27:/*        ALGORITHM.  IT MAKES USE OF THE IMPLICIT REPRESENTATION */
./mmd.c:41:/*        MAXINT - MAXIMUM MACHINE REPRESENTABLE (SHORT) INTEGER */
./mmd.c:49:/*                 SUBSCRIPTS FOR THE COMPRESSED STORAGE SCHEME. */
./mmd.c:59:/*     PROGRAM SUBROUTINES - */
./mmd.c:317:/*        TRANSFORMS THE QUOTIENT GRAPH REPRESENTATION OF THE */
./mmd.c:322:/*        MAXINT - ESTIMATE OF MAXIMUM REPRESENTABLE (SHORT) */
./mmd.c:569:/*        MAXINT - MAXIMUM MACHINE REPRESENTABLE (SHORT) */
./mmd.c:919:/*        PRODUCING THE PERMUTATION AND INVERSE PERMUTATION */
./old_colamd.c:4:approvals from U.S. Dept. of Energy) 
./old_colamd.c:12: * \brief An approximate minimum degree column ordering algorithm
./old_colamd.c:19:    colamd:  An approximate minimum degree column ordering algorithm.
./old_colamd.c:25:	than A'A.  This also provides a good ordering for sparse partial
./old_colamd.c:26:	pivoting methods, P(AQ) = LU, where Q is computed prior to numerical
./old_colamd.c:34:	prior to LU factorization using sparse partial pivoting, in the
./old_colamd.c:57:	THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY
./old_colamd.c:58:	EXPRESSED OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
./old_colamd.c:60:	Permission is hereby granted to use or copy this program for any
./old_colamd.c:61:	purpose, provided the above notices are retained on all copies.
./old_colamd.c:66:	code and to distribute modified code is granted, provided the above
./old_colamd.c:71:	This software is provided free of charge.
./old_colamd.c:80:	The colamdmex.c file provides a Matlab interface for colamd.
./old_colamd.c:81:	The symamdmex.c file provides a Matlab interface for symamd, which is
./old_colamd.c:93:    Refer to the comments preceding each routine for more details.
./old_colamd.c:134:		are removed prior to ordering.  Columns with more than
./old_colamd.c:136:		prior to ordering, and placed last in the output column
./old_colamd.c:139:		versions may use more knobs.  If so, they will be properly set
./old_colamd.c:198:		duplicate row indices may be be present.  However, colamd will
./old_colamd.c:215:			  order, and no duplicates were present.
./old_colamd.c:246:		is not present (that is, if a (double *) NULL pointer is passed
./old_colamd.c:260:/* colamd.h:  knob array size, stats output size, and global prototypes */
./old_colamd.c:309:	int thickness ;	/* number of original columns represented by this */
./old_colamd.c:324:	int prev ;	/* previous column in degree list, if col is in a */
./old_colamd.c:338:    int length ;	/* number of principal columns in this row */
./old_colamd.c:341:	int degree ;	/* number of principal & non-principal columns in row */
./old_colamd.c:370:#define DEAD_PRINCIPAL		(-1)
./old_colamd.c:371:#define DEAD_NON_PRINCIPAL	(-2)
./old_colamd.c:379:#define COL_IS_DEAD_PRINCIPAL(c)	(Col [c].start == DEAD_PRINCIPAL)
./old_colamd.c:381:#define KILL_PRINCIPAL_COL(c)		{ Col [c].start = DEAD_PRINCIPAL ; }
./old_colamd.c:382:#define KILL_NON_PRINCIPAL_COL(c)	{ Col [c].start = DEAD_NON_PRINCIPAL ; }
./old_colamd.c:384:/* Routines are either PUBLIC (user-callable) or PRIVATE (not user-callable) */
./old_colamd.c:386:#define PRIVATE static
./old_colamd.c:389:/* === Prototypes of PRIVATE routines ======================================= */
./old_colamd.c:392:PRIVATE int init_rows_cols
./old_colamd.c:402:PRIVATE void init_scoring
./old_colamd.c:416:PRIVATE int find_ordering
./old_colamd.c:430:PRIVATE void order_children
./old_colamd.c:437:PRIVATE void detect_super_cols
./old_colamd.c:450:PRIVATE int garbage_collection
./old_colamd.c:460:PRIVATE int clear_mark
./old_colamd.c:477:/* stdio.h:  for printf (no printing if debugging is turned off) */
./old_colamd.c:480:PRIVATE void debug_deg_lists
./old_colamd.c:492:PRIVATE void debug_mark
./old_colamd.c:500:PRIVATE void debug_matrix
./old_colamd.c:509:PRIVATE void debug_structures
./old_colamd.c:520:/* present when debugging */
./old_colamd.c:522:PRIVATE int debug_colamd ;	/* debug print level */
./old_colamd.c:524:#define DEBUG0(params) { (void) printf params ; }
./old_colamd.c:525:#define DEBUG1(params) { if (debug_colamd >= 1) (void) printf params ; }
./old_colamd.c:526:#define DEBUG2(params) { if (debug_colamd >= 2) (void) printf params ; }
./old_colamd.c:527:#define DEBUG3(params) { if (debug_colamd >= 3) (void) printf params ; }
./old_colamd.c:528:#define DEBUG4(params) { if (debug_colamd >= 4) (void) printf params ; }
./old_colamd.c:556:    value has been determined to provide good balance between the number of
./old_colamd.c:606:			prior to ordering.
./old_colamd.c:609:			prior to ordering, and placed last in the column
./old_colamd.c:647:    providing a permutation Q such that the Cholesky factorization
./old_colamd.c:657:    may be present.  However, colamd will work a little faster if columns are
./old_colamd.c:658:    sorted and no duplicates are present.  Matlab 5.2 always passes the matrix
./old_colamd.c:712:				sorted order and no duplicates were present.
./old_colamd.c:750:    debug_colamd = 0 ;		/* no debug printing */
./old_colamd.c:751:    /* get "D" environment variable, which gives the debug printing level */
./old_colamd.c:760:	/* n_row and n_col must be non-negative, A and p must be present */
./old_colamd.c:815:    /* === Order the non-principal columns ================================== */
./old_colamd.c:854:PRIVATE int init_rows_cols	/* returns status code */
./old_colamd.c:874:    int last_start ;		/* start index of previous column in A */
./old_colamd.c:876:    int last_row ;		/* previous row */
./old_colamd.c:895:	Col [col].shared3.prev = EMPTY ;
./old_colamd.c:943:	    /* prevent repeated row from being counted */
./old_colamd.c:1052:	    /* note that the lengths here are for pruned columns, i.e. */
./old_colamd.c:1088:PRIVATE void init_scoring
./old_colamd.c:1112:    int col_length ;		/* length of pruned column */
./old_colamd.c:1137:    /* factorization can proceed as far as possible. */
./old_colamd.c:1145:	    KILL_PRINCIPAL_COL (c) ;
./old_colamd.c:1172:	    KILL_PRINCIPAL_COL (c) ;
./old_colamd.c:1202:    /* pruned in the code below. */
./old_colamd.c:1232:	/* determine pruned column length */
./old_colamd.c:1240:	    KILL_PRINCIPAL_COL (c) ;
./old_colamd.c:1278:	/* only add principal columns to degree lists */
./old_colamd.c:1294:	    /* now add this column to dList at proper score location */
./old_colamd.c:1296:	    Col [c].shared3.prev = EMPTY ;
./old_colamd.c:1300:	    /* previous pointer to this new column */
./old_colamd.c:1303:		Col [next_col].shared3.prev = c ;
./old_colamd.c:1317:    DEBUG0 (("Live cols %d out of %d, non-princ: %d\n",
./old_colamd.c:1336:    Order the principal columns of the supercolumn form of the matrix
./old_colamd.c:1337:    (no supercolumns on input).  Uses a minimum approximate column minimum
./old_colamd.c:1341:PRIVATE int find_ordering	/* return the number of garbage collections */
./old_colamd.c:1386:    int pivot_col_thickness ;	/* number of columns represented by pivot col */
./old_colamd.c:1387:    int prev_col ;		/* Used by Dlist operations. */
./old_colamd.c:1448:	    Col [next_col].shared3.prev = EMPTY ;
./old_colamd.c:1565:	/* === Approximate degree computation =============================== */
./old_colamd.c:1567:	/* Here begins the computation of the approximate degree.  The column */
./old_colamd.c:1571:	/* excluded from the column score (we thus use an approximate */
./old_colamd.c:1575:	/* add them up) is proportional to the size of the data structure */
./old_colamd.c:1578:	/* is proportional to the size of that column (where size, in this */
./old_colamd.c:1608:	    prev_col = Col [col].shared3.prev ;
./old_colamd.c:1613:	    if (prev_col == EMPTY)
./old_colamd.c:1619:		Col [prev_col].shared4.degree_next = next_col ;
./old_colamd.c:1623:		Col [next_col].shared3.prev = prev_col ;
./old_colamd.c:1722:		KILL_PRINCIPAL_COL (col) ;
./old_colamd.c:1732:		/* === Prepare for supercolumn detection ==================== */
./old_colamd.c:1734:		DEBUG2 (("Preparing supercol detection for Col: %d.\n", col)) ;
./old_colamd.c:1748:		    /* degree list "hash" is non-empty, use prev (shared3) of */
./old_colamd.c:1767:	/* The approximate external column degree is now computed.  */
./old_colamd.c:1781:	KILL_PRINCIPAL_COL (pivot_col) ;
./old_colamd.c:1846:	    Col [col].shared3.prev = EMPTY ;
./old_colamd.c:1849:		Col [next_col].shared3.prev = col ;
./old_colamd.c:1877:    /* === All principal columns have now been ordered ====================== */
./old_colamd.c:1888:    The find_ordering routine has ordered all of the principal columns (the
./old_colamd.c:1889:    representatives of the supercolumns).  The non-principal columns have not
./old_colamd.c:1900:PRIVATE void order_children
./old_colamd.c:1916:    /* === Order each non-principal column ================================== */
./old_colamd.c:1920:	/* find an un-ordered non-principal column */
./old_colamd.c:1922:	if (!COL_IS_DEAD_PRINCIPAL (i) && Col [i].shared2.order == EMPTY)
./old_colamd.c:1925:	    /* once found, find its principal parent */
./old_colamd.c:1929:	    } while (!COL_IS_DEAD_PRINCIPAL (parent)) ;
./old_colamd.c:1931:	    /* now, order all un-ordered non-principal columns along path */
./old_colamd.c:1990:    For a column c in a hash bucket, Col [c].shared3.prev is NOT a "previous
./old_colamd.c:1997:    just been computed in the approximate degree computation.
./old_colamd.c:2001:PRIVATE void detect_super_cols
./old_colamd.c:2026:    int prev_c ;		/* column preceding c in hash bucket */
./old_colamd.c:2070:	    /* prev_c is the column preceding column c in the hash bucket */
./old_colamd.c:2071:	    prev_c = super_c ;
./old_colamd.c:2086:		    prev_c = c ;
./old_colamd.c:2110:		    prev_c = c ;
./old_colamd.c:2120:		KILL_NON_PRINCIPAL_COL (c) ;
./old_colamd.c:2124:		Col [prev_c].shared4.hash_next = Col [c].shared4.hash_next ;
./old_colamd.c:2157:PRIVATE int garbage_collection  /* returns the new value of pfree */
./old_colamd.c:2210:    /* === Prepare to defragment the rows =================================== */
./old_colamd.c:2289:PRIVATE int clear_mark	/* return the new value for tag_mark */
./old_colamd.c:2333:PRIVATE void debug_structures
./old_colamd.c:2415:    Prints the contents of the degree lists.  Counts the number of columns
./old_colamd.c:2420:PRIVATE void debug_deg_lists
./old_colamd.c:2494:PRIVATE void debug_mark
./old_colamd.c:2527:    Prints out the contents of the columns and the rows.
./old_colamd.c:2530:PRIVATE void debug_matrix
./old_colamd.h:4:approvals from U.S. Dept. of Energy) 
./old_colamd.h:15:/* === colamd prototypes and definitions ==================================== */
./old_colamd.h:56:/* === Prototypes of user-callable routines ================================= */
./pdGetDiagU.c:4:approvals from U.S. Dept. of Energy) 
./pdGetDiagU.c:11:/*! @file p@(pre)GetDiagU.c
./pdGetDiagU.c:14: * <pre>
./pdGetDiagU.c:18: * Created:  April 16, 2002
./pdGetDiagU.c:20: * </pre>
./pdGetDiagU.c:29: * <pre>
./pdGetDiagU.c:46: *          The 2D process mesh. It contains the MPI communicator, the number
./pdGetDiagU.c:47: *          of process rows (NPROW), the number of process columns (NPCOL),
./pdGetDiagU.c:48: *          and my process rank. It is an input argument to all the
./pdGetDiagU.c:53: *          On exit, it is available on all processes.
./pdGetDiagU.c:60: * data structures, and are on the diagonal processes of the
./pdGetDiagU.c:61: * 2D process grid.
./pdGetDiagU.c:64: * </pre>
./pdGetDiagU.c:72:    int nsupr; /* number of rows in the block L(:,k) (LDA) */
./pdGetDiagU.c:74:    int_t num_diag_procs, *diag_procs, *diag_len;
./pdGetDiagU.c:83:    get_diag_procs(n, Glu_persist, grid, &num_diag_procs,
./pdGetDiagU.c:84:		   &diag_procs, &diag_len);
./pdGetDiagU.c:86:    for (j = 1; j < num_diag_procs; ++j) jj = SUPERLU_MAX( jj, diag_len[j] );
./pdGetDiagU.c:89:    for (p = 0; p < num_diag_procs; ++p) {
./pdGetDiagU.c:90:	pkk = diag_procs[p];
./pdGetDiagU.c:94:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pdGetDiagU.c:97:		nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pdGetDiagU.c:100:		    dwork[lwork+i] = lusup[i*(nsupr+1)];
./pdGetDiagU.c:110:	for (k = p; k < nsupers; k += num_diag_procs) {
./pdGetDiagU.c:118:    SUPERLU_FREE(diag_procs);
./pddistribute.c:4:approvals from U.S. Dept. of Energy) 
./pddistribute.c:14: * \brief Re-distribute A on the 2D process mesh.
./pddistribute.c:15: * <pre>
./pddistribute.c:19: * </pre>
./pddistribute.c:30: * <pre>
./pddistribute.c:33: *   Re-distribute A on the 2D process mesh.
./pddistribute.c:51: *        The 2D process mesh.
./pddistribute.c:61: * </pre>
./pddistribute.c:82:    int    iam, it, p, procs, iam_g;
./pddistribute.c:96:    procs = grid->nprow * grid->npcol;
./pddistribute.c:101:    nnzToRecv = intCalloc_dist(2*procs);
./pddistribute.c:102:    nnzToSend = nnzToRecv + procs;	
./pddistribute.c:105:       COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
./pddistribute.c:111:  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pddistribute.c:115:	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pddistribute.c:127:    for (p = 0; p < procs; ++p) {
./pddistribute.c:137:    k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
./pddistribute.c:149:    if ( procs > 1 ) {
./pddistribute.c:151:	     SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) )
./pddistribute.c:153:      if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) )
./pddistribute.c:155:      if ( !(aij_send = (double **)SUPERLU_MALLOC(procs*sizeof(double*))) )
./pddistribute.c:163:      if ( !(ptr_to_send = intCalloc_dist(procs)) )
./pddistribute.c:172:      for (i = 0, j = 0, p = 0; p < procs; ++p) {
./pddistribute.c:180:    } /* if procs > 1 */
./pddistribute.c:193:  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pddistribute.c:197:	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pddistribute.c:219:    for (p = 0; p < procs; ++p) {
./pddistribute.c:226:	               p, iam+procs, grid->comm, &send_req[procs+p] ); 
./pddistribute.c:230:    for (p = 0; p < procs; ++p) {
./pddistribute.c:235:            MPI_Recv( dtemp, it, MPI_DOUBLE, p, p+procs,
./pddistribute.c:249:    for (p = 0; p < procs; ++p) {
./pddistribute.c:252:	    MPI_Wait( &send_req[procs+p], &status);
./pddistribute.c:262:    if ( procs > 1 ) {
./pddistribute.c:335: *   Distribute the matrix onto the 2D process mesh.
./pddistribute.c:365: *        The 2D process mesh.
./pddistribute.c:383:    int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
./pddistribute.c:385:	int iam, jbrow, kcol, krow, mycol, myrow, pc, pr;
./pddistribute.c:405:	double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pddistribute.c:406:    int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pddistribute.c:409:	RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
./pddistribute.c:411:	RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
./pddistribute.c:422:    int_t  **fsendx_plist; /* Column process list to send down Xk.   */
./pddistribute.c:429:    int_t  **bsendx_plist; /* Column process list to send down Xk.   */
./pddistribute.c:436:    int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./pddistribute.c:437:    int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
./pddistribute.c:438:    int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
./pddistribute.c:439:    int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
./pddistribute.c:441:    int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
./pddistribute.c:442:    int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
./pddistribute.c:443:    int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
./pddistribute.c:444:    int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
./pddistribute.c:471:#if ( PRNTlevel>=1 )
./pddistribute.c:474:#if ( PROFlevel>=1 ) 
./pddistribute.c:487://#if ( PRNTlevel>=1 )
./pddistribute.c:496:#if ( PROFlevel>=1 )
./pddistribute.c:503:#if ( PROFlevel>=1 )
./pddistribute.c:505:    if ( !iam ) printf("--------\n"
./pddistribute.c:511:#if ( PROFlevel>=1 )
./pddistribute.c:514:	/* We can propagate the new values of A into the existing
./pddistribute.c:520:	nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
./pddistribute.c:531:#if ( PRNTlevel>=1 )
./pddistribute.c:534:#if ( PROFlevel>=1 )
./pddistribute.c:551:	    if ( mycol == pc ) { /* Block column jb in my process column */
./pddistribute.c:560:			if ( myrow == PROW( gb, grid ) ) {
./pddistribute.c:592:#if ( PROFlevel>=1 )
./pddistribute.c:623:#if ( PROFlevel>=1 )
./pddistribute.c:632:#if ( PROFlevel>=1 )
./pddistribute.c:633:	if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
./pddistribute.c:642:#if ( PROFlevel>=1 )
./pddistribute.c:646:	 * propagate the values of A into them.
./pddistribute.c:648:	lsub = Glu_freeable->lsub;    /* compressed L subscripts */
./pddistribute.c:650:	usub = Glu_freeable->usub;    /* compressed U subscripts */
./pddistribute.c:663:#if ( PRNTlevel>=1 )
./pddistribute.c:668:	k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pddistribute.c:695:#if ( PRNTlevel>=1 )	
./pddistribute.c:702:	    if ( myrow == PROW( gb, grid ) ) {
./pddistribute.c:710:#if ( PROFlevel>=1 )
./pddistribute.c:715:	   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
./pddistribute.c:732:		    pr = PROW( gb, grid );
./pddistribute.c:735:			if  ( myrow == pr ) {
./pddistribute.c:744:#if ( PRNTlevel>=1 )
./pddistribute.c:756:	nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pddistribute.c:785:#if ( PROFlevel>=1 )
./pddistribute.c:787:	if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
./pddistribute.c:789:#if ( PRNTlevel>=1 )
./pddistribute.c:813:#if ( PRNTlevel>=1 )	
./pddistribute.c:833:		fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
./pddistribute.c:837:		fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
./pddistribute.c:847:	/* These lists of processes will be used for triangular solves. */
./pddistribute.c:850:	len = k * grid->nprow;
./pddistribute.c:854:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./pddistribute.c:861:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./pddistribute.c:864:#if ( PRNTlevel>=1 )
./pddistribute.c:869:	  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
./pddistribute.c:870:	  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
./pddistribute.c:875:	    if ( mycol == pc ) { /* Block column jb in my process column */
./pddistribute.c:885:			if ( myrow == PROW( gb, grid ) ) {
./pddistribute.c:894:		jbrow = PROW( jb, grid );
./pddistribute.c:899:#if ( PROFlevel>=1 )
./pddistribute.c:912:			pr = PROW( gb, grid );
./pddistribute.c:913:			if ( pr != jbrow &&
./pddistribute.c:914:			     myrow == jbrow &&  /* diag. proc. owning jb */
./pddistribute.c:915:			     bsendx_plist[ljb][pr] == EMPTY ) {
./pddistribute.c:916:			    bsendx_plist[ljb][pr] = YES;
./pddistribute.c:919:			if ( myrow == pr ) {
./pddistribute.c:957:			} /* if myrow == pr ... */
./pddistribute.c:962:#if ( PROFlevel>=1 )
./pddistribute.c:978:		    pr = PROW( gb, grid ); /* Process row owning this block */
./pddistribute.c:979:		    if ( pr != jbrow &&
./pddistribute.c:980:			 myrow == jbrow &&  /* diag. proc. owning jb */
./pddistribute.c:981:			 fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
./pddistribute.c:982:			fsendx_plist[ljb][pr] = YES;
./pddistribute.c:985:		    if ( myrow == pr ) {
./pddistribute.c:997:#if ( PRNTlevel>=1 )
./pddistribute.c:1044:		    /* Propagate the compressed row subscripts to Lindex[],
./pddistribute.c:1050:			if ( myrow == PROW( gb, grid ) ) {
./pddistribute.c:1071:				krow = PROW( jb, grid );
./pddistribute.c:1122:			// printf("iam %5d Lindval %5d\n",iam, Lindval_loc_bc_ptr[ljb][jj]);
./pddistribute.c:1126:			// printf("iam %5d Lindval %5d\n",iam, index[Lindval_loc_bc_ptr[ljb][jj+nrbl]]);
./pddistribute.c:1137:#if ( PROFlevel>=1 )
./pddistribute.c:1157:	nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
./pddistribute.c:1216:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pddistribute.c:1229:#if ( PROFlevel>=1 )
./pddistribute.c:1237:	if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pddistribute.c:1239:	if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pddistribute.c:1256:	if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pddistribute.c:1258:	for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
./pddistribute.c:1270:			pr = PROW( gb, grid );
./pddistribute.c:1271:			ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
./pddistribute.c:1282:		for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pddistribute.c:1283:		for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pddistribute.c:1284:		for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pddistribute.c:1288:		for (j=0;j<grid->nprow;++j){
./pddistribute.c:1291:			pr = PROW( gb, grid );
./pddistribute.c:1292:			if(gb==jb)Root=pr;
./pddistribute.c:1293:			if(myrow==pr)Iactive=1;		
./pddistribute.c:1298:		quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
./pddistribute.c:1301:			// printf("jb %5d damn\n",jb);
./pddistribute.c:1306:			for (j = 0; j < grid->nprow; ++j){
./pddistribute.c:1307:				if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pddistribute.c:1308:					ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pddistribute.c:1324:				// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pddistribute.c:1328:				// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb],'d'));
./pddistribute.c:1332:				// #if ( PRNTlevel>=1 )		
./pddistribute.c:1335:					for (j = 0; j < grid->nprow; ++j) {
./pddistribute.c:1342:					// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
./pddistribute.c:1344:					// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
./pddistribute.c:1345:					// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pddistribute.c:1346:					// // printf("\n");
./pddistribute.c:1361:#if ( PROFlevel>=1 )
./pddistribute.c:1363:if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
./pddistribute.c:1367:#if ( PROFlevel>=1 )
./pddistribute.c:1372:	nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pddistribute.c:1380:		pr = PROW( k, grid );
./pddistribute.c:1381:		if ( myrow == pr ) {
./pddistribute.c:1388:	/* Every process receives the count, but it is only useful on the
./pddistribute.c:1389:	   diagonal processes.  */
./pddistribute.c:1394:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pddistribute.c:1452:			pr = PROW( ib, grid );
./pddistribute.c:1453:			if ( myrow == pr ) { /* Block row ib in my process row */
./pddistribute.c:1462:		ib = myrow+lib*grid->nprow;  /* not sure */
./pddistribute.c:1464:			pr = PROW( ib, grid );
./pddistribute.c:1496:						ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pddistribute.c:1508:					// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
./pddistribute.c:1514:					// printf("iam %5d rtree lk %5d tag %5d root %5d\n",iam,lib,ib,RdTree_IsRoot(LRtree_ptr[lib],'d'));
./pddistribute.c:1519:					// #if ( PRNTlevel>=1 )
./pddistribute.c:1522:					// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pddistribute.c:1523:					// // printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pddistribute.c:1524:					// // // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pddistribute.c:1525:					// // printf("\n");
./pddistribute.c:1549:#if ( PROFlevel>=1 )
./pddistribute.c:1551:if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
./pddistribute.c:1554:#if ( PROFlevel>=1 )
./pddistribute.c:1563:	if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pddistribute.c:1565:	if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pddistribute.c:1581:	if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pddistribute.c:1583:	for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
./pddistribute.c:1598:				pr = PROW( gb, grid );
./pddistribute.c:1599:				ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],gb);
./pddistribute.c:1600:			// printf("gb:%5d jb: %5d nsupers: %5d\n",gb,jb,nsupers);
./pddistribute.c:1602:				//if(gb==jb)Root=pr;
./pddistribute.c:1607:		pr = PROW( jb, grid ); // take care of diagonal node stored as L
./pddistribute.c:1608:		// printf("jb %5d current: %5d",jb,ActiveFlagAll[pr+ljb*grid->nprow]);
./pddistribute.c:1610:		ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],jb);	
./pddistribute.c:1620:		// if ( mycol == pc ) { /* Block column jb in my process column */
./pddistribute.c:1622:		for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pddistribute.c:1623:		for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pddistribute.c:1624:		for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pddistribute.c:1628:		for (j=0;j<grid->nprow;++j){
./pddistribute.c:1631:			pr = PROW( gb, grid );
./pddistribute.c:1632:			if(gb==jb)Root=pr;
./pddistribute.c:1633:			if(myrow==pr)Iactive=1;		
./pddistribute.c:1637:		quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
./pddistribute.c:1638:	// printf("jb: %5d Iactive %5d\n",jb,Iactive);
./pddistribute.c:1641:			// printf("root:%5d jb: %5d\n",Root,jb);
./pddistribute.c:1646:			for (j = 0; j < grid->nprow; ++j){
./pddistribute.c:1647:				if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pddistribute.c:1648:					ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pddistribute.c:1652:	// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
./pddistribute.c:1664:				// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pddistribute.c:1669:				for (j = 0; j < grid->nprow; ++j) {
./pddistribute.c:1670:					// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
./pddistribute.c:1676:				// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
./pddistribute.c:1689:#if ( PROFlevel>=1 )
./pddistribute.c:1691:if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
./pddistribute.c:1694:#if ( PROFlevel>=1 )
./pddistribute.c:1699:	nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pddistribute.c:1707:		pr = PROW( k, grid );
./pddistribute.c:1708:		if ( myrow == pr ) {
./pddistribute.c:1715:	/* Every process receives the count, but it is only useful on the
./pddistribute.c:1716:	   diagonal processes.  */
./pddistribute.c:1721:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pddistribute.c:1804:				pr = PROW( ib, grid );
./pddistribute.c:1805:				if ( myrow == pr ) { /* Block row ib in my process row */
./pddistribute.c:1812:		pr = PROW( jb, grid );
./pddistribute.c:1813:		if ( myrow == pr ) { /* Block row ib in my process row */
./pddistribute.c:1821:		ib = myrow+lib*grid->nprow;  /* not sure */
./pddistribute.c:1823:			pr = PROW( ib, grid );
./pddistribute.c:1854:						ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pddistribute.c:1866:					// #if ( PRNTlevel>=1 )
./pddistribute.c:1868:					// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
./pddistribute.c:1871:					// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pddistribute.c:1872:					// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pddistribute.c:1873:					// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pddistribute.c:1874:					// printf("\n");
./pddistribute.c:1895:#if ( PROFlevel>=1 )
./pddistribute.c:1897:if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
./pddistribute.c:1934:#if ( PRNTlevel>=1 )
./pddistribute.c:1935:	if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
./pddistribute.c:1953:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pddistribute.c:1957:#if ( PROFlevel>=1 )
./pddistribute.c:1958:	if ( !iam ) printf(".. 1st distribute time:\n "
./pddistribute.c:1966:    if ( xa[A->ncol] > 0 ) { /* may not have any entries on this process. */
./pdgsequ.c:4:approvals from U.S. Dept. of Energy) 
./pdgsequ.c:24: <pre>    
./pdgsequ.c:37:    works well in practice.   
./pdgsequ.c:80:            The 2D process mesh.
./pdgsequ.c:82:</pre>
./pdgsequ.c:100:    int_t  procs;
./pdgsequ.c:217:    /* gather R from each process to get the global R.  */
./pdgsequ.c:219:    procs = grid->nprow * grid->npcol;
./pdgsequ.c:220:    if ( !(r_sizes = SUPERLU_MALLOC(2 * procs * sizeof(int))))
./pdgsequ.c:222:    displs = r_sizes + procs;
./pdgsequ.c:233:    for (i = 1; i < procs; ++i) displs[i] = displs[i-1] + r_sizes[i-1];
./pdgsmv.c:4:approvals from U.S. Dept. of Energy) 
./pdgsmv.c:16: * <pre>
./pdgsmv.c:20: * </pre>
./pdgsmv.c:31: int_t *row_to_proc,   /* Input. Mapping between rows and processes. */
./pdgsmv.c:37:    int iam, p, procs;
./pdgsmv.c:57:    procs = grid->nprow * grid->npcol;
./pdgsmv.c:66:    if ( !(SendCounts = SUPERLU_MALLOC(2*procs * sizeof(int))) )
./pdgsmv.c:68:    /*for (i = 0; i < 2*procs; ++i) SendCounts[i] = 0;*/
./pdgsmv.c:69:    RecvCounts = SendCounts + procs;
./pdgsmv.c:70:    if ( !(ptr_ind_tosend = intMalloc_dist(2*(procs+1))) )
./pdgsmv.c:72:    ptr_ind_torecv = ptr_ind_tosend + procs + 1;
./pdgsmv.c:78:       COUNT THE NUMBER OF X ENTRIES TO BE SENT TO EACH PROCESS.
./pdgsmv.c:86:    for (p = 0; p < procs; ++p) SendCounts[p] = 0;
./pdgsmv.c:91:            p = row_to_proc[jcol];
./pdgsmv.c:112:       LOAD THE X-INDICES TO BE SENT TO THE OTHER PROCESSES.
./pdgsmv.c:117:    for (p = 0, TotalIndSend = 0; p < procs; ++p) {
./pdgsmv.c:135:	        p = row_to_proc[jcol];
./pdgsmv.c:167:    for (p = 0, TotalValSend = 0; p < procs; ++p) {
./pdgsmv.c:177:	   SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))))
./pdgsmv.c:179:    recv_req = send_req + procs;
./pdgsmv.c:180:    for (p = 0; p < procs; ++p) {
./pdgsmv.c:191:    for (p = 0; p < procs; ++p) {
./pdgsmv.c:220:    PrintInt10("pdgsmv_init::rowptr", m_loc+1, rowptr);
./pdgsmv.c:221:    PrintInt10("pdgsmv_init::extern_start", m_loc, extern_start);
./pdgsmv.c:249:    int iam, procs;
./pdgsmv.c:268:    procs = grid->nprow * grid->npcol;
./pdgsmv.c:299:	   SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))))
./pdgsmv.c:301:    recv_req = send_req + procs;
./pdgsmv.c:302:    for (p = 0; p < procs; ++p) {
./pdgsmv.c:328:        for (p = 0; p < procs; ++p) {
./pdgsmv.c:350:        for (p = 0; p < procs; ++p) {
./pdgsmv_AXglobal.c:4:approvals from U.S. Dept. of Energy) 
./pdgsmv_AXglobal.c:16: * <pre>
./pdgsmv_AXglobal.c:20: * </pre>
./pdgsmv_AXglobal.c:29:static void dPrintMSRmatrix(int, double [], int_t [], gridinfo_t *);
./pdgsmv_AXglobal.c:43: int_t *mv_sup_to_proc /* output */
./pdgsmv_AXglobal.c:48:    int N_update;    /* Number of variables updated on this process (output) */
./pdgsmv_AXglobal.c:50:    int nprocs = grid->nprow * grid->npcol;
./pdgsmv_AXglobal.c:66:	PrintInt10("xsup", supno[n-1]+1, xsup);
./pdgsmv_AXglobal.c:67:	PrintInt10("supno", n, supno);
./pdgsmv_AXglobal.c:73:	/* Figure out mv_sup_to_proc[] on all processes. */
./pdgsmv_AXglobal.c:74:	for (p = 0; p < nprocs; ++p) {
./pdgsmv_AXglobal.c:75:	    t1 = n / nprocs;       /* Number of rows */
./pdgsmv_AXglobal.c:76:	    t2 = n - t1 * nprocs;  /* left-over, which will be assigned
./pdgsmv_AXglobal.c:77:				      to the first t2 processes.  */
./pdgsmv_AXglobal.c:79:	    else { /* First t2 processes will get one more row. */
./pdgsmv_AXglobal.c:97:		    mv_sup_to_proc[i] = p;
./pdgsmv_AXglobal.c:99:		    if ( mv_sup_to_proc[i] == p-1 ) {
./pdgsmv_AXglobal.c:100:			fprintf(stderr, 
./pdgsmv_AXglobal.c:101:				"mv_sup_to_proc conflicts at supno %d\n", i);
./pdgsmv_AXglobal.c:116:		printf("(%2d) N_update = %4d\t"
./pdgsmv_AXglobal.c:126:	t1 = nsupers / nprocs;
./pdgsmv_AXglobal.c:127:	t2 = nsupers - t1 * nprocs; /* left-over */
./pdgsmv_AXglobal.c:144:    PrintInt10("mv_sup_to_proc", nsupers, mv_sup_to_proc);
./pdgsmv_AXglobal.c:145:    dPrintMSRmatrix(N_update, *val, *bindx, grid);
./pdgsmv_AXglobal.c:155: * <pre>
./pdgsmv_AXglobal.c:166: * </pre> 
./pdgsmv_AXglobal.c:251: * <pre>
./pdgsmv_AXglobal.c:255: *   - ax product is distributed the same way as A
./pdgsmv_AXglobal.c:256: * </pre>
./pdgsmv_AXglobal.c:282: *   - ax product is distributed the same way as A
./pdgsmv_AXglobal.c:305: * Print the local MSR matrix
./pdgsmv_AXglobal.c:307:static void dPrintMSRmatrix
./pdgsmv_AXglobal.c:321:    printf("(%2d) MSR submatrix has %d rows -->\n", iam, m);
./pdgsmv_AXglobal.c:322:    PrintDouble5("val", nnzp1, val);
./pdgsmv_AXglobal.c:323:    PrintInt10("bindx", nnzp1, bindx);
./pdgsrfs.c:4:approvals from U.S. Dept. of Energy) 
./pdgsrfs.c:14: * \brief Improves the computed solution to a system of linear equations and provides error bounds and backward error estimates
./pdgsrfs.c:16: * <pre>
./pdgsrfs.c:23: * </pre>
./pdgsrfs.c:31: * <pre>
./pdgsrfs.c:35: * PDGSRFS improves the computed solution to a system of linear   
./pdgsrfs.c:36: * equations and provides error bounds and backward error estimates
./pdgsrfs.c:65: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgsrfs.c:66: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgsrfs.c:67: *        and my process rank. It is an input argument to all the
./pdgsrfs.c:81: *            transformed system A1*Y = Pc*Pr*B. where
./pdgsrfs.c:82: *            A1 = Pc*Pr*diag(R)*A*diag(C)*Pc' and Y = Pc*diag(C)^(-1)*X.
./pdgsrfs.c:83: *        On exit, the improved solution matrix Y.
./pdgsrfs.c:86: *        Y should be permutated by Pc^T, and premultiplied by diag(C)
./pdgsrfs.c:117: * </pre>
./pdgsrfs.c:190:    if ( !iam ) printf(".. eps = %e\tanorm = %e\tsafe1 = %e\tsafe2 = %e\n",
./pdgsrfs.c:230:#if ( PRNTlevel>= 1 )
./pdgsrfs.c:232:		printf("(%2d) .. Step " IFMT ": berr[j] = %e\n", iam, count, berr[j]);
./pdgsrfs_ABXglobal.c:4:approvals from U.S. Dept. of Energy) 
./pdgsrfs_ABXglobal.c:14: * \brief Improves the computed solution and provies error bounds
./pdgsrfs_ABXglobal.c:16: * <pre>
./pdgsrfs_ABXglobal.c:23: * </pre>
./pdgsrfs_ABXglobal.c:29:/*-- Function prototypes --*/
./pdgsrfs_ABXglobal.c:38: * <pre>
./pdgsrfs_ABXglobal.c:42: * pdgsrfs_ABXglobal improves the computed solution to a system of linear   
./pdgsrfs_ABXglobal.c:43: * equations and provides error bounds and backward error estimates
./pdgsrfs_ABXglobal.c:54: *        A is also permuted into the form Pc*Pr*A*Pc', where Pr and Pc
./pdgsrfs_ABXglobal.c:58: *        NOTE: Currently, A must reside in all processes when calling
./pdgsrfs_ABXglobal.c:72: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgsrfs_ABXglobal.c:73: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgsrfs_ABXglobal.c:74: *        and my process rank. It is an input argument to all the
./pdgsrfs_ABXglobal.c:83: *        NOTE: Currently, B must reside on all processes when calling
./pdgsrfs_ABXglobal.c:91: *        On exit, the improved solution matrix X.
./pdgsrfs_ABXglobal.c:92: *        If DiagScale = COL or BOTH, X should be premultiplied by diag(C)
./pdgsrfs_ABXglobal.c:95: *        NOTE: Currently, X must reside on all processes when calling
./pdgsrfs_ABXglobal.c:121: * </pre>
./pdgsrfs_ABXglobal.c:138:    int_t  N_update; /* Number of variables updated on this process */
./pdgsrfs_ABXglobal.c:140:			on this processor.                     */
./pdgsrfs_ABXglobal.c:143:    int_t *mv_sup_to_proc;  /* Supernode to process mapping in
./pdgsrfs_ABXglobal.c:149:          nprow, nsupers, nz, p;
./pdgsrfs_ABXglobal.c:156:    int_t num_diag_procs, *diag_procs; /* Record diagonal process numbers. */
./pdgsrfs_ABXglobal.c:157:    int_t *diag_len; /* Length of the X vector on diagonal processes. */
./pdgsrfs_ABXglobal.c:159:    /*-- Function prototypes --*/
./pdgsrfs_ABXglobal.c:185:    nprow = grid->nprow;
./pdgsrfs_ABXglobal.c:194:    get_diag_procs(n, Glu_persist, grid, &num_diag_procs,
./pdgsrfs_ABXglobal.c:195:		   &diag_procs, &diag_len);
./pdgsrfs_ABXglobal.c:196:#if ( PRNTlevel>=1 )
./pdgsrfs_ABXglobal.c:198:	printf(".. number of diag processes = " IFMT "\n", num_diag_procs);
./pdgsrfs_ABXglobal.c:199:	PrintInt10("diag_procs", num_diag_procs, diag_procs);
./pdgsrfs_ABXglobal.c:200:	PrintInt10("diag_len", num_diag_procs, diag_len);
./pdgsrfs_ABXglobal.c:204:    if ( !(mv_sup_to_proc = intCalloc_dist(nsupers)) )
./pdgsrfs_ABXglobal.c:205:	ABORT("Calloc fails for mv_sup_to_proc[]");
./pdgsrfs_ABXglobal.c:208:		          &val, &bindx, mv_sup_to_proc);
./pdgsrfs_ABXglobal.c:210:    i = CEILING( nsupers, nprow ); /* Number of local block rows */
./pdgsrfs_ABXglobal.c:214:    for (j = 1; j < num_diag_procs; ++j) jj = SUPERLU_MAX( jj, diag_len[j] );
./pdgsrfs_ABXglobal.c:238:	PrintDouble5("Mult A*x", N_update, ax);
./pdgsrfs_ABXglobal.c:255:    if ( !iam ) printf(".. eps = %e\tanorm = %e\tsafe1 = %e\tsafe2 = %e\n",
./pdgsrfs_ABXglobal.c:264:	/* Copy X into x on the diagonal processes. */
./pdgsrfs_ABXglobal.c:267:	for (p = 0; p < num_diag_procs; ++p) {
./pdgsrfs_ABXglobal.c:268:	    pkk = diag_procs[p];
./pdgsrfs_ABXglobal.c:270:		for (k = p; k < nsupers; k += num_diag_procs) {
./pdgsrfs_ABXglobal.c:276:		    dx_trs[ii-XK_H] = k;/* Block number prepended in header. */
./pdgsrfs_ABXglobal.c:280:	/* Copy B into b distributed the same way as matrix-vector product. */
./pdgsrfs_ABXglobal.c:313:#if ( PRNTlevel>= 1 )
./pdgsrfs_ABXglobal.c:315:		printf("(%2d) .. Step " IFMT ": berr[j] = %e\n", iam, count, berr[j]);
./pdgsrfs_ABXglobal.c:320:				   mv_sup_to_proc, dx_trs);
./pdgsrfs_ABXglobal.c:324:		for (p = 0; p < num_diag_procs; ++p) 
./pdgsrfs_ABXglobal.c:325:		    if ( iam == diag_procs[p] )
./pdgsrfs_ABXglobal.c:326:			for (k = p; k < nsupers; k += num_diag_procs) {
./pdgsrfs_ABXglobal.c:335:		/* Transfer x_trs (on diagonal processes) into X
./pdgsrfs_ABXglobal.c:336:		   (on all processes). */
./pdgsrfs_ABXglobal.c:338:					num_diag_procs, diag_procs, diag_len,
./pdgsrfs_ABXglobal.c:351:    SUPERLU_FREE(diag_procs);
./pdgsrfs_ABXglobal.c:358:    SUPERLU_FREE(mv_sup_to_proc);
./pdgsrfs_ABXglobal.c:370: * <pre>
./pdgsrfs_ABXglobal.c:372: * matrix-vector product.
./pdgsrfs_ABXglobal.c:373: * </pre>
./pdgsrfs_ABXglobal.c:377:		   LocalLU_t *Llu, gridinfo_t *grid, int_t mv_sup_to_proc[],
./pdgsrfs_ABXglobal.c:392:	pkk = PNUM( PROW( k, grid ), PCOL( k, grid ), grid );
./pdgsrfs_ABXglobal.c:393:	psrc = mv_sup_to_proc[k];
./pdgsrfs_ABXglobal.c:418: * <pre>
./pdgsrfs_ABXglobal.c:419: * Gather the components of x vector on the diagonal processes
./pdgsrfs_ABXglobal.c:420: * onto all processes, and combine them into the global vector y.
./pdgsrfs_ABXglobal.c:421: * </pre>
./pdgsrfs_ABXglobal.c:426:			gridinfo_t *grid, int_t num_diag_procs,
./pdgsrfs_ABXglobal.c:427:			int_t diag_procs[], int_t diag_len[],
./pdgsrfs_ABXglobal.c:439:    for (p = 0; p < num_diag_procs; ++p) {
./pdgsrfs_ABXglobal.c:440:	pkk = diag_procs[p];
./pdgsrfs_ABXglobal.c:444:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pdgsrfs_ABXglobal.c:457:	for (k = p; k < nsupers; k += num_diag_procs) {
./pdgssvx.c:4:approvals from U.S. Dept. of Energy) 
./pdgssvx.c:16: * <pre>
./pdgssvx.c:22: * April 5, 2015
./pdgssvx.c:25: * April 10, 2018  version 5.3
./pdgssvx.c:27: * </pre>
./pdgssvx.c:35: * <pre>
./pdgssvx.c:45: * to run accurately and efficiently on large numbers of processors.
./pdgssvx.c:70: *        m_loc is the number of rows local to this processor
./pdgssvx.c:81: *      -  grid, a structure describing the 2D processor mesh
./pdgssvx.c:83: *            improve the accuracy of the computed solution using 
./pdgssvx.c:91: *      are used when A is sufficiently similar to a previously 
./pdgssvx.c:92: *      solved problem to save time by reusing part or all of 
./pdgssvx.c:93: *      the previous factorization.)
./pdgssvx.c:106: *                             condition number and so improve the
./pdgssvx.c:145: *                A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgssvx.c:147: *               (Note that A1 = Pc*Pr*Aout, where Aout is the matrix stored
./pdgssvx.c:154: *            the same nonzero pattern as a previously factored matrix. In
./pdgssvx.c:155: *            this case the algorithm saves time by reusing the previously
./pdgssvx.c:168: *      previous column permutation from ScalePermstruct->perm_c is used as
./pdgssvx.c:188: *      all of the previously computed structure of L and U.
./pdgssvx.c:191: *            assuming not only the same nonzero pattern as the previously
./pdgssvx.c:207: *        o  ScalePermstruct->DiagScale, how the previous matrix was row
./pdgssvx.c:209: *        o  ScalePermstruct->R, the row scalings of the previous matrix,
./pdgssvx.c:211: *        o  ScalePermstruct->C, the columns scalings of the previous matrix, 
./pdgssvx.c:213: *        o  ScalePermstruct->perm_r, the row permutation of the previous
./pdgssvx.c:215: *        o  ScalePermstruct->perm_c, the column permutation of the previous 
./pdgssvx.c:217: *        o  all of LUstruct, the previously computed information about
./pdgssvx.c:231: *      identical to a matrix that has already been factored on a previous 
./pdgssvx.c:234: *      -  options->Fact = Factored: A is identical to a previously
./pdgssvx.c:235: *            factorized matrix, so the entire previous factorization
./pdgssvx.c:246: *              A from the previous call, so that it has been scaled and permuted)
./pdgssvx.c:264: *           be factorized based on the previous history.
./pdgssvx.c:277: *             pattern was performed prior to this one. Therefore, this
./pdgssvx.c:294: *             prior to this one. Therefore, this factorization will reuse
./pdgssvx.c:297: *             distributed data structure set up from the previous symbolic
./pdgssvx.c:327: *           = LargeDiag_APWM: use the parallel approximate-weight perfect
./pdgssvx.c:349: *           = SLU_DOUBLE: accumulate residual in double precision.
./pdgssvx.c:350: *           = SLU_EXTRA:  accumulate residual in extra precision.
./pdgssvx.c:352: *         NOTE: all options must be identical on all processes when
./pdgssvx.c:359: *           That is, A is stored in distributed compressed row format.
./pdgssvx.c:370: *           performed on the matrix Pc*Pr*diag(R)*A*diag(C)*Pc^T.
./pdgssvx.c:380: *           = ROW:     row equilibration, i.e., A was premultiplied by
./pdgssvx.c:391: *           Row permutation vector, which defines the permutation matrix Pr;
./pdgssvx.c:392: *           perm_r[i] = j means row i of A is in position j in Pr*A.
./pdgssvx.c:404: *           On exit, perm_c may be overwritten by the product of the input
./pdgssvx.c:428: *           process and is defined in the data structure of matrix A.
./pdgssvx.c:440: *         The 2D process mesh. It contains the MPI communicator, the number
./pdgssvx.c:441: *         of process rows (NPROW), the number of process columns (NPCOL),
./pdgssvx.c:442: *         and my process rank. It is an input argument to all the
./pdgssvx.c:464: *           Global data structure (xsup, supno) replicated on all processes,
./pdgssvx.c:501: * </pre>
./pdgssvx.c:520:	       replicated on all processrs.
./pdgssvx.c:521:	           (lsub, xlsub) contains the compressed subscript of
./pdgssvx.c:523:          	   (usub, xusub) contains the compressed subscript of
./pdgssvx.c:554:    int_t lk,k,knsupc,nsupr;
./pdgssvx.c:557:#if ( PRNTlevel>= 2 )
./pdgssvx.c:558:    double   dmin, dsum, dprod;
./pdgssvx.c:563:    int   noDomains, nprocs_num;
./pdgssvx.c:596:	printf("ERROR: Extra precise iterative refinement yet to support.\n");
./pdgssvx.c:606:	printf("ERROR: Relaxation (NREL) cannot be larger than max. supernode size (NSUP).\n"
./pdgssvx.c:629:    /* The following arrays are replicated on all processes. */
./pdgssvx.c:715:#if ( PRNTlevel>=1 )
./pdgssvx.c:716:		    fprintf(stderr, "The " IFMT "-th row of A is exactly zero\n", iinfo);
./pdgssvx.c:719:#if ( PRNTlevel>=1 )
./pdgssvx.c:720:                    fprintf(stderr, "The " IFMT "-th column of A is exactly zero\n", iinfo-n);
./pdgssvx.c:743:#if ( PRNTlevel>=1 )
./pdgssvx.c:745:		printf(".. equilibrated? *equed = %c\n", *equed);
./pdgssvx.c:760:	 * compressed row format to global A in compressed column format.
./pdgssvx.c:770:            pdCompRow_loc_to_CompCol_global(need_value, A, grid, &GA);
./pdgssvx.c:785:           Find the row permutation Pr for A, and apply Pr*[GA].
./pdgssvx.c:786:	   GA is overwritten by Pr*[GA].
./pdgssvx.c:807:	            if ( !iam ) { /* Process 0 finds a row permutation */
./pdgssvx.c:834:#if ( PRNTlevel>=2 )
./pdgssvx.c:837:	            dprod = 1.0;
./pdgssvx.c:854:#if ( PRNTlevel>=2 )
./pdgssvx.c:861:				        dprod *= fabs(a[i]);
./pdgssvx.c:880:                        /* Now permute global GA to prepare for symbfact() */
./pdgssvx.c:901:#if ( PRNTlevel>=2 )
./pdgssvx.c:903:		        if ( !iam ) printf("\tsmallest diagonal %e\n", dmin);
./pdgssvx.c:905:		        if ( !iam ) printf("\tsum of diagonal %e\n", dsum);
./pdgssvx.c:907:		        if ( !iam ) printf("\t product of diagonal %e\n", dprod);
./pdgssvx.c:915:		        printf("CombBLAS is not available\n"); fflush(stdout);
./pdgssvx.c:922:#if ( PRNTlevel>=1 )
./pdgssvx.c:924:		    printf(".. LDPERM job " IFMT "\t time: %.2f\n", job, t);
./pdgssvx.c:935:        if ( !iam ) PrintInt10("perm_r",  m, perm_r);
./pdgssvx.c:944:#if ( PRNTlevel>=1 )
./pdgssvx.c:945:	if ( !iam ) { printf(".. anorm %e\n", anorm); 	fflush(stdout); }
./pdgssvx.c:967:	    nprocs_num = grid->nprow * grid->npcol;
./pdgssvx.c:968:  	    noDomains = (int) ( pow(2, ((int) LOG2( nprocs_num ))));
./pdgssvx.c:971:               processes in grid->comm */
./pdgssvx.c:992:		printf("{" IFMT "," IFMT "}: pdgssvx: invalid ColPerm option when ParSymbfact is used\n",
./pdgssvx.c:1000:	// #pragma omp parallel  
./pdgssvx.c:1002:	// #pragma omp master
./pdgssvx.c:1009:	      flinfo = get_perm_c_parmetis(A, perm_r, perm_c, nprocs_num,
./pdgssvx.c:1015:#if ( PRNTlevel>=1 )
./pdgssvx.c:1016:	          fprintf(stderr, "Insufficient memory for get_perm_c parmetis\n");
./pdgssvx.c:1034:		/* GA = Pr*A, perm_r[] is already applied. */
./pdgssvx.c:1040:	        /* Form Pc*A*Pc^T to preserve the diagonal of the matrix GAC. */
./pdgssvx.c:1052:	        /* Perform a symbolic factorization on Pc*Pr*A*Pc^T and set up
./pdgssvx.c:1054:#if ( PRNTlevel>=1 ) 
./pdgssvx.c:1056:		    printf(".. symbfact(): relax " IFMT ", maxsuper " IFMT ", fill " IFMT "\n",
./pdgssvx.c:1066:	    	/* Every process does this. */
./pdgssvx.c:1073:#if ( PRNTlevel>=1 )
./pdgssvx.c:1075:		    	printf("\tNo of supers " IFMT "\n", Glu_persist->supno[n-1]+1);
./pdgssvx.c:1076:		    	printf("\tSize of G(L) " IFMT "\n", Glu_freeable->xlsub[n]);
./pdgssvx.c:1077:		    	printf("\tSize of G(U) " IFMT "\n", Glu_freeable->xusub[n]);
./pdgssvx.c:1078:		    	printf("\tint %d, short %d, float %d, double %d\n", 
./pdgssvx.c:1081:		    	printf("\tSYMBfact (MB):\tL\\U %.2f\ttotal %.2f\texpansions " IFMT "\n",
./pdgssvx.c:1089:#if ( PRNTlevel>=1 )
./pdgssvx.c:1091:		        fprintf(stderr,"symbfact() error returns " IFMT "\n",iinfo);
./pdgssvx.c:1099:	    	flinfo = symbfact_dist(nprocs_num, noDomains, A, perm_c, perm_r,
./pdgssvx.c:1106:#if ( PRNTlevel>=1 )
./pdgssvx.c:1107:	      	    fprintf(stderr, "Insufficient memory for parallel symbolic factorization.");
./pdgssvx.c:1132:	    /* Distribute Pc*Pr*diag(R)*A*diag(C)*Pc^T into L and U storage. 
./pdgssvx.c:1133:	       NOTE: the row permutation Pc*Pr is applied internally in the
./pdgssvx.c:1146:	    /* Distribute Pc*Pr*diag(R)*A*diag(C)*Pc' into L and U storage. 
./pdgssvx.c:1147:	       NOTE: the row permutation Pc*Pr is applied internally in the
./pdgssvx.c:1161:	/*if (!iam) printf ("\tDISTRIBUTE time  %8.2f\n", stat->utime[DIST]);*/
./pdgssvx.c:1165:    // #pragma omp parallel  
./pdgssvx.c:1167:	// #pragma omp master
./pdgssvx.c:1175:#if ( PRNTlevel>=1 )
./pdgssvx.c:1177:       SUM OVER ALL ENTRIES OF A AND PRINT NNZ AND SIZE OF A.
./pdgssvx.c:1203:			nsupr = lsub[1];
./pdgssvx.c:1205:				for (i = 0; i < nsupr; ++i) 
./pdgssvx.c:1206:					lsum +=lusup[j*nsupr+i];
./pdgssvx.c:1221:	print_options_dist(options);
./pdgssvx.c:1225:    printf(".. Ainfo mygid %5d   mysid %5d   nnz_loc " IFMT "  sum_loc  %e lsum_loc   %e nnz " IFMT " nnzLU %ld sum %e  lsum %e  N " IFMT "\n", iam_g,iam,Astore->rowptr[Astore->m_loc],asum, lsum, nnz_tot,nnzLU,asum_tot,lsum_tot,A->ncol);
./pdgssvx.c:1231:// #ifdef GPU_PROF
./pdgssvx.c:1240://          printf("File being opend is %s\n",ttemp );
./pdgssvx.c:1245://              fprintf(stderr," Couldn't open output file %s\n",ttemp);
./pdgssvx.c:1252://                  fprintf(fp,"%d,%d,%d,%d,%d,%d\n",gs1.mnk_min_stats[ii],gs1.mnk_min_stats[ii+nsup],
./pdgssvx.c:1259://          fprintf(fp,"Min %lf Max %lf totaltime %lf \n",gs1.osDgemmMin,gs1.osDgemmMax,stat->utime[FACT]);
./pdgssvx.c:1268:	if ( options->PrintStat ) {
./pdgssvx.c:1306:		printf("\n** Memory Usage **********************************\n");
./pdgssvx.c:1307:                printf("** NUMfact space (MB): (sum-of-all-processes)\n"
./pdgssvx.c:1310:                printf("** Total highmark (MB):\n"
./pdgssvx.c:1313:		       avg / grid->nprow / grid->npcol * 1e-6,
./pdgssvx.c:1315:		printf("**************************************************\n");
./pdgssvx.c:1318:	} /* end printing stats */
./pdgssvx.c:1403:    // #pragma omp parallel  
./pdgssvx.c:1405:	// #pragma omp master
./pdgssvx.c:1413:	   Use iterative refinement to improve the computed solution and
./pdgssvx.c:1417:	    /* Improve the solution by iterative refinement. */
./pdgssvx.c:1428:	        pdgsmv_init(A, SOLVEstruct->row_to_proc, grid,
./pdgssvx.c:1449:		        p = SOLVEstruct->row_to_proc[jcol];
./pdgssvx.c:1458:		   previous call to pdgsmv_init() */
./pdgssvx.c:1473:	        SOLVEstruct1->row_to_proc = SOLVEstruct->row_to_proc;
./pdgssvx.c:1475:	        SOLVEstruct1->num_diag_procs = SOLVEstruct->num_diag_procs;
./pdgssvx.c:1476:	        SOLVEstruct1->diag_procs = SOLVEstruct->diag_procs;
./pdgssvx.c:1502:	pdPermute_Dense_Matrix(fst_row, m_loc, SOLVEstruct->row_to_proc,
./pdgssvx.c:1506:	printf("\n (%d) .. After pdPermute_Dense_Matrix(): b =\n", iam);
./pdgssvx.c:1508:	  printf("\t(%d)\t%4d\t%.10f\n", iam, i+fst_row, B[i]);
./pdgssvx.c:1542:#if ( PRNTlevel>=1 )
./pdgssvx.c:1543:    if ( !iam ) printf(".. DiagScale = %d\n", ScalePermstruct->DiagScale);
./pdgssvx_ABglobal.c:4:approvals from U.S. Dept. of Energy) 
./pdgssvx_ABglobal.c:16: * <pre>
./pdgssvx_ABglobal.c:23: * </pre>
./pdgssvx_ABglobal.c:30: * <pre>
./pdgssvx_ABglobal.c:40: * to run accurately and efficiently on large numbers of processors.
./pdgssvx_ABglobal.c:51: *      -  grid, a structure describing the 2D processor mesh
./pdgssvx_ABglobal.c:53: *            improve the accuracy of the computed solution using 
./pdgssvx_ABglobal.c:61: *      are used when A is sufficiently similar to a previously 
./pdgssvx_ABglobal.c:62: *      solved problem to save time by reusing part or all of 
./pdgssvx_ABglobal.c:63: *      the previous factorization.)
./pdgssvx_ABglobal.c:76: *                           condition number and so improve the
./pdgssvx_ABglobal.c:106: *                Pc*Pr*diag(R)*A*diag(C)
./pdgssvx_ABglobal.c:108: *                Pr and Pc are row and columns permutation matrices determined
./pdgssvx_ABglobal.c:117: *                A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgssvx_ABglobal.c:126: *            the same nonzero pattern as a previously factored matrix. In this
./pdgssvx_ABglobal.c:127: *            case the algorithm saves time by reusing the previously computed
./pdgssvx_ABglobal.c:139: *      previous column permutation from ScalePermstruct->perm_c is used as
./pdgssvx_ABglobal.c:159: *      all of the previously computed structure of L and U.
./pdgssvx_ABglobal.c:162: *            assuming not only the same nonzero pattern as the previously
./pdgssvx_ABglobal.c:178: *      -  ScalePermstruct->DiagScale, how the previous matrix was row and/or
./pdgssvx_ABglobal.c:180: *      -  ScalePermstruct->R, the row scalings of the previous matrix, if any
./pdgssvx_ABglobal.c:181: *      -  ScalePermstruct->C, the columns scalings of the previous matrix, 
./pdgssvx_ABglobal.c:183: *      -  ScalePermstruct->perm_r, the row permutation of the previous matrix
./pdgssvx_ABglobal.c:184: *      -  ScalePermstruct->perm_c, the column permutation of the previous 
./pdgssvx_ABglobal.c:186: *      -  all of LUstruct, the previously computed information about L and U
./pdgssvx_ABglobal.c:200: *      identical to a matrix that has already been factored on a previous 
./pdgssvx_ABglobal.c:203: *      -  options->Fact = Factored: A is identical to a previously
./pdgssvx_ABglobal.c:204: *            factorized matrix, so the entire previous factorization
./pdgssvx_ABglobal.c:215: *            the previous call, so that it has been scaled and permuted)
./pdgssvx_ABglobal.c:232: *           be factorized based on the previous history.
./pdgssvx_ABglobal.c:245: *             pattern was performed prior to this one. Therefore, this
./pdgssvx_ABglobal.c:262: *             prior to this one. Therefore, this factorization will reuse
./pdgssvx_ABglobal.c:265: *             distributed data structure set up from the previous symbolic
./pdgssvx_ABglobal.c:295: *           = LargeDiag_APWM: use the parallel approximate-weight perfect
./pdgssvx_ABglobal.c:317: *           = SLU_DOUBLE: accumulate residual in double precision.
./pdgssvx_ABglobal.c:318: *           = SLU_EXTRA:  accumulate residual in extra precision.
./pdgssvx_ABglobal.c:320: *         NOTE: all options must be identical on all processes when
./pdgssvx_ABglobal.c:327: *         compressed column format (also known as Harwell-Boeing format).
./pdgssvx_ABglobal.c:331: *         On exit, A may be overwritten by Pc*Pr*diag(R)*A*diag(C),
./pdgssvx_ABglobal.c:337: *                Pr*diag(R)*A*diag(C).
./pdgssvx_ABglobal.c:339: *                Pc*Pr*diag(R)*A*diag(C).
./pdgssvx_ABglobal.c:341: *         performed on the matrix Pc*Pr*diag(R)*A*diag(C)*Pc^T.
./pdgssvx_ABglobal.c:343: *         NOTE: Currently, A must reside in all processes when calling
./pdgssvx_ABglobal.c:354: *           = ROW:     row equilibration, i.e., A was premultiplied by
./pdgssvx_ABglobal.c:365: *           Row permutation vector, which defines the permutation matrix Pr;
./pdgssvx_ABglobal.c:366: *           perm_r[i] = j means row i of A is in position j in Pr*A.
./pdgssvx_ABglobal.c:378: *           On exit, perm_c may be overwritten by the product of the input
./pdgssvx_ABglobal.c:403: *         NOTE: Currently, B must reside in all processes when calling
./pdgssvx_ABglobal.c:415: *         The 2D process mesh. It contains the MPI communicator, the number
./pdgssvx_ABglobal.c:416: *         of process rows (NPROW), the number of process columns (NPCOL),
./pdgssvx_ABglobal.c:417: *         and my process rank. It is an input argument to all the
./pdgssvx_ABglobal.c:439: *           Global data structure (xsup, supno) replicated on all processes,
./pdgssvx_ABglobal.c:469: * </pre>
./pdgssvx_ABglobal.c:484:	       replicated on all processrs.
./pdgssvx_ABglobal.c:485:	           (lsub, xlsub) contains the compressed subscript of
./pdgssvx_ABglobal.c:487:          	   (usub, xusub) contains the compressed subscript of
./pdgssvx_ABglobal.c:509:#if ( PRNTlevel>= 2 )
./pdgssvx_ABglobal.c:510:    double   dmin, dsum, dprod;
./pdgssvx_ABglobal.c:526:	fprintf(stderr, "Extra precise iterative refinement yet to support.");
./pdgssvx_ABglobal.c:644:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:645:			    fprintf(stderr, "The " IFMT "-th row of A is exactly zero\n", 
./pdgssvx_ABglobal.c:649:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:650:                            fprintf(stderr, "The " IFMT "-th column of A is exactly zero\n", 
./pdgssvx_ABglobal.c:683:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:685:		printf(".. equilibrated? *equed = %c\n", *equed);
./pdgssvx_ABglobal.c:719:		/* Process 0 finds a row permutation for large diagonal. */
./pdgssvx_ABglobal.c:747:#if ( PRNTlevel>=2 )
./pdgssvx_ABglobal.c:750:	    dprod = 1.0;
./pdgssvx_ABglobal.c:764:#if ( PRNTlevel>=2 )
./pdgssvx_ABglobal.c:766:				dprod *= fabs(a[i]);
./pdgssvx_ABglobal.c:792:#if ( PRNTlevel>=2 )
./pdgssvx_ABglobal.c:799:				dprod *= fabs(a[i]);
./pdgssvx_ABglobal.c:809:#if ( PRNTlevel>=2 )
./pdgssvx_ABglobal.c:811:		if ( !iam ) printf("\tsmallest diagonal %e\n", dmin);
./pdgssvx_ABglobal.c:813:		if ( !iam ) printf("\tsum of diagonal %e\n", dsum);
./pdgssvx_ABglobal.c:815:		if ( !iam ) printf("\t product of diagonal %e\n", dprod);
./pdgssvx_ABglobal.c:823:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:824:	if ( !iam ) printf(".. LDPERM job " IFMT "\t time: %.2f\n", job, t);
./pdgssvx_ABglobal.c:836:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:837:	if ( !iam ) printf(".. anorm %e\n", anorm);
./pdgssvx_ABglobal.c:855:	    /* Use an ordering provided by SuperLU */
./pdgssvx_ABglobal.c:864:	/* Form Pc*A*Pc' to preserve the diagonal of the matrix Pr*A. */
./pdgssvx_ABglobal.c:876:#if ( PRNTlevel>=1 ) 
./pdgssvx_ABglobal.c:878:		printf(".. symbfact(): relax " IFMT ", maxsuper " IFMT ", fill " IFMT "\n",
./pdgssvx_ABglobal.c:893:#if ( PRNTlevel>=1 ) 
./pdgssvx_ABglobal.c:895:		    printf("\tNo of supers " IFMT "\n", Glu_persist->supno[n-1]+1);
./pdgssvx_ABglobal.c:896:		    printf("\tSize of G(L) " IFMT "\n", Glu_freeable->xlsub[n]);
./pdgssvx_ABglobal.c:897:		    printf("\tSize of G(U) " IFMT "\n", Glu_freeable->xusub[n]);
./pdgssvx_ABglobal.c:898:		    printf("\tint %d, short %d, float %d, double %d\n", 
./pdgssvx_ABglobal.c:901:		    printf("\tSYMBfact (MB):\tL\\U %.2f\ttotal %.2f\texpansions " IFMT "\n",
./pdgssvx_ABglobal.c:908:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:910:		    fprintf(stderr, "symbfact() error returns " IFMT "\n", iinfo);
./pdgssvx_ABglobal.c:917:	/* Distribute the L and U factors onto the process grid. */
./pdgssvx_ABglobal.c:933:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:954:		printf("\tNUMfact (MB) all PEs:\tL\\U\t%.2f\tall\t%.2f\n",
./pdgssvx_ABglobal.c:956:		printf("\tAll space (MB):"
./pdgssvx_ABglobal.c:958:		       avg*1e-6, avg/grid->nprow/grid->npcol*1e-6, max*1e-6);
./pdgssvx_ABglobal.c:959:		printf("\tNumber of tiny pivots: %10d\n", stat->TinyPivots);
./pdgssvx_ABglobal.c:960:		printf(".. pdgstrf INFO = %d\n", *info);
./pdgssvx_ABglobal.c:967:	 * NOTE: rows of A were previously permuted to Pc*A.
./pdgssvx_ABglobal.c:1000:	   Permute the right-hand side to form Pr*B.
./pdgssvx_ABglobal.c:1042:	   Use iterative refinement to improve the computed solution and
./pdgssvx_ABglobal.c:1046:	    /* Improve the solution by iterative refinement. */
./pdgssvx_ABglobal.c:1083:#if ( PRNTlevel>=1 )
./pdgssvx_ABglobal.c:1084:    if ( !iam ) printf(".. DiagScale = %d\n", ScalePermstruct->DiagScale);
./pdgstrf.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrf.c:16: * <pre>
./pdgstrf.c:69: *       krow = PROW( k, grid );
./pdgstrf.c:80: *       if ( iam == Pkk ) multicast L_k,k to this process row;
./pdgstrf.c:82: *          Recv L_k,k from process Pkk;
./pdgstrf.c:89: *       if ( myrow == krow ) multicast U_k,k+1:N to this process column;
./pdgstrf.c:90: *       if ( mycol == kcol ) multicast L_k+1:N,k to this process row;
./pdgstrf.c:93: *          Recv U_k,k+1:N from process Pkj;
./pdgstrf.c:97: *          Recv L_k+1:N,k from process Pik;
./pdgstrf.c:101: *              if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf.c:107: * </pre>
./pdgstrf.c:123:    Name    : SUPERNODE_PROFILE  
./pdgstrf.c:124:    Purpose : For SuperNode Level profiling of various measurements such as gigaflop/sec
./pdgstrf.c:128:// #define SUPERNODE_PROFILE   
./pdgstrf.c:191: * <pre>
./pdgstrf.c:223: *           Global data structure (xsup, supno) replicated on all processes,
./pdgstrf.c:234: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrf.c:235: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrf.c:236: *        and my process rank. It is an input argument to all the
./pdgstrf.c:252: * </pre>
./pdgstrf.c:273:    int_t Pc, Pr;
./pdgstrf.c:279:    int nsupr, nbrow, segsize;
./pdgstrf.c:285:    int_t *iuip, *ruip; /* Pointers to U index/nzval; size ceil(NSUPERS/Pr). */
./pdgstrf.c:289:    double *tempu, *tempv, *tempr;
./pdgstrf.c:299:    int ldt, ldu, lead_zero, ncols, ncb, nrb, p, pr, pc, nblocks;
./pdgstrf.c:376:#if ( PRNTlevel>= 1)
./pdgstrf.c:384:#if ( PRNTlevel==3 )
./pdgstrf.c:387:#if ( PROFlevel>=1 )
./pdgstrf.c:397:    } gemm_profile;
./pdgstrf.c:398:    gemm_profile *gemm_stats;
./pdgstrf.c:422:    Pr = grid->nprow;
./pdgstrf.c:432:        fprintf (stderr, "Could not get TAG_UB\n");
./pdgstrf.c:437:#if ( PRNTlevel>=1 )
./pdgstrf.c:439:        printf ("MPI tag upper bound = %d\n", tag_ub); fflush(stdout);
./pdgstrf.c:445:        printf (" ***** warning s_eps = %e *****\n", s_eps);
./pdgstrf.c:448:#if (PROFlevel >= 1 )
./pdgstrf.c:449:    gemm_stats = (gemm_profile *) SUPERLU_MALLOC(nsupers * sizeof(gemm_profile));
./pdgstrf.c:451:    int *prof_sendR = intCalloc_dist(nsupers);
./pdgstrf.c:462:    if (Pr * Pc > 1) {
./pdgstrf.c:464:              (MPI_Request *) SUPERLU_MALLOC (Pr * sizeof (MPI_Request))))
./pdgstrf.c:483:	    tempr = Llu->Lval_buf_2[0];
./pdgstrf.c:485:		Llu->Lval_buf_2[jj+1] = tempr + i*(jj+1); /* vectorize */
./pdgstrf.c:501:	    tempr = Llu->Uval_buf_2[0];
./pdgstrf.c:503:                Llu->Uval_buf_2[jj+1] = tempr + i*(jj+1); /* vectorize */
./pdgstrf.c:551:        if (!(send_reqs_u[i] = (MPI_Request *) SUPERLU_MALLOC (2 * Pr * sizeof (MPI_Request))))
./pdgstrf.c:571:#pragma omp parallel default(shared)
./pdgstrf.c:572:    #pragma omp master
./pdgstrf.c:585:#if ( PRNTlevel>=1 )
./pdgstrf.c:587:       printf(".. Starting with %d OpenMP threads \n", num_threads );
./pdgstrf.c:594:    nrb = nsupers / Pr; /* number of row blocks, vertical  */
./pdgstrf.c:629:    PrintInt10("schedule:perm_c_supno", nsupers, perm_c_supno);
./pdgstrf.c:632:    printf("[%d] .. Turn off static schedule for debugging ..\n", iam);
./pdgstrf.c:649:        ib = lb * Pr + myrow;
./pdgstrf.c:662:    if (myrow < nsupers % grid->nprow) { /* leftover block rows */
./pdgstrf.c:663:        ib = nrb * Pr + myrow;
./pdgstrf.c:731:#if ( PRNTlevel>=1 )
./pdgstrf.c:733:        printf (".. thresh = s_eps %e * anorm %e = %e\n", s_eps, anorm,
./pdgstrf.c:735:        printf
./pdgstrf.c:753:    k = CEILING (nsupers, Pr);  /* Number of local block rows */
./pdgstrf.c:761:#pragma omp parallel for reduction(max :local_max_row_size) private(lk,lsub) 
./pdgstrf.c:798:    int Threads_per_process = get_thread_per_process();
./pdgstrf.c:799:    int buffer_size  = SUPERLU_MAX(max_row_size*Threads_per_process*ldt,get_max_buffer_size());
./pdgstrf.c:804:    /* Note that in following expression 8 can be anything
./pdgstrf.c:818:    double* bigU; /* for storing entire U(k,:) panel, prepare for GEMM.
./pdgstrf.c:823:#if ( PRNTlevel>=1 )
./pdgstrf.c:825:	printf("max_nrows in L panel %d\n", max_row_size);
./pdgstrf.c:826:	printf("\t.. GEMM buffer size: max_nrows X max_ncols = %d x " IFMT "\n",
./pdgstrf.c:828:	printf(".. BIG U size " IFMT "\t BIG V size " IFMT "\n", bigu_size, bigv_size);
./pdgstrf.c:839:#if ( PRNTlevel>=1 )
./pdgstrf.c:840:    if (!iam) printf("[%d] .. BIG V bigv_size %d, using buffer_size %d (on GPU)\n", iam, bigv_size, buffer_size);
./pdgstrf.c:847:#if ( PRNTlevel>=1 )
./pdgstrf.c:848:    printf(" Starting with %d Cuda Streams \n",nstreams );
./pdgstrf.c:873:        fprintf(stderr, "!!!! Error in allocating A in the device %ld \n",m*k*sizeof(double) );
./pdgstrf.c:881:        fprintf(stderr, "!!!! Error in allocating B in the device %ld \n",n*k*sizeof(double));
./pdgstrf.c:887:        fprintf(stderr, "!!!! Error in allocating C in the device \n" );
./pdgstrf.c:919:#if ( PRNTlevel>=1 )
./pdgstrf.c:921:	printf ("  Max row size is %d \n", max_row_size);
./pdgstrf.c:922:        printf ("  Threads per process %d \n", num_threads);
./pdgstrf.c:954:    int_t mrb=    (nsupers+Pr-1) / Pr;
./pdgstrf.c:1015:    krow = PROW (k, grid);
./pdgstrf.c:1025:        scp = &grid->rscp;      /* The scope of process row. */
./pdgstrf.c:1027:        /* Multicasts numeric values of L(:,0) to process rows. */
./pdgstrf.c:1042:#if ( PROFlevel>=1 )
./pdgstrf.c:1053:                printf ("[%d] first block cloumn Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pdgstrf.c:1057:#if ( PROFlevel>=1 )
./pdgstrf.c:1061:		++prof_sendR[lk];
./pdgstrf.c:1069:            scp = &grid->rscp;  /* The scope of process row. */
./pdgstrf.c:1070:#if ( PROFlevel>=1 )
./pdgstrf.c:1079:#if ( PROFlevel>=1 )
./pdgstrf.c:1092:            scp = &grid->cscp;  /* The scope of process column. */
./pdgstrf.c:1095:#if ( PROFlevel>=1 )
./pdgstrf.c:1104:#if ( PROFlevel>=1 )
./pdgstrf.c:1150:                    /* Multicasts numeric values of L(:,kk) to process rows. */
./pdgstrf.c:1152:                    msgcnt = msgcnts[look_id];  /* point to the proper count array */
./pdgstrf.c:1164:                    scp = &grid->rscp;  /* The scope of process row. */
./pdgstrf.c:1168:#if ( PROFlevel>=1 )
./pdgstrf.c:1177:#if ( PROFlevel>=1 )
./pdgstrf.c:1181:			    ++prof_sendR[lk];
./pdgstrf.c:1184:			    printf ("[%d] -1- Send L(:,%4d): #lsub1 %4d, #lusup1 %4d right to Pj %2d\n",
./pdgstrf.c:1193:                        scp = &grid->rscp;  /* The scope of process row. */
./pdgstrf.c:1195:#if ( PROFlevel>=1 )
./pdgstrf.c:1205:#if ( PROFlevel>=1 )
./pdgstrf.c:1215:            /* Pre-post irecv for U-row look-ahead */
./pdgstrf.c:1216:            krow = PROW (kk, grid);
./pdgstrf.c:1219:                    scp = &grid->cscp;  /* The scope of process column. */
./pdgstrf.c:1222:#if ( PROFlevel>=1 )
./pdgstrf.c:1231:#if ( PROFlevel>=1 )
./pdgstrf.c:1253:                krow = PROW (kk, grid);
./pdgstrf.c:1272:#if ( PROFlevel>=1 )
./pdgstrf.c:1290:#if ( PROFlevel>=1 )
./pdgstrf.c:1302:                    scp = &grid->cscp;  /* The scope of process column. */
./pdgstrf.c:1305:                        /* Parallel triangular solve across process row *krow* --
./pdgstrf.c:1309:/* #pragma omp parallel */ /* Sherry -- parallel done inside pdgstrs2 */
./pdgstrf.c:1319:                        /* Multicasts U(kk,:) to process columns. */
./pdgstrf.c:1331:                            for (pi = 0; pi < Pr; ++pi) {
./pdgstrf.c:1333:#if ( PROFlevel>=1 )
./pdgstrf.c:1342:                                               scp->comm, &send_reqs_u[look_id][pi + Pr]);
./pdgstrf.c:1344:#if ( PROFlevel>=1 )
./pdgstrf.c:1351:                                    printf ("[%d] Send U(%4d,:) to Pr %2d\n",
./pdgstrf.c:1366:         * == start processing the current row of U(k,:) *
./pdgstrf.c:1369:        krow = PROW (k, grid);
./pdgstrf.c:1383:#if ( PROFlevel>=1 )
./pdgstrf.c:1393:#if ( PROFlevel>=1 )
./pdgstrf.c:1403:                scp = &grid->rscp;  /* The scope of process row. */
./pdgstrf.c:1406:                 * Waiting for L(:,kk) for outer-product uptate  *
./pdgstrf.c:1411:#if ( PROFlevel>=1 )
./pdgstrf.c:1421:		    printf("\t[%d] k=%d, look_id=%d, recv_req[0] == MPI_REQUEST_NULL, msgcnt[0] = %d\n", 
./pdgstrf.c:1433:		    printf("\t[%d] k=%d, look_id=%d, recv_req[1] == MPI_REQUEST_NULL, msgcnt[1] = %d\n", 
./pdgstrf.c:1438:#if ( PROFlevel>=1 )
./pdgstrf.c:1444:                printf("[%d] Recv L(:,%4d): #lsub %4d, #lusup %4d from Pc %2d\n",
./pdgstrf.c:1449:#if ( PRNTlevel==3 )
./pdgstrf.c:1462:        scp = &grid->cscp;      /* The scope of process column. */
./pdgstrf.c:1471:                /* Parallel triangular solve across process row *krow* --
./pdgstrf.c:1475:/* #pragma omp parallel */ /* Sherry -- parallel done inside pdgstrs2 */
./pdgstrf.c:1484:                /* Multicasts U(k,:) along process columns. */
./pdgstrf.c:1493:                    for (pi = 0; pi < Pr; ++pi) {
./pdgstrf.c:1494:                        if (pi != myrow) { /* Matching recv was pre-posted before */
./pdgstrf.c:1495:#if ( PROFlevel>=1 )
./pdgstrf.c:1504:#if ( PROFlevel>=1 )
./pdgstrf.c:1512:                            printf ("[%d] Send U(%4d,:) down to Pr %2d\n", iam, k, pi);
./pdgstrf.c:1518:            } else { /* Panel U(k,:) already factorized from previous look-ahead */
./pdgstrf.c:1522:		* for outer-product update.                        *
./pdgstrf.c:1526:#if ( PROFlevel>=1 )
./pdgstrf.c:1529:                    for (pi = 0; pi < Pr; ++pi) {
./pdgstrf.c:1532:                            MPI_Wait (&send_reqs_u[look_id][pi + Pr], &status);
./pdgstrf.c:1535:#if ( PROFlevel>=1 )
./pdgstrf.c:1549:             * Wait for U(k,:) for outer-product updates. *
./pdgstrf.c:1553:#if ( PROFlevel>=1 )
./pdgstrf.c:1561:#if ( PROFlevel>=1 )
./pdgstrf.c:1569:                printf ("[%d] Recv U(%4d,:) from Pr %2d\n", iam, k, krow);
./pdgstrf.c:1571:#if ( PRNTlevel==3 )
./pdgstrf.c:1579:        } /* end if myrow == Pr(k) */
./pdgstrf.c:1585:         *         if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf.c:1593:            nsupr = lsub[1];    /* LDA of lusup. */
./pdgstrf.c:1664:                        scp = &grid->rscp;  /* The scope of process row. */
./pdgstrf.c:1668:#if ( PROFlevel>=1 )
./pdgstrf.c:1678:#if ( PROFlevel>=1 )
./pdgstrf.c:1698:                        /* Process column *kcol+1* multicasts numeric
./pdgstrf.c:1699:			   values of L(:,k+1) to process rows. */
./pdgstrf.c:1712:                        scp = &grid->rscp;  /* The scope of process row. */
./pdgstrf.c:1715:#if ( PROFlevel>=1 )
./pdgstrf.c:1724:#if ( PROFlevel>=1 )
./pdgstrf.c:1728:				++prof_sendR[lk];
./pdgstrf.c:1767:#if ( PRNTlevel>=1 )
./pdgstrf.c:1768:    /* Print detailed statistics */
./pdgstrf.c:1774:	printf("\nInitialization time\t%8.2lf seconds\n"
./pdgstrf.c:1776:        printf("\n==== Time breakdown in factorization (rank 0) ====\n");
./pdgstrf.c:1777:	printf("Panel factorization \t %8.2lf seconds\n",
./pdgstrf.c:1779:	printf(".. L-panel pxgstrf2 \t %8.2lf seconds\n", pdgstrf2_timer);
./pdgstrf.c:1780:	printf(".. U-panel pxgstrs2 \t %8.2lf seconds\n", pdgstrs2_timer);
./pdgstrf.c:1781:	printf("Time in Look-ahead update \t %8.2lf seconds\n", lookaheadupdatetimer);
./pdgstrf.c:1782:        printf("Time in Schur update \t\t %8.2lf seconds\n", NetSchurUpTimer);
./pdgstrf.c:1783:        printf(".. Time to Gather L buffer\t %8.2lf  (Separate L panel by Lookahead/Remain)\n", GatherLTimer);
./pdgstrf.c:1784:        printf(".. Time to Gather U buffer\t %8.2lf \n", GatherUTimer);
./pdgstrf.c:1786:        printf(".. Time in GEMM %8.2lf \n",
./pdgstrf.c:1788:        printf("\t* Look-ahead\t %8.2lf \n", LookAheadGEMMTimer);
./pdgstrf.c:1789:        printf("\t* Remain\t %8.2lf\tFlops %8.2le\tGflops %8.2lf\n", 
./pdgstrf.c:1791:        printf(".. Time to Scatter %8.2lf \n", 
./pdgstrf.c:1793:        printf("\t* Look-ahead\t %8.2lf \n", LookAheadScatterTimer);
./pdgstrf.c:1794:        printf("\t* Remain\t %8.2lf \n", RemainScatterTimer);
./pdgstrf.c:1796:        printf("Total factorization time            \t: %8.2lf seconds, \n", pxgstrfTimer);
./pdgstrf.c:1797:        printf("--------\n");
./pdgstrf.c:1798:	printf("GEMM maximum block: %d-%d-%d\n", gemm_max_m, gemm_max_k, gemm_max_n);
./pdgstrf.c:1803:    for (i = 0; i < Pr * Pc; ++i) {
./pdgstrf.c:1805:            dPrintLblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf.c:1806:            dPrintUblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf.c:1807:            printf ("(%d)\n", iam);
./pdgstrf.c:1808:            PrintInt10 ("Recv", nsupers, Llu->ToRecv);
./pdgstrf.c:1818:    if (Pr * Pc > 1) {
./pdgstrf.c:1827:            for (krow = 0; krow < Pr; ++krow) {
./pdgstrf.c:1908:#if ( PRNTlevel>=1 )
./pdgstrf.c:1934:#if ( PROFlevel>=1 )
./pdgstrf.c:1938:    /* Prepare error message - find the smallesr index i that U(i,i)==0 */
./pdgstrf.c:1944:#if ( PROFlevel>=1 )
./pdgstrf.c:1959:            printf ("\tPDGSTRF comm stat:"
./pdgstrf.c:1962:                    msg_cnt_sum / Pr / Pc, msg_cnt_max,
./pdgstrf.c:1963:                    msg_vol_sum / Pr / Pc * 1e-6, msg_vol_max * 1e-6);
./pdgstrf.c:1964:	    printf("\t\tcomm time on task 0: %8.2lf\n"
./pdgstrf.c:1972:	    printf("gemm_count %d\n", gemm_count);
./pdgstrf.c:1974:		fprintf(fgemm, "%8d%8d%8d\t %20.16e\t%8d\n", gemm_stats[i].m, gemm_stats[i].n,
./pdgstrf.c:1975:			gemm_stats[i].k, gemm_stats[i].microseconds, prof_sendR[i]);
./pdgstrf.c:1980:	SUPERLU_FREE(prof_sendR);
./pdgstrf.c:1984:#if ( PRNTlevel==3 )
./pdgstrf.c:1987:        printf (".. # msg of zero size\t%d\n", iinfo);
./pdgstrf.c:1990:        printf (".. # total msg\t%d\n", iinfo);
./pdgstrf.c:1994:    for (i = 0; i < Pr * Pc; ++i) {
./pdgstrf.c:1996:            dPrintLblocks (iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf.c:1997:            dPrintUblocks (iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf.c:1998:            printf ("(%d)\n", iam);
./pdgstrf.c:1999:            PrintInt10 ("Recv", nsupers, Llu->ToRecv);
./pdgstrf.c:2006:    printf ("(%d) num_copy=%d, num_update=%d\n", iam, num_copy, num_update);
./pdgstrf2.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrf2.c:16: * <pre>
./pdgstrf2.c:24: * <pre>
./pdgstrf2.c:30: *   Only the column processes that own block column *k* participate
./pdgstrf2.c:49: *        Global data structures (xsup, supno) replicated on all processes.
./pdgstrf2.c:52: *        The 2D process mesh.
./pdgstrf2.c:74: * </pre>
./pdgstrf2.c:88:    /* printf("entering pdgstrf2 %d \n", grid->iam); */
./pdgstrf2.c:89:    int cols_left, iam, l, pkk, pr;
./pdgstrf2.c:92:    int nsupr;                  /* number of rows in the block (LDA) */
./pdgstrf2.c:100:    int_t Pr;
./pdgstrf2.c:107:    Pr = grid->nprow;
./pdgstrf2.c:109:    krow = PROW (k, grid);
./pdgstrf2.c:110:    pkk = PNUM (PROW (k, grid), PCOL (k, grid), grid);
./pdgstrf2.c:117:        nsupr = Llu->Lrowind_bc_ptr[j][1];
./pdgstrf2.c:119:        nsupr = 0;
./pdgstrf2.c:121:    printf ("rank %d  Iter %d  k=%d \t dtrsm nsuper %d \n",
./pdgstrf2.c:122:            iam, k0, k, nsupr);
./pdgstrf2.c:135:#if ( PROFlevel>=1 )
./pdgstrf2.c:138:        for (pr = 0; pr < Pr; ++pr) {
./pdgstrf2.c:139:            if (pr != myrow) {
./pdgstrf2.c:140:                MPI_Wait (U_diag_blk_send_req + pr, &status);
./pdgstrf2.c:143:#if ( PROFlevel>=1 )
./pdgstrf2.c:152:    if (iam == pkk) {            /* diagonal process */
./pdgstrf2.c:161:#if ( PRNTlevel>=2 )
./pdgstrf2.c:162:                    printf ("(%d) .. col %d, tiny pivot %e  ",
./pdgstrf2.c:168:#if ( PRNTlevel>=2 )
./pdgstrf2.c:169:                    printf ("replaced by %e\n", lusup[i]);
./pdgstrf2.c:176:            for (l = 0; l < cols_left; ++l, i += nsupr, ++u_diag_cnt)
./pdgstrf2.c:182:            for (l = 0; l < cols_left; ++l, i += nsupr, ++u_diag_cnt) {
./pdgstrf2.c:198:                /* l = nsupr - j - 1;  */
./pdgstrf2.c:201:                       &ujrow[ld_ujrow], &incy, &lusup[luptr + nsupr + 1],
./pdgstrf2.c:202:                       &nsupr);
./pdgstrf2.c:208:            luptr += nsupr + 1; /* move to next column */
./pdgstrf2.c:218:#if ( PROFlevel>=1 )
./pdgstrf2.c:221:            for (pr = 0; pr < Pr; ++pr) {
./pdgstrf2.c:222:                if (pr != krow) {
./pdgstrf2.c:225:                    MPI_Isend (ublk_ptr, nsupc * nsupc, MPI_DOUBLE, pr,
./pdgstrf2.c:227:                               comm, U_diag_blk_send_req + pr);
./pdgstrf2.c:231:#if ( PROFlevel>=1 )
./pdgstrf2.c:241:        /* pragma below would be changed by an MKL call */
./pdgstrf2.c:243:        l = nsupr - nsupc;
./pdgstrf2.c:247:        printf ("calling dtrsm\n");
./pdgstrf2.c:248:        printf ("dtrsm diagonal param 11:  %d \n", nsupr);
./pdgstrf2.c:253:                &alpha, ublk_ptr, &ld_ujrow, &lusup[nsupc], &nsupr,
./pdgstrf2.c:257:                &alpha, ublk_ptr, &ld_ujrow, &lusup[nsupc], &nsupr);
./pdgstrf2.c:260:    } else {  /* non-diagonal process */
./pdgstrf2.c:269:        // printf("hello message receiving%d %d\n",(nsupc*(nsupc+1))>>1,SLU_MPI_TAG(4,k0));
./pdgstrf2.c:270:#if ( PROFlevel>=1 )
./pdgstrf2.c:276:#if ( PROFlevel>=1 )
./pdgstrf2.c:281:        if (nsupr > 0) {
./pdgstrf2.c:285:            printf ("dtrsm non diagonal param 11:  %d \n", nsupr);
./pdgstrf2.c:287:                printf (" Rank :%d \t Empty block column occurred :\n", iam);
./pdgstrf2.c:290:            dtrsm_ ("R", "U", "N", "N", &nsupr, &nsupc,
./pdgstrf2.c:291:                    &alpha, ublk_ptr, &ld_ujrow, lusup, &nsupr, 1, 1, 1, 1);
./pdgstrf2.c:293:            dtrsm_ ("R", "U", "N", "N", &nsupr, &nsupc,
./pdgstrf2.c:294:                    &alpha, ublk_ptr, &ld_ujrow, lusup, &nsupr);
./pdgstrf2.c:296:	    stat->ops[FACT] += (flops_t) nsupc * (nsupc+1) * nsupr;
./pdgstrf2.c:301:    /* printf("exiting pdgstrf2 %d \n", grid->iam);  */
./pdgstrf2.c:313:    printf("====Entering pdgstrs2==== \n");
./pdgstrf2.c:317:    int nsupr;                /* number of rows in the block L(:,k) (LDA) */
./pdgstrf2.c:338:    pkk = PNUM (PROW (k, grid), PCOL (k, grid), grid);
./pdgstrf2.c:339:    //int k_row_cycle = k / grid->nprow;  /* for which cycle k exist (to assign rowwise thread blocking) */
./pdgstrf2.c:347:        nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pdgstrf2.c:350:        nsupr = Llu->Lsub_buf_2[k0 % (1 + stat->num_look_aheads)][1];   /* LDA of lusup[] */
./pdgstrf2.c:376:    // https://stackoverflow.com/questions/13065943/task-based-programming-pragma-omp-task-versus-pragma-omp-parallel-for
./pdgstrf2.c:377:#pragma omp parallel for schedule(static) default(shared) \
./pdgstrf2.c:378:    private(b,j,iukp,rukp,segsize)
./pdgstrf2.c:388:#pragma omp task default(shared) firstprivate(segsize,rukp) if (segsize > 30)
./pdgstrf2.c:390:		    int_t luptr = (knsupc - segsize) * (nsupr + 1);
./pdgstrf2.c:391:		    //printf("[2] segsize %d, nsupr %d\n", segsize, nsupr);
./pdgstrf2.c:394:                    dtrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
./pdgstrf2.c:397:                    dtrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
./pdgstrf2.c:405:/* #pragma omp taskwait */
./pdgstrf_X1.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrf_X1.c:14: * <pre>
./pdgstrf_X1.c:58: *       krow = PROW( k, grid );
./pdgstrf_X1.c:69: *       if ( iam == Pkk ) multicast L_k,k to this process row;
./pdgstrf_X1.c:71: *          Recv L_k,k from process Pkk;
./pdgstrf_X1.c:78: *       if ( myrow == krow ) multicast U_k,k+1:N to this process column;
./pdgstrf_X1.c:79: *       if ( mycol == kcol ) multicast L_k+1:N,k to this process row;
./pdgstrf_X1.c:82: *          Recv U_k,k+1:N from process Pkj;
./pdgstrf_X1.c:86: *          Recv L_k+1:N,k from process Pik;
./pdgstrf_X1.c:90: *              if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf_X1.c:99: * </pre>
./pdgstrf_X1.c:110: * Internal prototypes
./pdgstrf_X1.c:129: * <pre>
./pdgstrf_X1.c:161: *           Global data structure (xsup, supno) replicated on all processes,
./pdgstrf_X1.c:172: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrf_X1.c:173: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrf_X1.c:174: *        and my process rank. It is an input argument to all the
./pdgstrf_X1.c:190: * </pre>
./pdgstrf_X1.c:215:    int_t Pc, Pr;
./pdgstrf_X1.c:218:    int   nsupr, nbrow, segsize;
./pdgstrf_X1.c:230:    int_t  *iuip, *ruip;/* Pointers to U index/nzval; size ceil(NSUPERS/Pr). */
./pdgstrf_X1.c:250:#if ( PRNTlevel==3 )
./pdgstrf_X1.c:253:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:276:    Pr = grid->nprow;
./pdgstrf_X1.c:290:    if ( Pr*Pc > 1 ) {
./pdgstrf_X1.c:312:#if ( PRNTlevel>=1 )
./pdgstrf_X1.c:314:	printf(".. thresh = s_eps %e * anorm %e = %e\n", s_eps, anorm, thresh);
./pdgstrf_X1.c:315:	printf(".. Buffer size: Lsub %d\tLval %d\tUsub %d\tUval %d\tLDA %d\n",
./pdgstrf_X1.c:346:    k = CEILING( nsupers, Pr ); /* Number of local block rows */
./pdgstrf_X1.c:375:	scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_X1.c:377:	/* Process column *kcol* multicasts numeric values of L(:,k) 
./pdgstrf_X1.c:378:	   to process rows. */
./pdgstrf_X1.c:390:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:401:		printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pdgstrf_X1.c:407:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:417:	    scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_X1.c:423:	    printf("(%d) Post Irecv L(:,%4d)\n", iam, 0);
./pdgstrf_X1.c:434:	krow = PROW( k, grid );
./pdgstrf_X1.c:451:		scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_X1.c:452:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:458:		/*probe_recv(iam, kcol, (4*k)%NTAGS, mpi_int_t, scp->comm, 
./pdgstrf_X1.c:464:		/*probe_recv(iam, kcol, (4*k+1)%NTAGS, MPI_DOUBLE, scp->comm, 
./pdgstrf_X1.c:473:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:478:		printf("(%d) Recv L(:,%4d): lsub %4d, lusup %4d from Pc %2d\n",
./pdgstrf_X1.c:484:#if ( PRNTlevel==3 )
./pdgstrf_X1.c:491:	scp = &grid->cscp; /* The scope of process column. */
./pdgstrf_X1.c:494:	    /* Parallel triangular solve across process row *krow* --
./pdgstrf_X1.c:502:	    /* Multicasts U(k,:) to process columns. */
./pdgstrf_X1.c:514:		for (pi = 0; pi < Pr; ++pi) {
./pdgstrf_X1.c:516:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:529:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:536:			printf("(%d) Send U(%4d,:) to Pr %2d\n", iam, k, pi);
./pdgstrf_X1.c:543:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:549:		/*probe_recv(iam, krow, (4*k+2)%NTAGS, mpi_int_t, scp->comm, 
./pdgstrf_X1.c:554:		/*probe_recv(iam, krow, (4*k+3)%NTAGS, MPI_DOUBLE, scp->comm, 
./pdgstrf_X1.c:562:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:569:		printf("(%d) Recv U(%4d,:) from Pr %2d\n", iam, k, krow);
./pdgstrf_X1.c:571:#if ( PRNTlevel==3 )
./pdgstrf_X1.c:576:	} /* if myrow == Pr(k) */
./pdgstrf_X1.c:582:	 *         if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf_X1.c:589:	    nsupr = lsub[1]; /* LDA of lusup. */
./pdgstrf_X1.c:624:		/* Prepare to call DGEMM. */
./pdgstrf_X1.c:645:		  printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pdgstrf_X1.c:673:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_X1.c:677:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_X1.c:736:/*#pragma _CRI cache_bypass nzval,tempv*/
./pdgstrf_X1.c:768:	    /* Process column *kcol+1* multicasts numeric values of L(:,k+1) 
./pdgstrf_X1.c:769:	       to process rows. */
./pdgstrf_X1.c:779:	    scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_X1.c:783:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:796:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:803:		    printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pdgstrf_X1.c:810:		scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_X1.c:816:		printf("(%d) Post Irecv L(:,%4d)\n", iam, k+1);
./pdgstrf_X1.c:834:		/* Prepare to call DGEMM. */
./pdgstrf_X1.c:849:		printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pdgstrf_X1.c:883:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_X1.c:887:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_X1.c:948:/*#pragma _CRI cache_bypass nzval,tempv*/
./pdgstrf_X1.c:976:    if ( Pr*Pc > 1 ) {
./pdgstrf_X1.c:990:    /* Prepare error message. */
./pdgstrf_X1.c:992:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:996:#if ( PROFlevel>=1 )
./pdgstrf_X1.c:1011:	    printf("\tPDGSTRF comm stat:"
./pdgstrf_X1.c:1014:		   msg_cnt_sum/Pr/Pc, msg_cnt_max,
./pdgstrf_X1.c:1015:		   msg_vol_sum/Pr/Pc*1e-6, msg_vol_max*1e-6);
./pdgstrf_X1.c:1023:#if ( PRNTlevel==3 )
./pdgstrf_X1.c:1025:    if ( !iam ) printf(".. # msg of zero size\t%d\n", iinfo);
./pdgstrf_X1.c:1027:    if ( !iam ) printf(".. # total msg\t%d\n", iinfo);
./pdgstrf_X1.c:1030:#if ( PRNTlevel==2 )
./pdgstrf_X1.c:1031:    for (i = 0; i < Pr * Pc; ++i) {
./pdgstrf_X1.c:1033:	    dPrintLblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf_X1.c:1034:	    dPrintUblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf_X1.c:1035:	    printf("(%d)\n", iam);
./pdgstrf_X1.c:1036:	    PrintInt10("Recv", nsupers, Llu->ToRecv);
./pdgstrf_X1.c:1043:    printf("(%d) num_copy=%d, num_update=%d\n", iam, num_copy, num_update);
./pdgstrf_X1.c:1054: * <pre>
./pdgstrf_X1.c:1058: *   Only the process column that owns block column *k* participates
./pdgstrf_X1.c:1071: *        Global data structures (xsup, supno) replicated on all processes.
./pdgstrf_X1.c:1074: *        The 2D process mesh.
./pdgstrf_X1.c:1090: * </pre>
./pdgstrf_X1.c:1103:    int    nsupr; /* number of rows in the block (LDA) */
./pdgstrf_X1.c:1117:    krow  = PROW( k, grid );
./pdgstrf_X1.c:1118:    pkk   = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pdgstrf_X1.c:1124:    if ( Llu->Lrowind_bc_ptr[j] ) nsupr = Llu->Lrowind_bc_ptr[j][1];
./pdgstrf_X1.c:1131:	   the process column. */
./pdgstrf_X1.c:1132:	if ( iam == pkk ) { /* Diagonal process. */
./pdgstrf_X1.c:1136:#if ( PRNTlevel>=2 )
./pdgstrf_X1.c:1137:		    printf("(%d) .. col %d, tiny pivot %e  ",
./pdgstrf_X1.c:1143:#if ( PRNTlevel>=2 )
./pdgstrf_X1.c:1144:		    printf("replaced by %e\n", lusup[i]);
./pdgstrf_X1.c:1149:	    for (l = 0; l < c; ++l, i += nsupr)	ujrow[l] = lusup[i];
./pdgstrf_X1.c:1162:	    printf("..(%d) k %d, j %d: Send ujrow[0] %e\n",iam,k,j,ujrow[0]);
./pdgstrf_X1.c:1164:	    printf("..(%d) k %d, j %d: Recv ujrow[0] %e\n",iam,k,j,ujrow[0]);
./pdgstrf_X1.c:1182:		for (i = luptr+1; i < luptr-j+nsupr; ++i) lusup[i] *= temp;
./pdgstrf_X1.c:1183:		stat->ops[FACT] += nsupr-j-1;
./pdgstrf_X1.c:1185:		for (i = luptr; i < luptr+nsupr; ++i) lusup[i] *= temp;
./pdgstrf_X1.c:1186:		stat->ops[FACT] += nsupr;
./pdgstrf_X1.c:1193:		l = nsupr - j - 1;
./pdgstrf_X1.c:1196:		     &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pdgstrf_X1.c:1199:		      &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pdgstrf_X1.c:1204:		SGER(&nsupr, &c, &alpha, &lusup[luptr], &incx, 
./pdgstrf_X1.c:1205:		     &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pdgstrf_X1.c:1207:		dger_(&nsupr, &c, &alpha, &lusup[luptr], &incx, 
./pdgstrf_X1.c:1208:		      &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pdgstrf_X1.c:1210:		stat->ops[FACT] += 2 * nsupr * c;
./pdgstrf_X1.c:1215:	if ( iam == pkk ) luptr += nsupr + 1;
./pdgstrf_X1.c:1216:	else luptr += nsupr;
./pdgstrf_X1.c:1226: * <pre> 
./pdgstrf_X1.c:1231: *   Only the process column that owns block column *k* participates
./pdgstrf_X1.c:1244: *        Global data structures (xsup, supno) replicated on all processes.
./pdgstrf_X1.c:1247: *        The 2D process mesh.
./pdgstrf_X1.c:1255: * </pre>
./pdgstrf_X1.c:1274:    int    nsupr; /* number of rows in the block L(:,k) (LDA) */
./pdgstrf_X1.c:1289:    pkk  = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pdgstrf_X1.c:1299:	nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pdgstrf_X1.c:1302:	nsupr = Llu->Lsub_buf_2[k%2][1]; /* LDA of lusup[] */
./pdgstrf_X1.c:1316:		luptr = (knsupc - segsize) * (nsupr + 1);
./pdgstrf_X1.c:1318:		STRSV(ftcs1, ftcs2, ftcs3, &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_X1.c:1321:		dtrsv_("L", "N", "U", &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_X1.c:1333:probe_recv(int iam, int source, int tag, MPI_Datatype datatype, MPI_Comm comm,
./pdgstrf_X1.c:1339:    MPI_Probe( source, tag, comm, &status );
./pdgstrf_X1.c:1342:        printf("(%d) Recv'ed count %d > buffer size $d\n",
./pdgstrf_irecv.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrf_irecv.c:16: * <pre>
./pdgstrf_irecv.c:61: *       krow = PROW( k, grid );
./pdgstrf_irecv.c:72: *       if ( iam == Pkk ) multicast L_k,k to this process row;
./pdgstrf_irecv.c:74: *          Recv L_k,k from process Pkk;
./pdgstrf_irecv.c:81: *       if ( myrow == krow ) multicast U_k,k+1:N to this process column;
./pdgstrf_irecv.c:82: *       if ( mycol == kcol ) multicast L_k+1:N,k to this process row;
./pdgstrf_irecv.c:85: *          Recv U_k,k+1:N from process Pkj;
./pdgstrf_irecv.c:89: *          Recv L_k+1:N,k from process Pik;
./pdgstrf_irecv.c:93: *              if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf_irecv.c:102: * </pre>
./pdgstrf_irecv.c:112: * Internal prototypes
./pdgstrf_irecv.c:128: * <pre>
./pdgstrf_irecv.c:160: *           Global data structure (xsup, supno) replicated on all processes,
./pdgstrf_irecv.c:171: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrf_irecv.c:172: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrf_irecv.c:173: *        and my process rank. It is an input argument to all the
./pdgstrf_irecv.c:189: * </pre>
./pdgstrf_irecv.c:213:    int_t Pc, Pr;
./pdgstrf_irecv.c:216:    int   nsupr, nbrow, segsize;
./pdgstrf_irecv.c:228:    int_t  *iuip, *ruip;/* Pointers to U index/nzval; size ceil(NSUPERS/Pr). */
./pdgstrf_irecv.c:246:#if ( PRNTlevel==3 )
./pdgstrf_irecv.c:249:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:272:    Pr = grid->nprow;
./pdgstrf_irecv.c:286:    if ( Pr*Pc > 1 ) {
./pdgstrf_irecv.c:308:#if ( PRNTlevel>=1 )
./pdgstrf_irecv.c:310:	printf(".. thresh = s_eps %e * anorm %e = %e\n", s_eps, anorm, thresh);
./pdgstrf_irecv.c:311:	printf(".. Buffer size: Lsub %d\tLval %d\tUsub %d\tUval %d\tLDA %d\n",
./pdgstrf_irecv.c:337:    k = CEILING( nsupers, Pr ); /* Number of local block rows */
./pdgstrf_irecv.c:366:	scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_irecv.c:368:	/* Process column *kcol* multicasts numeric values of L(:,k) 
./pdgstrf_irecv.c:369:	   to process rows. */
./pdgstrf_irecv.c:381:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:392:		printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pdgstrf_irecv.c:398:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:408:	    scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_irecv.c:414:	    printf("(%d) Post Irecv L(:,%4d)\n", iam, 0);
./pdgstrf_irecv.c:425:	krow = PROW( k, grid );
./pdgstrf_irecv.c:442:		scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_irecv.c:443:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:449:		/*probe_recv(iam, kcol, (4*k)%NTAGS, mpi_int_t, scp->comm, 
./pdgstrf_irecv.c:455:		/*probe_recv(iam, kcol, (4*k+1)%NTAGS, MPI_DOUBLE, scp->comm, 
./pdgstrf_irecv.c:464:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:469:		printf("(%d) Recv L(:,%4d): lsub %4d, lusup %4d from Pc %2d\n",
./pdgstrf_irecv.c:475:#if ( PRNTlevel==3 )
./pdgstrf_irecv.c:482:	scp = &grid->cscp; /* The scope of process column. */
./pdgstrf_irecv.c:485:	    /* Parallel triangular solve across process row *krow* --
./pdgstrf_irecv.c:493:	    /* Multicasts U(k,:) to process columns. */
./pdgstrf_irecv.c:505:		for (pi = 0; pi < Pr; ++pi) {
./pdgstrf_irecv.c:507:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:520:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:527:			printf("(%d) Send U(%4d,:) to Pr %2d\n", iam, k, pi);
./pdgstrf_irecv.c:534:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:540:		/*probe_recv(iam, krow, (4*k+2)%NTAGS, mpi_int_t, scp->comm, 
./pdgstrf_irecv.c:545:		/*probe_recv(iam, krow, (4*k+3)%NTAGS, MPI_DOUBLE, scp->comm, 
./pdgstrf_irecv.c:553:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:560:		printf("(%d) Recv U(%4d,:) from Pr %2d\n", iam, k, krow);
./pdgstrf_irecv.c:562:#if ( PRNTlevel==3 )
./pdgstrf_irecv.c:567:	} /* if myrow == Pr(k) */
./pdgstrf_irecv.c:573:	 *         if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf_irecv.c:580:	    nsupr = lsub[1]; /* LDA of lusup. */
./pdgstrf_irecv.c:615:		/* Prepare to call DGEMM. */
./pdgstrf_irecv.c:636:		  printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pdgstrf_irecv.c:664:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_irecv.c:668:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_irecv.c:672:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_irecv.c:731:/*#pragma _CRI cache_bypass nzval,tempv*/
./pdgstrf_irecv.c:763:	    /* Process column *kcol+1* multicasts numeric values of L(:,k+1) 
./pdgstrf_irecv.c:764:	       to process rows. */
./pdgstrf_irecv.c:774:	    scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_irecv.c:778:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:791:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:798:		    printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pdgstrf_irecv.c:805:		scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_irecv.c:811:		printf("(%d) Post Irecv L(:,%4d)\n", iam, k+1);
./pdgstrf_irecv.c:829:		/* Prepare to call DGEMM. */
./pdgstrf_irecv.c:844:		printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pdgstrf_irecv.c:878:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_irecv.c:882:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_irecv.c:886:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_irecv.c:947:/*#pragma _CRI cache_bypass nzval,tempv*/
./pdgstrf_irecv.c:975:    if ( Pr*Pc > 1 ) {
./pdgstrf_irecv.c:989:    /* Prepare error message. */
./pdgstrf_irecv.c:991:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:995:#if ( PROFlevel>=1 )
./pdgstrf_irecv.c:1010:	    printf("\tPDGSTRF comm stat:"
./pdgstrf_irecv.c:1013:		   msg_cnt_sum/Pr/Pc, msg_cnt_max,
./pdgstrf_irecv.c:1014:		   msg_vol_sum/Pr/Pc*1e-6, msg_vol_max*1e-6);
./pdgstrf_irecv.c:1022:#if ( PRNTlevel==3 )
./pdgstrf_irecv.c:1024:    if ( !iam ) printf(".. # msg of zero size\t%d\n", iinfo);
./pdgstrf_irecv.c:1026:    if ( !iam ) printf(".. # total msg\t%d\n", iinfo);
./pdgstrf_irecv.c:1030:    for (i = 0; i < Pr * Pc; ++i) {
./pdgstrf_irecv.c:1032:	    dPrintLblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf_irecv.c:1033:	    dPrintUblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf_irecv.c:1034:	    printf("(%d)\n", iam);
./pdgstrf_irecv.c:1035:	    PrintInt10("Recv", nsupers, Llu->ToRecv);
./pdgstrf_irecv.c:1042:    printf("(%d) num_copy=%d, num_update=%d\n", iam, num_copy, num_update);
./pdgstrf_irecv.c:1053: * <pre>
./pdgstrf_irecv.c:1057: *   Only the process column that owns block column *k* participates
./pdgstrf_irecv.c:1070: *        Global data structures (xsup, supno) replicated on all processes.
./pdgstrf_irecv.c:1073: *        The 2D process mesh.
./pdgstrf_irecv.c:1089: * </pre>
./pdgstrf_irecv.c:1101:    int    nsupr; /* number of rows in the block (LDA) */
./pdgstrf_irecv.c:1115:    krow  = PROW( k, grid );
./pdgstrf_irecv.c:1116:    pkk   = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pdgstrf_irecv.c:1122:    if ( Llu->Lrowind_bc_ptr[j] ) nsupr = Llu->Lrowind_bc_ptr[j][1];
./pdgstrf_irecv.c:1129:	   the process column. */
./pdgstrf_irecv.c:1130:	if ( iam == pkk ) { /* Diagonal process. */
./pdgstrf_irecv.c:1134:#if ( PRNTlevel>=2 )
./pdgstrf_irecv.c:1135:		    printf("(%d) .. col %d, tiny pivot %e  ",
./pdgstrf_irecv.c:1141:#if ( PRNTlevel>=2 )
./pdgstrf_irecv.c:1142:		    printf("replaced by %e\n", lusup[i]);
./pdgstrf_irecv.c:1147:	    for (l = 0; l < c; ++l, i += nsupr)	ujrow[l] = lusup[i];
./pdgstrf_irecv.c:1160:	    printf("..(%d) k %d, j %d: Send ujrow[0] %e\n",iam,k,j,ujrow[0]);
./pdgstrf_irecv.c:1162:	    printf("..(%d) k %d, j %d: Recv ujrow[0] %e\n",iam,k,j,ujrow[0]);
./pdgstrf_irecv.c:1180:		for (i = luptr+1; i < luptr-j+nsupr; ++i) lusup[i] *= temp;
./pdgstrf_irecv.c:1181:		stat->ops[FACT] += nsupr-j-1;
./pdgstrf_irecv.c:1183:		for (i = luptr; i < luptr+nsupr; ++i) lusup[i] *= temp;
./pdgstrf_irecv.c:1184:		stat->ops[FACT] += nsupr;
./pdgstrf_irecv.c:1191:		l = nsupr - j - 1;
./pdgstrf_irecv.c:1194:		     &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pdgstrf_irecv.c:1197:		      &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pdgstrf_irecv.c:1202:		SGER(&nsupr, &c, &alpha, &lusup[luptr], &incx, 
./pdgstrf_irecv.c:1203:		     &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pdgstrf_irecv.c:1205:		dger_(&nsupr, &c, &alpha, &lusup[luptr], &incx, 
./pdgstrf_irecv.c:1206:		      &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pdgstrf_irecv.c:1208:		stat->ops[FACT] += 2 * nsupr * c;
./pdgstrf_irecv.c:1213:	if ( iam == pkk ) luptr += nsupr + 1;
./pdgstrf_irecv.c:1214:	else luptr += nsupr;
./pdgstrf_irecv.c:1240: *   Only the process row that owns block row *k* participates
./pdgstrf_irecv.c:1253: *        Global data structures (xsup, supno) replicated on all processes.
./pdgstrf_irecv.c:1256: *        The 2D process mesh.
./pdgstrf_irecv.c:1269:    int    nsupr; /* number of rows in the block L(:,k) (LDA) */
./pdgstrf_irecv.c:1284:    pkk  = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pdgstrf_irecv.c:1294:	nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pdgstrf_irecv.c:1297:	nsupr = Llu->Lsub_buf_2[k%2][1]; /* LDA of lusup[] */
./pdgstrf_irecv.c:1311:		luptr = (knsupc - segsize) * (nsupr + 1);
./pdgstrf_irecv.c:1313:		STRSV(ftcs1, ftcs2, ftcs3, &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_irecv.c:1316:		dtrsv_("L", "N", "U", &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_irecv.c:1319:		dtrsv_("L", "N", "U", &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_irecv.c:1331:probe_recv(int iam, int source, int tag, MPI_Datatype datatype, MPI_Comm comm,
./pdgstrf_irecv.c:1337:    MPI_Probe( source, tag, comm, &status );
./pdgstrf_irecv.c:1340:        printf("(%d) Recv'ed count %d > buffer size $d\n",
./pdgstrf_sherry.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrf_sherry.c:29: * Internal prototypes
./pdgstrf_sherry.c:80: *       krow = PROW( k, grid );
./pdgstrf_sherry.c:91: *       if ( iam == Pkk ) multicast L_k,k to this process row;
./pdgstrf_sherry.c:93: *          Recv L_k,k from process Pkk;
./pdgstrf_sherry.c:100: *       if ( myrow == krow ) multicast U_k,k+1:N to this process column;
./pdgstrf_sherry.c:101: *       if ( mycol == kcol ) multicast L_k+1:N,k to this process row;
./pdgstrf_sherry.c:104: *          Recv U_k,k+1:N from process Pkj;
./pdgstrf_sherry.c:108: *          Recv L_k+1:N,k from process Pik;
./pdgstrf_sherry.c:112: *              if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf_sherry.c:162: *           Global data structure (xsup, supno) replicated on all processes,
./pdgstrf_sherry.c:173: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrf_sherry.c:174: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrf_sherry.c:175: *        and my process rank. It is an input argument to all the
./pdgstrf_sherry.c:209:    int_t Pc, Pr;
./pdgstrf_sherry.c:212:    int   nsupr, nbrow, segsize;
./pdgstrf_sherry.c:224:    int_t  *iuip, *ruip;/* Pointers to U index/nzval; size ceil(NSUPERS/Pr). */
./pdgstrf_sherry.c:242:#if ( PRNTlevel==3 )
./pdgstrf_sherry.c:245:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:268:    Pr = grid->nprow;
./pdgstrf_sherry.c:282:    if ( Pr*Pc > 1 ) {
./pdgstrf_sherry.c:298:	       (MPI_Request *) SUPERLU_MALLOC(Pr*sizeof(MPI_Request))))
./pdgstrf_sherry.c:309:#if ( PRNTlevel>=1 )
./pdgstrf_sherry.c:311:	printf(".. thresh = s_eps %e * anorm %e = %e\n", s_eps, anorm, thresh);
./pdgstrf_sherry.c:312:	printf(".. Buffer size: Lsub %d\tLval %d\tUsub %d\tUval %d\tLDA %d\n",
./pdgstrf_sherry.c:338:    k = CEILING( nsupers, Pr ); /* Number of local block rows */
./pdgstrf_sherry.c:370:	scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_sherry.c:372:	/* Process column *kcol* multicasts numeric values of L(:,k) 
./pdgstrf_sherry.c:373:	   to process rows. */
./pdgstrf_sherry.c:385:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:396:		printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pdgstrf_sherry.c:402:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:412:	    scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_sherry.c:418:	    printf("(%d) Post Irecv L(:,%4d)\n", iam, 0);
./pdgstrf_sherry.c:429:	krow = PROW( k, grid );
./pdgstrf_sherry.c:446:		scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_sherry.c:447:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:453:		/*probe_recv(iam, kcol, (4*k)%NTAGS, mpi_int_t, scp->comm, 
./pdgstrf_sherry.c:459:		/*probe_recv(iam, kcol, (4*k+1)%NTAGS, MPI_DOUBLE, scp->comm, 
./pdgstrf_sherry.c:468:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:473:		printf("(%d) Recv L(:,%4d): lsub %4d, lusup %4d from Pc %2d\n",
./pdgstrf_sherry.c:479:#if ( PRNTlevel==3 )
./pdgstrf_sherry.c:486:	scp = &grid->cscp; /* The scope of process column. */
./pdgstrf_sherry.c:489:	    /* Parallel triangular solve across process row *krow* --
./pdgstrf_sherry.c:497:	    /* Multicasts U(k,:) to process columns. */
./pdgstrf_sherry.c:509:		for (pi = 0; pi < Pr; ++pi) {
./pdgstrf_sherry.c:511:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:524:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:531:			printf("(%d) Send U(%4d,:) to Pr %2d\n", iam, k, pi);
./pdgstrf_sherry.c:538:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:544:		/*probe_recv(iam, krow, (4*k+2)%NTAGS, mpi_int_t, scp->comm, 
./pdgstrf_sherry.c:549:		/*probe_recv(iam, krow, (4*k+3)%NTAGS, MPI_DOUBLE, scp->comm, 
./pdgstrf_sherry.c:557:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:564:		printf("(%d) Recv U(%4d,:) from Pr %2d\n", iam, k, krow);
./pdgstrf_sherry.c:566:#if ( PRNTlevel==3 )
./pdgstrf_sherry.c:571:	} /* if myrow == Pr(k) */
./pdgstrf_sherry.c:577:	 *         if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pdgstrf_sherry.c:584:	    nsupr = lsub[1]; /* LDA of lusup. */
./pdgstrf_sherry.c:619:		/* Prepare to call DGEMM. */
./pdgstrf_sherry.c:640:		  printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pdgstrf_sherry.c:668:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_sherry.c:672:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_sherry.c:676:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_sherry.c:735:/*#pragma _CRI cache_bypass nzval,tempv*/
./pdgstrf_sherry.c:769:	    /* Process column *kcol+1* multicasts numeric values of L(:,k+1) 
./pdgstrf_sherry.c:770:	       to process rows. */
./pdgstrf_sherry.c:780:	    scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_sherry.c:784:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:797:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:804:		    printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pdgstrf_sherry.c:811:		scp = &grid->rscp; /* The scope of process row. */
./pdgstrf_sherry.c:817:		printf("(%d) Post Irecv L(:,%4d)\n", iam, k+1);
./pdgstrf_sherry.c:835:		/* Prepare to call DGEMM. */
./pdgstrf_sherry.c:850:		printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pdgstrf_sherry.c:884:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_sherry.c:888:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_sherry.c:892:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pdgstrf_sherry.c:953:/*#pragma _CRI cache_bypass nzval,tempv*/
./pdgstrf_sherry.c:981:    if ( Pr*Pc > 1 ) {
./pdgstrf_sherry.c:989:	    for (krow = 0; krow < Pr; ++krow)
./pdgstrf_sherry.c:1002:    /* Prepare error message. */
./pdgstrf_sherry.c:1004:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:1008:#if ( PROFlevel>=1 )
./pdgstrf_sherry.c:1023:	    printf("\tPDGSTRF comm stat:"
./pdgstrf_sherry.c:1026:		   msg_cnt_sum/Pr/Pc, msg_cnt_max,
./pdgstrf_sherry.c:1027:		   msg_vol_sum/Pr/Pc*1e-6, msg_vol_max*1e-6);
./pdgstrf_sherry.c:1035:#if ( PRNTlevel==3 )
./pdgstrf_sherry.c:1037:    if ( !iam ) printf(".. # msg of zero size\t%d\n", iinfo);
./pdgstrf_sherry.c:1039:    if ( !iam ) printf(".. # total msg\t%d\n", iinfo);
./pdgstrf_sherry.c:1043:    for (i = 0; i < Pr * Pc; ++i) {
./pdgstrf_sherry.c:1045:	    dPrintLblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf_sherry.c:1046:	    dPrintUblocks(iam, nsupers, grid, Glu_persist, Llu);
./pdgstrf_sherry.c:1047:	    printf("(%d)\n", iam);
./pdgstrf_sherry.c:1048:	    PrintInt10("Recv", nsupers, Llu->ToRecv);
./pdgstrf_sherry.c:1055:    printf("(%d) num_copy=%d, num_update=%d\n", iam, num_copy, num_update);
./pdgstrf_sherry.c:1078: *   Only the column processes that owns block column *k* participate
./pdgstrf_sherry.c:1091: *        Global data structures (xsup, supno) replicated on all processes.
./pdgstrf_sherry.c:1094: *        The 2D process mesh.
./pdgstrf_sherry.c:1116:    int    cols_left, iam, l, pkk, pr;
./pdgstrf_sherry.c:1118:    int    nsupr; /* number of rows in the block (LDA) */
./pdgstrf_sherry.c:1123:    int_t  Pr;
./pdgstrf_sherry.c:1135:    Pr    = grid->nprow;
./pdgstrf_sherry.c:1136:    krow  = PROW( k, grid );
./pdgstrf_sherry.c:1137:    pkk   = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pdgstrf_sherry.c:1143:    if ( Llu->Lrowind_bc_ptr[j] ) nsupr = Llu->Lrowind_bc_ptr[j][1];
./pdgstrf_sherry.c:1150:    if ( iam == pkk ) { /* diagonal process */
./pdgstrf_sherry.c:1154:            for (pr = 0; pr < Pr; ++pr)
./pdgstrf_sherry.c:1155:                if (pr != krow)
./pdgstrf_sherry.c:1156:                    MPI_Wait(U_diag_blk_send_req + pr, &status);
./pdgstrf_sherry.c:1165:#if ( PRNTlevel>=2 )
./pdgstrf_sherry.c:1166:		    printf("(%d) .. col %d, tiny pivot %e  ",
./pdgstrf_sherry.c:1172:#if ( PRNTlevel>=2 )
./pdgstrf_sherry.c:1173:		    printf("replaced by %e\n", lusup[i]);
./pdgstrf_sherry.c:1179:	    for (l = 0; l < cols_left; ++l, i += nsupr, ++u_diag_cnt)
./pdgstrf_sherry.c:1186:		for (i = luptr+1; i < luptr-j+nsupr; ++i) lusup[i] *= temp;
./pdgstrf_sherry.c:1187:		stat->ops[FACT] += nsupr-j-1;
./pdgstrf_sherry.c:1192:		l = nsupr - j - 1;
./pdgstrf_sherry.c:1195:		     &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pdgstrf_sherry.c:1198:		      &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pdgstrf_sherry.c:1204:	    luptr += nsupr + 1;	                 /* move to next column */
./pdgstrf_sherry.c:1210:	    for (pr = 0; pr < Pr; ++pr)
./pdgstrf_sherry.c:1211:		if (pr != krow)
./pdgstrf_sherry.c:1212:		    MPI_Isend(ublk_ptr, u_diag_cnt, MPI_DOUBLE, pr,
./pdgstrf_sherry.c:1213:			      ((k<<2)+2)%NTAGS, comm, U_diag_blk_send_req + pr);
./pdgstrf_sherry.c:1217:    } else  { /* non-diagonal process */
./pdgstrf_sherry.c:1238:		for (i = luptr; i < luptr+nsupr; ++i) lusup[i] *= temp;
./pdgstrf_sherry.c:1239:		stat->ops[FACT] += nsupr;
./pdgstrf_sherry.c:1245:		SGER(&nsupr, &cols_left, &alpha, &lusup[luptr], &incx, 
./pdgstrf_sherry.c:1246:		     &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pdgstrf_sherry.c:1248:		dger_(&nsupr, &cols_left, &alpha, &lusup[luptr], &incx, 
./pdgstrf_sherry.c:1249:		      &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pdgstrf_sherry.c:1251:		stat->ops[FACT] += 2 * nsupr * cols_left;
./pdgstrf_sherry.c:1256:	    luptr += nsupr;                      /* move to next column */
./pdgstrf_sherry.c:1284: *   Only the process row that owns block row *k* participates
./pdgstrf_sherry.c:1297: *        Global data structures (xsup, supno) replicated on all processes.
./pdgstrf_sherry.c:1300: *        The 2D process mesh.
./pdgstrf_sherry.c:1313:    int    nsupr; /* number of rows in the block L(:,k) (LDA) */
./pdgstrf_sherry.c:1328:    pkk  = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pdgstrf_sherry.c:1338:	nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pdgstrf_sherry.c:1341:	nsupr = Llu->Lsub_buf_2[k%2][1]; /* LDA of lusup[] */
./pdgstrf_sherry.c:1355:		luptr = (knsupc - segsize) * (nsupr + 1);
./pdgstrf_sherry.c:1357:		STRSV(ftcs1, ftcs2, ftcs3, &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_sherry.c:1360:		dtrsv_("L", "N", "U", &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_sherry.c:1363:		dtrsv_("L", "N", "U", &segsize, &lusup[luptr], &nsupr, 
./pdgstrf_sherry.c:1375:probe_recv(int iam, int source, int tag, MPI_Datatype datatype, MPI_Comm comm,
./pdgstrf_sherry.c:1381:    MPI_Probe( source, tag, comm, &status );
./pdgstrf_sherry.c:1384:        printf("(%d) Recv'ed count %d > buffer size $d\n",
./pdgstrs.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrs.c:15: * general N-by-N matrix A using the LU factors computed previously.
./pdgstrs.c:17: * <pre>
./pdgstrs.c:22: * </pre>
./pdgstrs.c:43: *          if all local updates done, Isend lsum[] to diagonal process;
./pdgstrs.c:45: *      } else if ( message is LSUM ) { .. this must be a diagonal process 
./pdgstrs.c:49: *              Isend Xi down to the current process column;
./pdgstrs.c:62: *   + prepend a header recording the global block number.
./pdgstrs.c:90: * Function prototypes
./pdgstrs.c:102: * <pre>
./pdgstrs.c:105: *   Re-distribute B on the diagonal processes of the 2D process mesh.
./pdgstrs.c:135: *        The solution vector. It is valid only on the diagonal processes.
./pdgstrs.c:142: *        The 2D process mesh.
./pdgstrs.c:150: * </pre>
./pdgstrs.c:168:    int    p, procs;
./pdgstrs.c:183:    procs = grid->nprow * grid->npcol;
./pdgstrs.c:187:    SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
./pdgstrs.c:188:    RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
./pdgstrs.c:189:    RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
./pdgstrs.c:190:    sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
./pdgstrs.c:191:    sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
./pdgstrs.c:192:    rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
./pdgstrs.c:193:    rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
./pdgstrs.c:201:	if(procs==1){ // faster memory copy when procs=1 
./pdgstrs.c:204:#pragma omp parallel default (shared)
./pdgstrs.c:208:#pragma omp master
./pdgstrs.c:213:#pragma	omp	taskloop private (i,l,irow,k,j,knsupc) untied 
./pdgstrs.c:216:			irow = perm_c[perm_r[i+fst_row]]; /* Row number in Pc*Pr*B */
./pdgstrs.c:222:			x[l - XK_H] = k;      /* Block number prepended in the header. */
./pdgstrs.c:232:		k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pdgstrs.c:233:		l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pdgstrs.c:240:		if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c:242:		if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c:244:		if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c:246:		if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c:249:		for (p = 0; p < procs; ++p) {
./pdgstrs.c:257:			irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
./pdgstrs.c:259:		p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
./pdgstrs.c:272:		// printf(".. copy to send buffer time\t%8.4f\n", t);	
./pdgstrs.c:294:		   Copy buffer into X on the diagonal processes.
./pdgstrs.c:299:		for (p = 0; p < procs; ++p) {
./pdgstrs.c:302:			/* Only the diagonal processes do this; the off-diagonal processes
./pdgstrs.c:309:			x[l - XK_H] = k;      /* Block number prepended in the header. */
./pdgstrs.c:320:		// printf(".. copy to x time\t%8.4f\n", t);	
./pdgstrs.c:339: * <pre>
./pdgstrs.c:342: *   Re-distribute X on the diagonal processes to B distributed on all
./pdgstrs.c:343: *   the processes.
./pdgstrs.c:349: * </pre>
./pdgstrs.c:366:    int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
./pdgstrs.c:368:    int  iam, p, q, pkk, procs;
./pdgstrs.c:369:    int_t  num_diag_procs, *diag_procs;
./pdgstrs.c:385:    procs = grid->nprow * grid->npcol;
./pdgstrs.c:388:    SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
./pdgstrs.c:389:    RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
./pdgstrs.c:390:    RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
./pdgstrs.c:391:    sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
./pdgstrs.c:392:    sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
./pdgstrs.c:393:    rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
./pdgstrs.c:394:    rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
./pdgstrs.c:399:	if(procs==1){ //faster memory copy when procs=1
./pdgstrs.c:402:#pragma omp parallel default (shared)
./pdgstrs.c:406:#pragma omp master
./pdgstrs.c:411:#pragma	omp	taskloop private (k,knsupc,lk,irow,l,i,j) untied 
./pdgstrs.c:427:		k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pdgstrs.c:428:		l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pdgstrs.c:434:		if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c:436:		if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c:438:		if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c:440:		if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c:443:		for (p = 0; p < procs; ++p) {
./pdgstrs.c:447:		num_diag_procs = SOLVEstruct->num_diag_procs;
./pdgstrs.c:448:		diag_procs = SOLVEstruct->diag_procs;
./pdgstrs.c:449: 		for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
./pdgstrs.c:450:		pkk = diag_procs[p];
./pdgstrs.c:452:			for (k = p; k < nsupers; k += num_diag_procs) {
./pdgstrs.c:463:				q = row_to_proc[ii];
./pdgstrs.c:526: * <pre>
./pdgstrs.c:531: * </pre>
./pdgstrs.c:551:    int    Pc, Pr, iam;
./pdgstrs.c:552:    int    knsupc, nsupr;
./pdgstrs.c:565:#if ( PROFlevel>=1 )
./pdgstrs.c:569:#if ( PRNTlevel>=1 )
./pdgstrs.c:571:	printf("computing inverse of diagonal blocks...\n");
./pdgstrs.c:581:    Pr = grid->nprow;
./pdgstrs.c:591:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrs.c:600:         krow = PROW( k, grid );
./pdgstrs.c:604:	     if ( mycol == kcol ) { /* diagonal process */
./pdgstrs.c:611:		  nsupr = lsub[1];	
./pdgstrs.c:624:		          Linv[j*knsupc+i] = lusup[j*nsupr+i];	
./pdgstrs.c:627:			  Uinv[j*knsupc+i] = lusup[j*nsupr+i];	
./pdgstrs.c:640:#if ( PROFlevel>=1 )
./pdgstrs.c:643:	printf(".. L-diag_inv time\t%10.5f\n", t);
./pdgstrs.c:655: * <pre>
./pdgstrs.c:664: *     A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgstrs.c:666: *     A1 * Y = Pc*Pr*B1, where B was overwritten by B1 = diag(R)*B, and
./pdgstrs.c:667: * the permutation to B1 by Pc*Pr is applied internally in this routine.
./pdgstrs.c:681: *        A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgstrs.c:684: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrs.c:685: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrs.c:686: *        and my process rank. It is an input argument to all the
./pdgstrs.c:721: * </pre>       
./pdgstrs.c:772:    int    Pc, Pr, iam;
./pdgstrs.c:773:    int    knsupc, nsupr, nprobe;
./pdgstrs.c:790:    			 Count the number of local block products to
./pdgstrs.c:797:    			 from processes in this row. 
./pdgstrs.c:798:    			 It is only valid on the diagonal processes. */
./pdgstrs.c:813:    			 processes in this row. */
./pdgstrs.c:827:    int_t tmpresult;
./pdgstrs.c:829:    // #if ( PROFlevel>=1 )
./pdgstrs.c:843:	int_t procs = grid->nprow * grid->npcol;
./pdgstrs.c:864:	#pragma omp threadprivate(thread_id)
./pdgstrs.c:868:#pragma omp parallel default(shared)
./pdgstrs.c:877:#if ( PRNTlevel>=1 )
./pdgstrs.c:879:	printf("num_thread: %5d\n", num_thread);
./pdgstrs.c:901:    Pr = grid->nprow;
./pdgstrs.c:911:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrs.c:934:    if ( !(leaf_send = intMalloc_dist((CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))*aln_i)) )
./pdgstrs.c:937:    if ( !(root_send = intMalloc_dist((CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))*aln_i)) )
./pdgstrs.c:948:    /* Obtain ilsum[] and ldalsum for process column 0. */
./pdgstrs.c:961:#pragma omp parallel default(shared) private(ii)
./pdgstrs.c:981:#pragma omp parallel default(shared) private(ii)
./pdgstrs.c:1008:    /* Redistribute B into X on the diagonal processes. */
./pdgstrs.c:1009:     //PROFILE_PHYSICS_INIT();
./pdgstrs.c:1012:     //PROFILE_PHYSICS_FINISH();
./pdgstrs.c:1013:#if ( PRNTlevel>=1 )
./pdgstrs.c:1015:    if ( !iam) printf(".. B to X redistribute time\t%8.4f\n", t);
./pdgstrs.c:1022:	#pragma omp simd lastprivate(krow,lk,il)
./pdgstrs.c:1025:	krow = PROW( k, grid );
./pdgstrs.c:1029:	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
./pdgstrs.c:1034:	   Initialize the async Bcast trees on all processes.
./pdgstrs.c:1036:	//PROFILE_DYNAMIC_INIT();
./pdgstrs.c:1042:			// printf("LBtree_ptr lk %5d\n",lk); 
./pdgstrs.c:1051:	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pdgstrs.c:1061:if(procs==1){
./pdgstrs.c:1063:		gb = myrow+lk*grid->nprow;  /* not sure */
./pdgstrs.c:1079:			gb = myrow+lk*grid->nprow;  /* not sure */
./pdgstrs.c:1082:				if(mycol==kcol) { /* Diagonal process */
./pdgstrs.c:1095:#pragma omp simd
./pdgstrs.c:1100:			//PROFILE_DYNAMIC_FINISH();						
./pdgstrs.c:1106:        int BC_buffer_size = proc * maxrecvsz*(recvbc+1) + 1; 
./pdgstrs.c:1107:        int RD_buffer_size = proc * maxrecvsz*(nfrecvmod+1) + 1; 
./pdgstrs.c:1110:        //int BC_buffer_size = ceil( sqrt(proc) ) * maxrecvsz*(recvbc+1) + 1; 
./pdgstrs.c:1111:        //int RD_buffer_size = ceil( sqrt(proc) ) * maxrecvsz*(nfrecvmod+1) + 1; 
./pdgstrs.c:1124:	if ( !(BCsendoffset = (int*)SUPERLU_MALLOC( proc * sizeof(int))) )  // this needs to be optimized for 1D row mapping
./pdgstrs.c:1126:	if ( !(RDsendoffset = (int*)SUPERLU_MALLOC( proc * sizeof(int))) )  // this needs to be optimized for 1D row mapping
./pdgstrs.c:1129:	memset(BCsendoffset, 0, proc * sizeof(int));
./pdgstrs.c:1130:        memset(RDsendoffset, 0, proc * sizeof(int));
./pdgstrs.c:1134:        printf("iam %d, winl %d, winl_rd, %d, BC_max_tasknum=%d,RD_max_tasknum=%d\n",iam, winl,winl_rd, recvbc,nfrecvmod);
./pdgstrs.c:1143:        printf("iam %d, BC_max_tasknum=%d,RD_max_tasknum=%d\n",iam,recvbc, nfrecvmod);
./pdgstrs.c:1147:	printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n,  nbtree %4d\n,  nrtree %4d\n",
./pdgstrs.c:1152:#if ( PRNTlevel>=1 )
./pdgstrs.c:1154:	if ( !iam) printf(".. Setup L-solve time\t%8.4f\n", t);
./pdgstrs.c:1171:	   Solve the leaf nodes first by all the diagonal processes.
./pdgstrs.c:1174:	printf("(%2d) nleaf %4d\n", iam, nleaf);
./pdgstrs.c:1180:#pragma omp parallel default (shared) 
./pdgstrs.c:1187:#pragma	omp	for firstprivate(nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nowait	
./pdgstrs.c:1192:#if ( PROFlevel>=1 )
./pdgstrs.c:1204:				nsupr = lsub[1];
./pdgstrs.c:1222:#pragma omp simd
./pdgstrs.c:1229:#if ( PROFlevel>=1 )
./pdgstrs.c:1236:				printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c:1239:			 	* Send Xk to process column Pc[k].
./pdgstrs.c:1247:#pragma omp atomic capture
./pdgstrs.c:1256:#pragma	omp	for firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nowait	
./pdgstrs.c:1262:#if ( PROFlevel>=1 )
./pdgstrs.c:1275:				nsupr = lsub[1];
./pdgstrs.c:1280:							lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:1283:							lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
./pdgstrs.c:1286:							lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:1290:#if ( PROFlevel>=1 )
./pdgstrs.c:1298:				printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c:1302:				 * Send Xk to process column Pc[k].
./pdgstrs.c:1310:#pragma omp atomic capture
./pdgstrs.c:1324:#pragma omp parallel default (shared)
./pdgstrs.c:1329:#pragma omp master
./pdgstrs.c:1334:#pragma	omp	taskloop private (k,ii,lk) num_tasks(num_thread*8) nogroup
./pdgstrs.c:1340:			/* Diagonal process */
./pdgstrs.c:1378:        printf("iam %d, now receive BCtask %f, RDtask %f\n",iam, recvbuf_BC_fwd[0], recvbuf_RD_fwd[0]);      
./pdgstrs.c:1387:			   Compute the internal nodes asynchronously by all processes.
./pdgstrs.c:1422:		                krow = PROW( k, grid );
./pdgstrs.c:1463:	#pragma omp simd
./pdgstrs.c:1471:	#pragma omp simd
./pdgstrs.c:1479:	        			nsupr = lsub[1];
./pdgstrs.c:1481:#if ( PROFlevel>=1 )
./pdgstrs.c:1501:	#pragma omp simd
./pdgstrs.c:1509:							lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:1512:							lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
./pdgstrs.c:1515:								lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:1519:#if ( PROFlevel>=1 )
./pdgstrs.c:1526:					printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c:1530:               				 * Send Xk to process column Pc[k].
./pdgstrs.c:1544:               					krow = PROW( k, grid );
./pdgstrs.c:1559:#pragma omp simd
./pdgstrs.c:1572:#if ( PRNTlevel>=1 )
./pdgstrs.c:1576:			printf(".. L-solve time\t%8.4f\n", t);
./pdgstrs.c:1584:			printf(".. L-solve time (MAX) \t%8.4f\n", tmax);	
./pdgstrs.c:1595:			printf("(%d) .. After L-solve: y =\n", iam);
./pdgstrs.c:1597:				krow = PROW( k, grid );
./pdgstrs.c:1599:				if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs.c:1604:						printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pdgstrs.c:1645:		 * on the diagonal processes.
./pdgstrs.c:1648:	//PROFILE_BAROTROPIC_INIT();	 
./pdgstrs.c:1664:#pragma omp parallel default(shared) private(ii)
./pdgstrs.c:1671:	#pragma omp simd lastprivate(krow,lk,il)
./pdgstrs.c:1674:	krow = PROW( k, grid );
./pdgstrs.c:1678:	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
./pdgstrs.c:1684:		krow = PROW( k, grid );
./pdgstrs.c:1701:		for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs.c:1703:				printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pdgstrs.c:1705:					printf("(%2d) Local col %2d: # row blocks %2d\n",
./pdgstrs.c:1709:							printf("(%2d) .. row blk %2d:\
./pdgstrs.c:1720:		for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs.c:1722:				printf("\n(%d) bsendx_plist[][]", iam);
./pdgstrs.c:1724:					printf("\n(%d) .. local col %2d: ", iam, lb);
./pdgstrs.c:1725:					for (i = 0; i < Pr; ++i)
./pdgstrs.c:1726:						printf("%4d", bsendx_plist[lb][i]);
./pdgstrs.c:1728:				printf("\n");
./pdgstrs.c:1738:	   Initialize the async Bcast trees on all processes.
./pdgstrs.c:1745:			// printf("UBtree_ptr lk %5d\n",lk); 
./pdgstrs.c:1754:	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pdgstrs.c:1762:			// printf("here lk %5d myid %5d\n",lk,iam);
./pdgstrs.c:1769:			gb = myrow+lk*grid->nprow;  /* not sure */
./pdgstrs.c:1772:				if(mycol==kcol) { /* Diagonal process */
./pdgstrs.c:1783:	#pragma omp simd
./pdgstrs.c:1786:	// for (i = 0; i < nlb; ++i)printf("bmod[i]: %5d\n",bmod[i]);
./pdgstrs.c:1792://PROFILE_BAROTROPIC_FINISH();
./pdgstrs.c:1794:	printf("(%2d) nbrecvx %4d,  nbrecvmod %4d,  nroot %4d\n,  nbtree %4d\n,  nrtree %4d\n",
./pdgstrs.c:1800:#if ( PRNTlevel>=1 )
./pdgstrs.c:1802:	if ( !iam) printf(".. Setup U-solve time\t%8.4f\n", t);
./pdgstrs.c:1809:		 * Solve the roots first by all the diagonal processes.
./pdgstrs.c:1812:		printf("(%2d) nroot %4d\n", iam, nroot);
./pdgstrs.c:1817://PROFILE_LND_INIT();
./pdgstrs.c:1819:#pragma omp parallel default (shared) 
./pdgstrs.c:1823:#pragma omp master
./pdgstrs.c:1827:#pragma	omp	taskloop firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,jj,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Uinv,i,lib,rtemp_loc,nroot_send_tmp) nogroup		
./pdgstrs.c:1832:#if ( PROFlevel>=1 )
./pdgstrs.c:1848:			nsupr = lsub[1];
./pdgstrs.c:1868:					#pragma omp simd
./pdgstrs.c:1876:						lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:1879:						lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
./pdgstrs.c:1882:						lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:1886:			// printf("x_u: %f\n",x[ii+i]);
./pdgstrs.c:1891:				// printf("x: %f\n",x[ii+i]);
./pdgstrs.c:1895:#if ( PROFlevel>=1 )
./pdgstrs.c:1902:			printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c:1906:			 * Send Xk to process column Pc[k].
./pdgstrs.c:1911:#pragma omp atomic capture
./pdgstrs.c:1923:#pragma omp parallel default (shared) 
./pdgstrs.c:1927:#pragma omp master
./pdgstrs.c:1931:#pragma	omp	taskloop private (ii,jj,k,lk) nogroup		
./pdgstrs.c:1968:		 * Compute the internal nodes asychronously by all processes.
./pdgstrs.c:1972:#pragma omp parallel default (shared) 
./pdgstrs.c:1976:#pragma omp master 
./pdgstrs.c:1980:			// printf("iam %4d nbrecv %4d nbrecvx %4d nbrecvmod %4d\n", iam, nbrecv, nbrecvxnbrecvmod);
./pdgstrs.c:1986:#if ( PROFlevel>=1 )
./pdgstrs.c:1996:#if ( PROFlevel>=1 )		 
./pdgstrs.c:2006:			printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs.c:2039:						#pragma omp simd
./pdgstrs.c:2046:			// #pragma omp atomic capture
./pdgstrs.c:2057:								#pragma omp simd
./pdgstrs.c:2065:								#pragma omp simd
./pdgstrs.c:2073:						nsupr = lsub[1];
./pdgstrs.c:2094:								#pragma omp simd
./pdgstrs.c:2102:									lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:2105:									lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
./pdgstrs.c:2108:									lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c:2112:#if ( PROFlevel>=1 )
./pdgstrs.c:2119:						printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c:2123:						 * Send Xk to process column Pc[k].
./pdgstrs.c:2145:								#pragma omp simd
./pdgstrs.c:2157:        //PROFILE_LND_FINISH();
./pdgstrs.c:2158:#if ( PRNTlevel>=1 )
./pdgstrs.c:2161:		if ( !iam ) printf(".. U-solve time\t%8.4f\n", t);
./pdgstrs.c:2165:			printf(".. U-solve time (MAX) \t%8.4f\n", tmax);	
./pdgstrs.c:2178:			printf("\n(%d) .. After U-solve: x (ON DIAG PROCS) = \n", iam);
./pdgstrs.c:2182:				krow = PROW( k, grid );
./pdgstrs.c:2185:				if ( iam == diag ) { /* Diagonal process. */
./pdgstrs.c:2191:							printf("\t(%d)\t%4d\t%.10f\n",
./pdgstrs.c:2201://PROFILE_ICE_INIT();
./pdgstrs.c:2204://PROFILE_ICE_FINISH();
./pdgstrs.c:2206:#if ( PRNTlevel>=1 )
./pdgstrs.c:2208:		if ( !iam) printf(".. X to B redistribute time\t%8.4f\n", t);
./pdgstrs.c:2222:#if ( PRNTlevel>=2 )
./pdgstrs.c:2223:			if(iam==0)printf("thread %5d gemm %9.5f\n",i,stat_loc[i]->utime[SOL_GEMM]);
./pdgstrs.c:2272:#if ( PROFlevel>=2 )
./pdgstrs.c:2285:				printf ("\tPDGSTRS comm stat:"
./pdgstrs.c:2288:						msg_cnt_sum / Pr / Pc, msg_cnt_max,
./pdgstrs.c:2289:						msg_vol_sum / Pr / Pc * 1e-6, msg_vol_max * 1e-6);
./pdgstrs.c.bak:4:approvals from U.S. Dept. of Energy) 
./pdgstrs.c.bak:15: * general N-by-N matrix A using the LU factors computed previously.
./pdgstrs.c.bak:17: * <pre>
./pdgstrs.c.bak:22: * </pre>
./pdgstrs.c.bak:43: *          if all local updates done, Isend lsum[] to diagonal process;
./pdgstrs.c.bak:45: *      } else if ( message is LSUM ) { .. this must be a diagonal process 
./pdgstrs.c.bak:49: *              Isend Xi down to the current process column;
./pdgstrs.c.bak:62: *   + prepend a header recording the global block number.
./pdgstrs.c.bak:90: * Function prototypes
./pdgstrs.c.bak:102: * <pre>
./pdgstrs.c.bak:105: *   Re-distribute B on the diagonal processes of the 2D process mesh.
./pdgstrs.c.bak:135: *        The solution vector. It is valid only on the diagonal processes.
./pdgstrs.c.bak:142: *        The 2D process mesh.
./pdgstrs.c.bak:150: * </pre>
./pdgstrs.c.bak:168:    int    p, procs;
./pdgstrs.c.bak:183:    procs = grid->nprow * grid->npcol;
./pdgstrs.c.bak:187:    SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
./pdgstrs.c.bak:188:    RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
./pdgstrs.c.bak:189:    RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
./pdgstrs.c.bak:190:    sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
./pdgstrs.c.bak:191:    sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
./pdgstrs.c.bak:192:    rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
./pdgstrs.c.bak:193:    rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
./pdgstrs.c.bak:201:	if(procs==1){ // faster memory copy when procs=1 
./pdgstrs.c.bak:204:#pragma omp parallel default (shared)
./pdgstrs.c.bak:208:#pragma omp master
./pdgstrs.c.bak:213:#pragma	omp	taskloop private (i,l,irow,k,j,knsupc) untied 
./pdgstrs.c.bak:216:			irow = perm_c[perm_r[i+fst_row]]; /* Row number in Pc*Pr*B */
./pdgstrs.c.bak:222:			x[l - XK_H] = k;      /* Block number prepended in the header. */
./pdgstrs.c.bak:232:		k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pdgstrs.c.bak:233:		l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pdgstrs.c.bak:240:		if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c.bak:242:		if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c.bak:244:		if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c.bak:246:		if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c.bak:249:		for (p = 0; p < procs; ++p) {
./pdgstrs.c.bak:257:			irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
./pdgstrs.c.bak:259:		p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
./pdgstrs.c.bak:272:		// printf(".. copy to send buffer time\t%8.4f\n", t);	
./pdgstrs.c.bak:294:		   Copy buffer into X on the diagonal processes.
./pdgstrs.c.bak:299:		for (p = 0; p < procs; ++p) {
./pdgstrs.c.bak:302:			/* Only the diagonal processes do this; the off-diagonal processes
./pdgstrs.c.bak:309:			x[l - XK_H] = k;      /* Block number prepended in the header. */
./pdgstrs.c.bak:320:		// printf(".. copy to x time\t%8.4f\n", t);	
./pdgstrs.c.bak:339: * <pre>
./pdgstrs.c.bak:342: *   Re-distribute X on the diagonal processes to B distributed on all
./pdgstrs.c.bak:343: *   the processes.
./pdgstrs.c.bak:349: * </pre>
./pdgstrs.c.bak:366:    int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
./pdgstrs.c.bak:368:    int  iam, p, q, pkk, procs;
./pdgstrs.c.bak:369:    int_t  num_diag_procs, *diag_procs;
./pdgstrs.c.bak:385:    procs = grid->nprow * grid->npcol;
./pdgstrs.c.bak:388:    SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
./pdgstrs.c.bak:389:    RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
./pdgstrs.c.bak:390:    RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
./pdgstrs.c.bak:391:    sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
./pdgstrs.c.bak:392:    sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
./pdgstrs.c.bak:393:    rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
./pdgstrs.c.bak:394:    rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
./pdgstrs.c.bak:399:	if(procs==1){ //faster memory copy when procs=1
./pdgstrs.c.bak:402:#pragma omp parallel default (shared)
./pdgstrs.c.bak:406:#pragma omp master
./pdgstrs.c.bak:411:#pragma	omp	taskloop private (k,knsupc,lk,irow,l,i,j) untied 
./pdgstrs.c.bak:427:		k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pdgstrs.c.bak:428:		l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pdgstrs.c.bak:434:		if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c.bak:436:		if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pdgstrs.c.bak:438:		if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c.bak:440:		if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pdgstrs.c.bak:443:		for (p = 0; p < procs; ++p) {
./pdgstrs.c.bak:447:		num_diag_procs = SOLVEstruct->num_diag_procs;
./pdgstrs.c.bak:448:		diag_procs = SOLVEstruct->diag_procs;
./pdgstrs.c.bak:449: 		for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
./pdgstrs.c.bak:450:		pkk = diag_procs[p];
./pdgstrs.c.bak:452:			for (k = p; k < nsupers; k += num_diag_procs) {
./pdgstrs.c.bak:463:				q = row_to_proc[ii];
./pdgstrs.c.bak:526: * <pre>
./pdgstrs.c.bak:531: * </pre>
./pdgstrs.c.bak:551:    int    Pc, Pr, iam;
./pdgstrs.c.bak:552:    int    knsupc, nsupr;
./pdgstrs.c.bak:565:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:569:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:571:	printf("computing inverse of diagonal blocks...\n");
./pdgstrs.c.bak:581:    Pr = grid->nprow;
./pdgstrs.c.bak:591:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrs.c.bak:600:         krow = PROW( k, grid );
./pdgstrs.c.bak:604:	     if ( mycol == kcol ) { /* diagonal process */
./pdgstrs.c.bak:611:		  nsupr = lsub[1];	
./pdgstrs.c.bak:624:		          Linv[j*knsupc+i] = lusup[j*nsupr+i];	
./pdgstrs.c.bak:627:			  Uinv[j*knsupc+i] = lusup[j*nsupr+i];	
./pdgstrs.c.bak:640:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:643:	printf(".. L-diag_inv time\t%10.5f\n", t);
./pdgstrs.c.bak:655: * <pre>
./pdgstrs.c.bak:664: *     A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgstrs.c.bak:666: *     A1 * Y = Pc*Pr*B1, where B was overwritten by B1 = diag(R)*B, and
./pdgstrs.c.bak:667: * the permutation to B1 by Pc*Pr is applied internally in this routine.
./pdgstrs.c.bak:681: *        A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgstrs.c.bak:684: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrs.c.bak:685: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrs.c.bak:686: *        and my process rank. It is an input argument to all the
./pdgstrs.c.bak:721: * </pre>       
./pdgstrs.c.bak:770:    int    Pc, Pr, iam;
./pdgstrs.c.bak:771:    int    knsupc, nsupr, nprobe;
./pdgstrs.c.bak:788:    			 Count the number of local block products to
./pdgstrs.c.bak:795:    			 from processes in this row. 
./pdgstrs.c.bak:796:    			 It is only valid on the diagonal processes. */
./pdgstrs.c.bak:811:    			 processes in this row. */
./pdgstrs.c.bak:825:    int_t tmpresult;
./pdgstrs.c.bak:827:    // #if ( PROFlevel>=1 )
./pdgstrs.c.bak:841:	int_t procs = grid->nprow * grid->npcol;
./pdgstrs.c.bak:862:	#pragma omp threadprivate(thread_id)
./pdgstrs.c.bak:866:#pragma omp parallel default(shared)
./pdgstrs.c.bak:875:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:877:	printf("num_thread: %5d\n", num_thread);
./pdgstrs.c.bak:899:    Pr = grid->nprow;
./pdgstrs.c.bak:909:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrs.c.bak:932:    if ( !(leaf_send = intMalloc_dist((CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))*aln_i)) )
./pdgstrs.c.bak:935:    if ( !(root_send = intMalloc_dist((CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))*aln_i)) )
./pdgstrs.c.bak:946:    /* Obtain ilsum[] and ldalsum for process column 0. */
./pdgstrs.c.bak:959:#pragma omp parallel default(shared) private(ii)
./pdgstrs.c.bak:979:#pragma omp parallel default(shared) private(ii)
./pdgstrs.c.bak:1006:    /* Redistribute B into X on the diagonal processes. */
./pdgstrs.c.bak:1007:     //PROFILE_PHYSICS_INIT();
./pdgstrs.c.bak:1010:     //PROFILE_PHYSICS_FINISH();
./pdgstrs.c.bak:1011:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:1013:    if ( !iam) printf(".. B to X redistribute time\t%8.4f\n", t);
./pdgstrs.c.bak:1020:	#pragma omp simd lastprivate(krow,lk,il)
./pdgstrs.c.bak:1023:	krow = PROW( k, grid );
./pdgstrs.c.bak:1027:	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
./pdgstrs.c.bak:1032:	   Initialize the async Bcast trees on all processes.
./pdgstrs.c.bak:1034:	//PROFILE_DYNAMIC_INIT();
./pdgstrs.c.bak:1040:			// printf("LBtree_ptr lk %5d\n",lk); 
./pdgstrs.c.bak:1049:	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pdgstrs.c.bak:1059:if(procs==1){
./pdgstrs.c.bak:1061:		gb = myrow+lk*grid->nprow;  /* not sure */
./pdgstrs.c.bak:1077:			gb = myrow+lk*grid->nprow;  /* not sure */
./pdgstrs.c.bak:1080:				if(mycol==kcol) { /* Diagonal process */
./pdgstrs.c.bak:1093:#pragma omp simd
./pdgstrs.c.bak:1104:			//PROFILE_DYNAMIC_FINISH();						
./pdgstrs.c.bak:1110:        printf("iam %d, winl %d, winl_rd, %d, BC_max_tasknum=%d,RD_max_tasknum=%d\n",iam, winl,winl_rd, recvbc,nfrecvmod);
./pdgstrs.c.bak:1115:        printf("iam %d, BC_max_tasknum=%d,RD_max_tasknum=%d\n",iam,recvbc, nfrecvmod);
./pdgstrs.c.bak:1119:	printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n,  nbtree %4d\n,  nrtree %4d\n",
./pdgstrs.c.bak:1124:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:1126:	if ( !iam) printf(".. Setup L-solve time\t%8.4f\n", t);
./pdgstrs.c.bak:1143:	   Solve the leaf nodes first by all the diagonal processes.
./pdgstrs.c.bak:1146:	printf("(%2d) nleaf %4d\n", iam, nleaf);
./pdgstrs.c.bak:1151://PROFILE_BAROCLINIC_INIT();
./pdgstrs.c.bak:1153:#pragma omp parallel default (shared) 
./pdgstrs.c.bak:1161:#pragma	omp	for firstprivate(nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nowait	
./pdgstrs.c.bak:1167:				// #pragma	omp	task firstprivate (k,nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Linv,i,lib,rtemp_loc)	 	
./pdgstrs.c.bak:1171:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1185:					nsupr = lsub[1];
./pdgstrs.c.bak:1203:					#pragma omp simd
./pdgstrs.c.bak:1210:					// printf("x_l: %f\n",x[ii+i]);
./pdgstrs.c.bak:1215:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1226:					printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c.bak:1230:					 * Send Xk to process column Pc[k].
./pdgstrs.c.bak:1238:#pragma omp atomic capture
./pdgstrs.c.bak:1248:#pragma	omp	for firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nowait	
./pdgstrs.c.bak:1254:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1268:					nsupr = lsub[1];
./pdgstrs.c.bak:1273:							lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:1276:							lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
./pdgstrs.c.bak:1279:							lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:1284:					// printf("x_l: %f\n",x[ii+i]);
./pdgstrs.c.bak:1289:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1300:					printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c.bak:1304:					 * Send Xk to process column Pc[k].
./pdgstrs.c.bak:1312:#pragma omp atomic capture
./pdgstrs.c.bak:1326:#pragma omp parallel default (shared)
./pdgstrs.c.bak:1332:#pragma omp master
./pdgstrs.c.bak:1337:#pragma	omp	taskloop private (k,ii,lk) num_tasks(num_thread*8) nogroup
./pdgstrs.c.bak:1344:							/* Diagonal process */
./pdgstrs.c.bak:1355:						// } /* if diagonal process ... */
./pdgstrs.c.bak:1385:                                                          printf("iam %d, now receive BCtask %f, RDtask %f\n",iam, recvbuf_BC_fwd[0], recvbuf_RD_fwd[0]);      
./pdgstrs.c.bak:1394:			   Compute the internal nodes asynchronously by all processes.
./pdgstrs.c.bak:1418:		                krow = PROW( k, grid );
./pdgstrs.c.bak:1459:	#pragma omp simd
./pdgstrs.c.bak:1467:	#pragma omp simd
./pdgstrs.c.bak:1475:	        			nsupr = lsub[1];
./pdgstrs.c.bak:1477:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1497:	#pragma omp simd
./pdgstrs.c.bak:1505:							lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:1508:							lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
./pdgstrs.c.bak:1511:								lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:1515:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1522:					printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c.bak:1526:               				 * Send Xk to process column Pc[k].
./pdgstrs.c.bak:1540:               					krow = PROW( k, grid );
./pdgstrs.c.bak:1559:#pragma omp parallel default (shared) 
./pdgstrs.c.bak:1563:#pragma omp master 
./pdgstrs.c.bak:1568:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1576://                                                          printf("iam %d, now receive BCtask %d, RDtask %d\n",iam, recvbuf_BC_fwd[0], recvbuf_RD_fwd[0]);      
./pdgstrs.c.bak:1585:						// #pragma omp taskyield
./pdgstrs.c.bak:1588:#if ( PROFlevel>=1 )		 
./pdgstrs.c.bak:1600:							printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs.c.bak:1622:										krow = PROW( k, grid );
./pdgstrs.c.bak:1655:								// #pragma omp atomic capture
./pdgstrs.c.bak:1667:												#pragma omp simd
./pdgstrs.c.bak:1675:													#pragma omp simd
./pdgstrs.c.bak:1684:											nsupr = lsub[1];
./pdgstrs.c.bak:1686:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1706:													#pragma omp simd
./pdgstrs.c.bak:1715:														lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:1718:														lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
./pdgstrs.c.bak:1721:														lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:1725:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:1732:											printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c.bak:1736:											 * Send Xk to process column Pc[k].
./pdgstrs.c.bak:1750:												krow = PROW( k, grid );
./pdgstrs.c.bak:1768:												#pragma omp simd
./pdgstrs.c.bak:1785://PROFILE_BAROCLINIC_FINISH();
./pdgstrs.c.bak:1786:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:1790:			printf(".. L-solve time\t%8.4f\n", t);
./pdgstrs.c.bak:1798:			printf(".. L-solve time (MAX) \t%8.4f\n", tmax);	
./pdgstrs.c.bak:1809:			printf("(%d) .. After L-solve: y =\n", iam);
./pdgstrs.c.bak:1811:				krow = PROW( k, grid );
./pdgstrs.c.bak:1813:				if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs.c.bak:1818:						printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pdgstrs.c.bak:1859:		 * on the diagonal processes.
./pdgstrs.c.bak:1862:	//PROFILE_BAROTROPIC_INIT();	 
./pdgstrs.c.bak:1878:#pragma omp parallel default(shared) private(ii)
./pdgstrs.c.bak:1885:	#pragma omp simd lastprivate(krow,lk,il)
./pdgstrs.c.bak:1888:	krow = PROW( k, grid );
./pdgstrs.c.bak:1892:	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
./pdgstrs.c.bak:1898:		krow = PROW( k, grid );
./pdgstrs.c.bak:1915:		for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs.c.bak:1917:				printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pdgstrs.c.bak:1919:					printf("(%2d) Local col %2d: # row blocks %2d\n",
./pdgstrs.c.bak:1923:							printf("(%2d) .. row blk %2d:\
./pdgstrs.c.bak:1934:		for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs.c.bak:1936:				printf("\n(%d) bsendx_plist[][]", iam);
./pdgstrs.c.bak:1938:					printf("\n(%d) .. local col %2d: ", iam, lb);
./pdgstrs.c.bak:1939:					for (i = 0; i < Pr; ++i)
./pdgstrs.c.bak:1940:						printf("%4d", bsendx_plist[lb][i]);
./pdgstrs.c.bak:1942:				printf("\n");
./pdgstrs.c.bak:1952:	   Initialize the async Bcast trees on all processes.
./pdgstrs.c.bak:1959:			// printf("UBtree_ptr lk %5d\n",lk); 
./pdgstrs.c.bak:1968:	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pdgstrs.c.bak:1976:			// printf("here lk %5d myid %5d\n",lk,iam);
./pdgstrs.c.bak:1983:			gb = myrow+lk*grid->nprow;  /* not sure */
./pdgstrs.c.bak:1986:				if(mycol==kcol) { /* Diagonal process */
./pdgstrs.c.bak:1997:	#pragma omp simd
./pdgstrs.c.bak:2000:	// for (i = 0; i < nlb; ++i)printf("bmod[i]: %5d\n",bmod[i]);
./pdgstrs.c.bak:2006://PROFILE_BAROTROPIC_FINISH();
./pdgstrs.c.bak:2008:	printf("(%2d) nbrecvx %4d,  nbrecvmod %4d,  nroot %4d\n,  nbtree %4d\n,  nrtree %4d\n",
./pdgstrs.c.bak:2014:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:2016:	if ( !iam) printf(".. Setup U-solve time\t%8.4f\n", t);
./pdgstrs.c.bak:2023:		 * Solve the roots first by all the diagonal processes.
./pdgstrs.c.bak:2026:		printf("(%2d) nroot %4d\n", iam, nroot);
./pdgstrs.c.bak:2031://PROFILE_LND_INIT();
./pdgstrs.c.bak:2033:#pragma omp parallel default (shared) 
./pdgstrs.c.bak:2037:#pragma omp master
./pdgstrs.c.bak:2041:#pragma	omp	taskloop firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,jj,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Uinv,i,lib,rtemp_loc,nroot_send_tmp) nogroup		
./pdgstrs.c.bak:2046:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:2062:			nsupr = lsub[1];
./pdgstrs.c.bak:2082:					#pragma omp simd
./pdgstrs.c.bak:2090:						lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:2093:						lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
./pdgstrs.c.bak:2096:						lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:2100:			// printf("x_u: %f\n",x[ii+i]);
./pdgstrs.c.bak:2105:				// printf("x: %f\n",x[ii+i]);
./pdgstrs.c.bak:2109:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:2116:			printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c.bak:2120:			 * Send Xk to process column Pc[k].
./pdgstrs.c.bak:2125:#pragma omp atomic capture
./pdgstrs.c.bak:2137:#pragma omp parallel default (shared) 
./pdgstrs.c.bak:2141:#pragma omp master
./pdgstrs.c.bak:2145:#pragma	omp	taskloop private (ii,jj,k,lk) nogroup		
./pdgstrs.c.bak:2182:		 * Compute the internal nodes asychronously by all processes.
./pdgstrs.c.bak:2186:#pragma omp parallel default (shared) 
./pdgstrs.c.bak:2190:#pragma omp master 
./pdgstrs.c.bak:2194:			// printf("iam %4d nbrecv %4d nbrecvx %4d nbrecvmod %4d\n", iam, nbrecv, nbrecvxnbrecvmod);
./pdgstrs.c.bak:2200:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:2210:#if ( PROFlevel>=1 )		 
./pdgstrs.c.bak:2220:			printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs.c.bak:2253:						#pragma omp simd
./pdgstrs.c.bak:2260:			// #pragma omp atomic capture
./pdgstrs.c.bak:2271:								#pragma omp simd
./pdgstrs.c.bak:2279:								#pragma omp simd
./pdgstrs.c.bak:2287:						nsupr = lsub[1];
./pdgstrs.c.bak:2308:								#pragma omp simd
./pdgstrs.c.bak:2316:									lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:2319:									lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
./pdgstrs.c.bak:2322:									lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs.c.bak:2326:#if ( PROFlevel>=1 )
./pdgstrs.c.bak:2333:						printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs.c.bak:2337:						 * Send Xk to process column Pc[k].
./pdgstrs.c.bak:2359:								#pragma omp simd
./pdgstrs.c.bak:2371:        //PROFILE_LND_FINISH();
./pdgstrs.c.bak:2372:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:2375:		if ( !iam ) printf(".. U-solve time\t%8.4f\n", t);
./pdgstrs.c.bak:2379:			printf(".. U-solve time (MAX) \t%8.4f\n", tmax);	
./pdgstrs.c.bak:2392:			printf("\n(%d) .. After U-solve: x (ON DIAG PROCS) = \n", iam);
./pdgstrs.c.bak:2396:				krow = PROW( k, grid );
./pdgstrs.c.bak:2399:				if ( iam == diag ) { /* Diagonal process. */
./pdgstrs.c.bak:2405:							printf("\t(%d)\t%4d\t%.10f\n",
./pdgstrs.c.bak:2415://PROFILE_ICE_INIT();
./pdgstrs.c.bak:2418://PROFILE_ICE_FINISH();
./pdgstrs.c.bak:2420:#if ( PRNTlevel>=1 )
./pdgstrs.c.bak:2422:		if ( !iam) printf(".. X to B redistribute time\t%8.4f\n", t);
./pdgstrs.c.bak:2436:#if ( PRNTlevel>=2 )
./pdgstrs.c.bak:2437:			if(iam==0)printf("thread %5d gemm %9.5f\n",i,stat_loc[i]->utime[SOL_GEMM]);
./pdgstrs.c.bak:2486:#if ( PROFlevel>=2 )
./pdgstrs.c.bak:2499:				printf ("\tPDGSTRS comm stat:"
./pdgstrs.c.bak:2502:						msg_cnt_sum / Pr / Pc, msg_cnt_max,
./pdgstrs.c.bak:2503:						msg_vol_sum / Pr / Pc * 1e-6, msg_vol_max * 1e-6);
./pdgstrs1.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrs1.c:16: * <pre>
./pdgstrs1.c:25: * </pre>
./pdgstrs1.c:33: * Function prototypes
./pdgstrs1.c:48: * <pre>
./pdgstrs1.c:61: * distributed in the diagonal processes.
./pdgstrs1.c:75: *        The 2D process mesh.
./pdgstrs1.c:82: *              the diagonal processes.
./pdgstrs1.c:94: * </pre>      
./pdgstrs1.c:116:    int_t  Pc, Pr;
./pdgstrs1.c:117:    int    knsupc, nsupr;
./pdgstrs1.c:132:			     processes in this row. */
./pdgstrs1.c:141:			     processes in this row. */
./pdgstrs1.c:166:    Pr = grid->nprow;
./pdgstrs1.c:173:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrs1.c:202:    /* Compute ilsum[] and ldalsum for process column 0. */
./pdgstrs1.c:223:     * Prepended the block number in the header for lsum[].
./pdgstrs1.c:227:	krow = PROW( k, grid );
./pdgstrs1.c:236:     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
./pdgstrs1.c:244:	    krow = PROW( k, grid );
./pdgstrs1.c:252:	/*PrintInt10("mod_bit", nlb, mod_bit);*/
./pdgstrs1.c:254:	/* Every process receives the count, but it is only useful on the
./pdgstrs1.c:255:	   diagonal processes.  */
./pdgstrs1.c:259:	    krow = PROW( k, grid );
./pdgstrs1.c:263:		if ( mycol == kcol ) { /* diagonal process */
./pdgstrs1.c:273:	    krow = PROW( k, grid );
./pdgstrs1.c:276:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs1.c:278:		    i = 1;  /* Contribution from non-diagonal process. */
./pdgstrs1.c:282:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs1.c:286:		    printf("(%2d) frecv[%4d]  %2d\n", iam, k, frecv[lk]);
./pdgstrs1.c:296:       Solve the leaf nodes first by all the diagonal processes.
./pdgstrs1.c:299:    printf("(%2d) nleaf %4d\n", iam, nleaf);
./pdgstrs1.c:302:	krow = PROW( k, grid );
./pdgstrs1.c:304:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs1.c:313:		nsupr = lsub[1];
./pdgstrs1.c:316:		      lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:319:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs1.c:322:		       lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:327:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs1.c:331:		 * Send Xk to process column Pc[k].
./pdgstrs1.c:333:		for (p = 0; p < Pr; ++p)
./pdgstrs1.c:346:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs1.c:362:	} /* if diagonal process ... */
./pdgstrs1.c:366:     * Compute the internal nodes asynchronously by all processes.
./pdgstrs1.c:369:    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
./pdgstrs1.c:377:	/* -MPI- FATAL: Remote protocol queue full */
./pdgstrs1.c:389:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs1.c:429:		  nsupr = lsub[1];
./pdgstrs1.c:432:			lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:435:			 lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs1.c:438:			 lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:442:		  printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs1.c:446:		   * Send Xk to process column Pc[k].
./pdgstrs1.c:449:		  for (p = 0; p < Pr; ++p)
./pdgstrs1.c:461:			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs1.c:482:	      printf("(%2d) Recv'd wrong message tag %4d\n", iam,  status.MPI_TAG);
./pdgstrs1.c:490:#if ( PRNTlevel>=2 )
./pdgstrs1.c:492:    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
./pdgstrs1.c:497:    if ( !iam ) printf("\n.. After L-solve: y =\n");
./pdgstrs1.c:499:	krow = PROW( k, grid );
./pdgstrs1.c:501:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs1.c:506:		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pdgstrs1.c:526:     * on the diagonal processes.
./pdgstrs1.c:539:     * Compute brecv[] and nbrecvmod counts on the diagonal processes.
./pdgstrs1.c:547:	    krow = PROW( k, grid );
./pdgstrs1.c:550:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs1.c:556:	/* Every process receives the count, but it is only useful on the
./pdgstrs1.c:557:	   diagonal processes.  */
./pdgstrs1.c:561:	    krow = PROW( k, grid );
./pdgstrs1.c:564:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs1.c:565:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs1.c:569:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pdgstrs1.c:579:	    krow = PROW( k, grid );
./pdgstrs1.c:582:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs1.c:584:		    i = 1;  /* Contribution from non-diagonal process. */
./pdgstrs1.c:588:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs1.c:592:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pdgstrs1.c:603:	krow = PROW( k, grid );
./pdgstrs1.c:673:    for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs1.c:675:	    printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pdgstrs1.c:677:		printf("(%2d) Local col %2d: # row blocks %2d\n",
./pdgstrs1.c:681:			printf("(%2d) .. row blk %2d:\
./pdgstrs1.c:692:    for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs1.c:694:	    printf("\n(%d) bsendx_plist[][]", iam);
./pdgstrs1.c:696:		printf("\n(%d) .. local col %2d: ", iam, lb);
./pdgstrs1.c:697:		for (i = 0; i < Pr; ++i)
./pdgstrs1.c:698:		    printf("%4d", bsendx_plist[lb][i]);
./pdgstrs1.c:700:	    printf("\n");
./pdgstrs1.c:707:#if ( PRNTlevel>=2 )
./pdgstrs1.c:709:    if ( !iam) printf(".. Setup U-solve time\t%8.2f\n", t);
./pdgstrs1.c:714:     * Solve the roots first by all the diagonal processes.
./pdgstrs1.c:717:    printf("(%2d) nroot %4d\n", iam, nroot);
./pdgstrs1.c:720:	krow = PROW( k, grid );
./pdgstrs1.c:722:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process. */
./pdgstrs1.c:731:		nsupr = lsub[1];
./pdgstrs1.c:734:		      lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:737:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs1.c:740:		       lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:745:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs1.c:748:		 * Send Xk to process column Pc[k].
./pdgstrs1.c:750:		for (p = 0; p < Pr; ++p)
./pdgstrs1.c:762:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs1.c:775:	} /* if diagonal process ... */
./pdgstrs1.c:780:     * Compute the internal nodes asynchronously by all processes.
./pdgstrs1.c:790:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs1.c:822:		    nsupr = lsub[1];
./pdgstrs1.c:825:			  lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:828:			   lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs1.c:831:			   lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs1.c:835:		    printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs1.c:838:		     * Send Xk to process column Pc[k].
./pdgstrs1.c:841:		    for (p = 0; p < Pr; ++p)
./pdgstrs1.c:853:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs1.c:872:		printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
./pdgstrs1.c:880:#if ( PRNTlevel>=2 )
./pdgstrs1.c:882:    if ( !iam ) printf(".. U-solve time\t%8.2f\n", t);
./pdgstrsL.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrsL.c:15: * lower triangular factor computed previously by PDGSTRF.
./pdgstrsL.c:17: * <pre>
./pdgstrsL.c:21: * </pre>
./pdgstrsL.c:29: * Function prototypes
./pdgstrsL.c:41: * <pre>
./pdgstrsL.c:44: *   Re-distribute B on the diagonal processes of the 2D process mesh.
./pdgstrsL.c:74: *        The solution vector. It is valid only on the diagonal processes.
./pdgstrsL.c:81: *        The 2D process mesh.
./pdgstrsL.c:89: * </pre>
./pdgstrsL.c:107:    int    p, procs;
./pdgstrsL.c:119:    procs = grid->nprow * grid->npcol;
./pdgstrsL.c:123:    SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
./pdgstrsL.c:124:    RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
./pdgstrsL.c:125:    RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
./pdgstrsL.c:126:    sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
./pdgstrsL.c:127:    sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
./pdgstrsL.c:128:    rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
./pdgstrsL.c:129:    rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
./pdgstrsL.c:136:    k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pdgstrsL.c:137:    l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pdgstrsL.c:145:    for (p = 0; p < procs; ++p) {
./pdgstrsL.c:152:        irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
./pdgstrsL.c:154:	p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
./pdgstrsL.c:175:       Copy buffer into X on the diagonal processes.
./pdgstrsL.c:178:    for (p = 0; p < procs; ++p) {
./pdgstrsL.c:181:	    /* Only the diagonal processes do this; the off-diagonal processes
./pdgstrsL.c:188:	    x[l - XK_H] = k;      /* Block number prepended in the header. */
./pdgstrsL.c:208: * <pre>
./pdgstrsL.c:211: *   Re-distribute X on the diagonal processes to B distributed on all
./pdgstrsL.c:212: *   the processes.
./pdgstrsL.c:218: * </pre>
./pdgstrsL.c:235:    int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
./pdgstrsL.c:237:    int  iam, p, q, pkk, procs;
./pdgstrsL.c:238:    int_t  num_diag_procs, *diag_procs;
./pdgstrsL.c:251:    procs = grid->nprow * grid->npcol;
./pdgstrsL.c:254:    SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
./pdgstrsL.c:255:    RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
./pdgstrsL.c:256:    RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
./pdgstrsL.c:257:    sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
./pdgstrsL.c:258:    sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
./pdgstrsL.c:259:    rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
./pdgstrsL.c:260:    rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
./pdgstrsL.c:264:    k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pdgstrsL.c:265:    l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pdgstrsL.c:272:    for (p = 0; p < procs; ++p) {
./pdgstrsL.c:276:    num_diag_procs = SOLVEstruct->num_diag_procs;
./pdgstrsL.c:277:    diag_procs = SOLVEstruct->diag_procs;
./pdgstrsL.c:279:    for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
./pdgstrsL.c:280:	pkk = diag_procs[p];
./pdgstrsL.c:282:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pdgstrsL.c:293:		    q = row_to_proc[ii];
./pdgstrsL.c:339: * <pre>
./pdgstrsL.c:344: * lower triangular factor computed previously by PDGSTRF.
./pdgstrsL.c:347: *     A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgstrsL.c:349: *     A1 * Y = Pc*Pr*B1, where B was overwritten by B1 = diag(R)*B, and
./pdgstrsL.c:350: * the permutation to B1 by Pc*Pr is applied internally in this routine.
./pdgstrsL.c:364: *        A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pdgstrsL.c:367: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrsL.c:368: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrsL.c:369: *        and my process rank. It is an input argument to all the
./pdgstrsL.c:404: * </pre>       
./pdgstrsL.c:431:    int_t  Pc, Pr;
./pdgstrsL.c:432:    int    knsupc, nsupr;
./pdgstrsL.c:445:                             Count the number of local block products to
./pdgstrsL.c:450:                             from processes in this row. 
./pdgstrsL.c:451:                             It is only valid on the diagonal processes. */
./pdgstrsL.c:478:    Pr = grid->nprow;
./pdgstrsL.c:486:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrsL.c:517:    /* Obtain ilsum[] and ldalsum for process column 0. */
./pdgstrsL.c:537:    /* Redistribute B into X on the diagonal processes. */
./pdgstrsL.c:545:	krow = PROW( k, grid );
./pdgstrsL.c:549:	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
./pdgstrsL.c:555:     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
./pdgstrsL.c:562:	    krow = PROW( k, grid );
./pdgstrsL.c:570:	/*PrintInt10("mod_bit", nlb, mod_bit);*/
./pdgstrsL.c:572:#if ( PROFlevel>=2 )
./pdgstrsL.c:575:	/* Every process receives the count, but it is only useful on the
./pdgstrsL.c:576:	   diagonal processes.  */
./pdgstrsL.c:579:#if ( PROFlevel>=2 )
./pdgstrsL.c:584:	    krow = PROW( k, grid );
./pdgstrsL.c:588:		if ( mycol == kcol ) { /* diagonal process */
./pdgstrsL.c:598:       Solve the leaf nodes first by all the diagonal processes.
./pdgstrsL.c:601:    printf("(%2d) nleaf %4d\n", iam, nleaf);
./pdgstrsL.c:604:	krow = PROW( k, grid );
./pdgstrsL.c:606:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrsL.c:615:		nsupr = lsub[1];
./pdgstrsL.c:618:		      lusup, &nsupr, &x[ii], &knsupc);
./pdgstrsL.c:621:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrsL.c:624:		       lusup, &nsupr, &x[ii], &knsupc);
./pdgstrsL.c:629:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrsL.c:633:		 * Send Xk to process column Pc[k].
./pdgstrsL.c:635:		for (p = 0; p < Pr; ++p) {
./pdgstrsL.c:647:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrsL.c:663:	} /* if diagonal process ... */
./pdgstrsL.c:667:       Compute the internal nodes asynchronously by all processes.
./pdgstrsL.c:670:    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
./pdgstrsL.c:678:	/* -MPI- FATAL: Remote protocol queue full */
./pdgstrsL.c:690:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrsL.c:715:	  case LSUM: /* Receiver must be a diagonal process */
./pdgstrsL.c:731:		  nsupr = lsub[1];
./pdgstrsL.c:734:			lusup, &nsupr, &x[ii], &knsupc);
./pdgstrsL.c:737:			 lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrsL.c:740:			 lusup, &nsupr, &x[ii], &knsupc);
./pdgstrsL.c:744:		  printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrsL.c:748:		   * Send Xk to process column Pc[k].
./pdgstrsL.c:751:		  for (p = 0; p < Pr; ++p) {
./pdgstrsL.c:763:			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrsL.c:784:	      printf("(%2d) Recv'd wrong message tag %4d\n", status.MPI_TAG);
./pdgstrsL.c:792:#if ( PRNTlevel>=2 )
./pdgstrsL.c:794:    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
./pdgstrsL.c:800:      printf("(%d) .. After L-solve: y =\n", iam);
./pdgstrsL.c:802:	  krow = PROW( k, grid );
./pdgstrsL.c:804:	  if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrsL.c:809:		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pdgstrsL.c:826:    /* Re-distribute X on the diagonal processes to B distributed on all
./pdgstrsL.c:827:       the processes.   */
./pdgstrs_Bglobal.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrs_Bglobal.c:16: * <pre>
./pdgstrs_Bglobal.c:25: * </pre>
./pdgstrs_Bglobal.c:33: * Function prototypes
./pdgstrs_Bglobal.c:50: * <pre>
./pdgstrs_Bglobal.c:71: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrs_Bglobal.c:72: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrs_Bglobal.c:73: *        and my process rank. It is an input argument to all the
./pdgstrs_Bglobal.c:85: *              processes when calling this routine.
./pdgstrs_Bglobal.c:100: * </pre>    
./pdgstrs_Bglobal.c:125:    int    Pc, Pr, iam;
./pdgstrs_Bglobal.c:126:    int    knsupc, nsupr;
./pdgstrs_Bglobal.c:141:			     processes in this row. */
./pdgstrs_Bglobal.c:150:			     processes in this row. */
./pdgstrs_Bglobal.c:175:    Pr = grid->nprow;
./pdgstrs_Bglobal.c:182:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrs_Bglobal.c:212:    /* Obtain ilsum[] and ldalsum for process column 0. */
./pdgstrs_Bglobal.c:236:     * Copy B into X on the diagonal processes.
./pdgstrs_Bglobal.c:241:	krow = PROW( k, grid );
./pdgstrs_Bglobal.c:245:	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
./pdgstrs_Bglobal.c:247:	    if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal.c:249:		x[jj - XK_H] = k;  /* Block number prepended in the header. */
./pdgstrs_Bglobal.c:259:     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
./pdgstrs_Bglobal.c:267:	    krow = PROW( k, grid );
./pdgstrs_Bglobal.c:276:	/* Every process receives the count, but it is only useful on the
./pdgstrs_Bglobal.c:277:	   diagonal processes.  */
./pdgstrs_Bglobal.c:281:	    krow = PROW( k, grid );
./pdgstrs_Bglobal.c:285:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal.c:295:	    krow = PROW( k, grid );
./pdgstrs_Bglobal.c:298:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs_Bglobal.c:300:		    i = 1;  /* Contribution from non-diagonal process. */
./pdgstrs_Bglobal.c:304:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal.c:308:		    printf("(%2d) frecv[%4d]  %2d\n", iam, k, frecv[lk]);
./pdgstrs_Bglobal.c:318:       Solve the leaf nodes first by all the diagonal processes.
./pdgstrs_Bglobal.c:321:    printf("(%2d) nleaf %4d\n", iam, nleaf);
./pdgstrs_Bglobal.c:324:	krow = PROW( k, grid );
./pdgstrs_Bglobal.c:326:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs_Bglobal.c:335:		nsupr = lsub[1];
./pdgstrs_Bglobal.c:338:		      lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:341:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs_Bglobal.c:344:		       lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:349:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal.c:353:		 * Send Xk to process column Pc[k].
./pdgstrs_Bglobal.c:355:		for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal.c:374:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal.c:390:	} /* if diagonal process ... */
./pdgstrs_Bglobal.c:394:       Compute the internal nodes asynchronously by all processes.
./pdgstrs_Bglobal.c:397:    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
./pdgstrs_Bglobal.c:405:	/* -MPI- FATAL: Remote protocol queue full */
./pdgstrs_Bglobal.c:419:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs_Bglobal.c:444:	  case LSUM: /* Receiver must be a diagonal process */
./pdgstrs_Bglobal.c:459:		  nsupr = lsub[1];
./pdgstrs_Bglobal.c:462:			lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:465:			 lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs_Bglobal.c:468:			 lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:473:		  printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal.c:477:		   * Send Xk to process column Pc[k].
./pdgstrs_Bglobal.c:480:		  for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal.c:497:			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal.c:518:	      printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
./pdgstrs_Bglobal.c:526:#if ( PRNTlevel>=2 )
./pdgstrs_Bglobal.c:528:    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
./pdgstrs_Bglobal.c:533:    printf("\n(%d) .. After L-solve: y =\n", iam);
./pdgstrs_Bglobal.c:535:	krow = PROW( k, grid );
./pdgstrs_Bglobal.c:537:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs_Bglobal.c:542:		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pdgstrs_Bglobal.c:562:     * on the diagonal processes.
./pdgstrs_Bglobal.c:575:     * Compute brecv[] and nbrecvmod counts on the diagonal processes.
./pdgstrs_Bglobal.c:583:	    krow = PROW( k, grid );
./pdgstrs_Bglobal.c:586:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs_Bglobal.c:592:	/* Every process receives the count, but it is only useful on the
./pdgstrs_Bglobal.c:593:	   diagonal processes.  */
./pdgstrs_Bglobal.c:597:	    krow = PROW( k, grid );
./pdgstrs_Bglobal.c:600:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs_Bglobal.c:601:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal.c:605:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pdgstrs_Bglobal.c:615:	    krow = PROW( k, grid );
./pdgstrs_Bglobal.c:618:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs_Bglobal.c:620:		    i = 1;  /* Contribution from non-diagonal process. */
./pdgstrs_Bglobal.c:624:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal.c:628:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pdgstrs_Bglobal.c:639:	krow = PROW( k, grid );
./pdgstrs_Bglobal.c:710:    for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs_Bglobal.c:712:	    printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pdgstrs_Bglobal.c:714:		printf("(%2d) Local col %2d: # row blocks %2d\n",
./pdgstrs_Bglobal.c:718:			printf("(%2d) .. row blk %2d:\
./pdgstrs_Bglobal.c:729:    for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs_Bglobal.c:731:	    printf("\n(%d) bsendx_plist[][]", iam);
./pdgstrs_Bglobal.c:733:		printf("\n(%d) .. local col %2d: ", iam, lb);
./pdgstrs_Bglobal.c:734:		for (i = 0; i < Pr; ++i)
./pdgstrs_Bglobal.c:735:		    printf("%4d", bsendx_plist[lb][i]);
./pdgstrs_Bglobal.c:737:	    printf("\n");
./pdgstrs_Bglobal.c:744:#if ( PRNTlevel>=2 )
./pdgstrs_Bglobal.c:746:    if ( !iam) printf(".. Setup U-solve time\t%8.2f\n", t);
./pdgstrs_Bglobal.c:751:     * Solve the roots first by all the diagonal processes.
./pdgstrs_Bglobal.c:754:    printf("(%2d) nroot %4d\n", iam, nroot);
./pdgstrs_Bglobal.c:757:	krow = PROW( k, grid );
./pdgstrs_Bglobal.c:759:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal.c:768:		nsupr = lsub[1];
./pdgstrs_Bglobal.c:771:		      lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:774:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs_Bglobal.c:777:		       lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:782:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal.c:785:		 * Send Xk to process column Pc[k].
./pdgstrs_Bglobal.c:787:		for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal.c:804:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal.c:817:	} /* if diagonal process ... */
./pdgstrs_Bglobal.c:822:     * Compute the internal nodes asynchronously by all processes.
./pdgstrs_Bglobal.c:833:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs_Bglobal.c:850:	    case LSUM: /* Receiver must be a diagonal process */
./pdgstrs_Bglobal.c:865:		    nsupr = lsub[1];
./pdgstrs_Bglobal.c:868:			  lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:871:			   lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pdgstrs_Bglobal.c:874:			   lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal.c:878:		    printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal.c:881:		     * Send Xk to process column Pc[k].
./pdgstrs_Bglobal.c:884:		    for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal.c:901:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal.c:920:		printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
./pdgstrs_Bglobal.c:928:#if ( PRNTlevel>=2 )
./pdgstrs_Bglobal.c:930:    if ( !iam ) printf(".. U-solve time\t%8.2f\n", t);
./pdgstrs_Bglobal.c:934:    /* Copy the solution X into B (on all processes). */
./pdgstrs_Bglobal.c:936:	int_t num_diag_procs, *diag_procs, *diag_len;
./pdgstrs_Bglobal.c:939:	get_diag_procs(n, Glu_persist, grid, &num_diag_procs,
./pdgstrs_Bglobal.c:940:		       &diag_procs, &diag_len);
./pdgstrs_Bglobal.c:942:	for (j = 1; j < num_diag_procs; ++j) jj = SUPERLU_MAX(jj, diag_len[j]);
./pdgstrs_Bglobal.c:946:			   grid, num_diag_procs, diag_procs, diag_len,
./pdgstrs_Bglobal.c:948:	SUPERLU_FREE(diag_procs);
./pdgstrs_Bglobal.c:986: * Gather the components of x vector on the diagonal processes
./pdgstrs_Bglobal.c:987: * onto all processes, and combine them into the global vector y.
./pdgstrs_Bglobal.c:992:		   gridinfo_t *grid, int_t num_diag_procs,
./pdgstrs_Bglobal.c:993:		   int_t diag_procs[], int_t diag_len[],
./pdgstrs_Bglobal.c:1006:    for (p = 0; p < num_diag_procs; ++p) {
./pdgstrs_Bglobal.c:1007:	pkk = diag_procs[p];
./pdgstrs_Bglobal.c:1011:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pdgstrs_Bglobal.c:1028:	for (k = p; k < nsupers; k += num_diag_procs) {
./pdgstrs_Bglobal_Bsend.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrs_Bglobal_Bsend.c:14: * <pre>
./pdgstrs_Bglobal_Bsend.c:22: * </pre>
./pdgstrs_Bglobal_Bsend.c:31:   Use MPI_Bsend with a large buffer attached in the main program */
./pdgstrs_Bglobal_Bsend.c:35: * Function prototypes
./pdgstrs_Bglobal_Bsend.c:49: * <pre>
./pdgstrs_Bglobal_Bsend.c:70: *        The 2D process mesh. It contains the MPI communicator, the number
./pdgstrs_Bglobal_Bsend.c:71: *        of process rows (NPROW), the number of process columns (NPCOL),
./pdgstrs_Bglobal_Bsend.c:72: *        and my process rank. It is an input argument to all the
./pdgstrs_Bglobal_Bsend.c:84: *              processes when calling this routine.
./pdgstrs_Bglobal_Bsend.c:99: * </pre>      
./pdgstrs_Bglobal_Bsend.c:123:    int_t  Pc, Pr;
./pdgstrs_Bglobal_Bsend.c:124:    int    knsupc, nsupr;
./pdgstrs_Bglobal_Bsend.c:140:			     processes in this row. */
./pdgstrs_Bglobal_Bsend.c:149:			     processes in this row. */
./pdgstrs_Bglobal_Bsend.c:155:    /*-- Function prototypes --*/
./pdgstrs_Bglobal_Bsend.c:177:      printf("Using MPI_Bsend in triangular solve\n");
./pdgstrs_Bglobal_Bsend.c:182:    Pr = grid->nprow;
./pdgstrs_Bglobal_Bsend.c:189:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pdgstrs_Bglobal_Bsend.c:207:    if ( !(send_req = (MPI_Request*) SUPERLU_MALLOC(Pr*sizeof(MPI_Request))) )
./pdgstrs_Bglobal_Bsend.c:209:    for (i = 0; i < Pr; ++i) send_req[i] = MPI_REQUEST_NULL;
./pdgstrs_Bglobal_Bsend.c:219:    /* Obtain ilsum[] and ldalsum for process column 0. */
./pdgstrs_Bglobal_Bsend.c:242:     * Copy B into X on the diagonal processes.
./pdgstrs_Bglobal_Bsend.c:247:	krow = PROW( k, grid );
./pdgstrs_Bglobal_Bsend.c:251:	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
./pdgstrs_Bglobal_Bsend.c:253:	    if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal_Bsend.c:255:		x[jj - XK_H] = k;  /* Block number prepended in the header. */
./pdgstrs_Bglobal_Bsend.c:265:     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
./pdgstrs_Bglobal_Bsend.c:271:	    krow = PROW( k, grid );
./pdgstrs_Bglobal_Bsend.c:274:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs_Bglobal_Bsend.c:276:		    i = 1;  /* Contribution from non-diagonal process. */
./pdgstrs_Bglobal_Bsend.c:280:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal_Bsend.c:284:		    printf("(%2d) frecv[%4d]  %2d\n", iam, k, frecv[lk]);
./pdgstrs_Bglobal_Bsend.c:293:       Solve the leaf nodes first by all the diagonal processes.
./pdgstrs_Bglobal_Bsend.c:296:    printf("(%2d) nleaf %4d\n", iam, nleaf);
./pdgstrs_Bglobal_Bsend.c:299:	krow = PROW( k, grid );
./pdgstrs_Bglobal_Bsend.c:301:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs_Bglobal_Bsend.c:310:		nsupr = lsub[1];
./pdgstrs_Bglobal_Bsend.c:313:		      lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:316:		       lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:321:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal_Bsend.c:325:		 * Send Xk to process column Pc[k].
./pdgstrs_Bglobal_Bsend.c:327:		for (p = 0; p < Pr; ++p)
./pdgstrs_Bglobal_Bsend.c:349:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal_Bsend.c:365:		/* Wait for previous Isends to complete. */
./pdgstrs_Bglobal_Bsend.c:366:		for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal_Bsend.c:373:	} /* if diagonal process ... */
./pdgstrs_Bglobal_Bsend.c:377:       Compute the internal nodes asynchronously by all processes.
./pdgstrs_Bglobal_Bsend.c:380:    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
./pdgstrs_Bglobal_Bsend.c:388:	/* -MPI- FATAL: Remote protocol queue full */
./pdgstrs_Bglobal_Bsend.c:400:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs_Bglobal_Bsend.c:440:		  nsupr = lsub[1];
./pdgstrs_Bglobal_Bsend.c:443:			lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:446:			 lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:450:		  printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal_Bsend.c:454:		   * Send Xk to process column Pc[k].
./pdgstrs_Bglobal_Bsend.c:457:		  for (p = 0; p < Pr; ++p)
./pdgstrs_Bglobal_Bsend.c:480:			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal_Bsend.c:496:		  /* Wait for the previous Isends to complete. */
./pdgstrs_Bglobal_Bsend.c:497:		  for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal_Bsend.c:508:	      printf("(%2d) Recv'd wrong message tag %4d\n", status.MPI_TAG);
./pdgstrs_Bglobal_Bsend.c:516:#if ( PRNTlevel>=2 )
./pdgstrs_Bglobal_Bsend.c:518:    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
./pdgstrs_Bglobal_Bsend.c:522:#if ( PRNTlevel==2 )
./pdgstrs_Bglobal_Bsend.c:523:    if ( !iam ) printf("\n.. After L-solve: y =\n");
./pdgstrs_Bglobal_Bsend.c:525:	krow = PROW( k, grid );
./pdgstrs_Bglobal_Bsend.c:527:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pdgstrs_Bglobal_Bsend.c:532:		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pdgstrs_Bglobal_Bsend.c:549:     * on the diagonal processes.
./pdgstrs_Bglobal_Bsend.c:562:     * Compute brecv[] and nbrecvmod counts on the diagonal processes.
./pdgstrs_Bglobal_Bsend.c:568:	    krow = PROW( k, grid );
./pdgstrs_Bglobal_Bsend.c:571:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pdgstrs_Bglobal_Bsend.c:573:		    i = 1;  /* Contribution from non-diagonal process. */
./pdgstrs_Bglobal_Bsend.c:577:		if ( mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal_Bsend.c:581:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pdgstrs_Bglobal_Bsend.c:591:	krow = PROW( k, grid );
./pdgstrs_Bglobal_Bsend.c:661:    for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs_Bglobal_Bsend.c:663:	    printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pdgstrs_Bglobal_Bsend.c:665:		printf("(%2d) Local col %2d: # row blocks %2d\n",
./pdgstrs_Bglobal_Bsend.c:669:			printf("(%2d) .. row blk %2d:\
./pdgstrs_Bglobal_Bsend.c:680:    for (p = 0; p < Pr*Pc; ++p) {
./pdgstrs_Bglobal_Bsend.c:682:	    printf("\n(%d) bsendx_plist[][]", iam);
./pdgstrs_Bglobal_Bsend.c:684:		printf("\n(%d) .. local col %2d: ", iam, lb);
./pdgstrs_Bglobal_Bsend.c:685:		for (i = 0; i < Pr; ++i)
./pdgstrs_Bglobal_Bsend.c:686:		    printf("%4d", bsendx_plist[lb][i]);
./pdgstrs_Bglobal_Bsend.c:688:	    printf("\n");
./pdgstrs_Bglobal_Bsend.c:695:#if ( PRNTlevel>=3 )
./pdgstrs_Bglobal_Bsend.c:697:    if ( !iam) printf(".. Setup U-solve time\t%8.2f\n", t);
./pdgstrs_Bglobal_Bsend.c:702:     * Solve the roots first by all the diagonal processes.
./pdgstrs_Bglobal_Bsend.c:705:    printf("(%2d) nroot %4d\n", iam, nroot);
./pdgstrs_Bglobal_Bsend.c:708:	krow = PROW( k, grid );
./pdgstrs_Bglobal_Bsend.c:710:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process. */
./pdgstrs_Bglobal_Bsend.c:719:		nsupr = lsub[1];
./pdgstrs_Bglobal_Bsend.c:722:		      lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:725:		       lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:730:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal_Bsend.c:733:		 * Send Xk to process column Pc[k].
./pdgstrs_Bglobal_Bsend.c:735:		for (p = 0; p < Pr; ++p)
./pdgstrs_Bglobal_Bsend.c:757:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal_Bsend.c:770:		/* Wait for the previous Isends to complete. */
./pdgstrs_Bglobal_Bsend.c:771:		for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal_Bsend.c:778:	} /* if diagonal process ... */
./pdgstrs_Bglobal_Bsend.c:783:     * Compute the internal nodes asynchronously by all processes.
./pdgstrs_Bglobal_Bsend.c:794:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pdgstrs_Bglobal_Bsend.c:826:		    nsupr = lsub[1];
./pdgstrs_Bglobal_Bsend.c:829:			  lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:832:			   lusup, &nsupr, &x[ii], &knsupc);
./pdgstrs_Bglobal_Bsend.c:836:		    printf("(%2d) Solve X[%2d]\n", iam, k);
./pdgstrs_Bglobal_Bsend.c:839:		     * Send Xk to process column Pc[k].
./pdgstrs_Bglobal_Bsend.c:842:		    for (p = 0; p < Pr; ++p)
./pdgstrs_Bglobal_Bsend.c:865:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_Bglobal_Bsend.c:879:		    /* Wait for the previous Isends to complete. */
./pdgstrs_Bglobal_Bsend.c:880:		    for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal_Bsend.c:892:		printf("(%2d) Recv'd wrong message tag %4d\n", status.MPI_TAG);
./pdgstrs_Bglobal_Bsend.c:900:#if ( PRNTlevel>=3 )
./pdgstrs_Bglobal_Bsend.c:902:    if ( !iam ) printf(".. U-solve time\t%8.2f\n", t);
./pdgstrs_Bglobal_Bsend.c:906:    /* Copy the solution X into B (on all processes). */
./pdgstrs_Bglobal_Bsend.c:908:	int_t num_diag_procs, *diag_procs, *diag_len;
./pdgstrs_Bglobal_Bsend.c:911:	get_diag_procs(n, Glu_persist, grid, &num_diag_procs,
./pdgstrs_Bglobal_Bsend.c:912:		       &diag_procs, &diag_len);
./pdgstrs_Bglobal_Bsend.c:914:	for (j = 1; j < num_diag_procs; ++j) jj = SUPERLU_MAX(jj, diag_len[j]);
./pdgstrs_Bglobal_Bsend.c:918:			   grid, num_diag_procs, diag_procs, diag_len,
./pdgstrs_Bglobal_Bsend.c:920:	SUPERLU_FREE(diag_procs);
./pdgstrs_Bglobal_Bsend.c:941:    for (p = 0; p < Pr; ++p) {
./pdgstrs_Bglobal_Bsend.c:962: * <pre>
./pdgstrs_Bglobal_Bsend.c:963: * Gather the components of x vector on the diagonal processes
./pdgstrs_Bglobal_Bsend.c:964: * onto all processes, and combine them into the global vector y.
./pdgstrs_Bglobal_Bsend.c:965: * </pre>
./pdgstrs_Bglobal_Bsend.c:970:		   gridinfo_t *grid, int_t num_diag_procs,
./pdgstrs_Bglobal_Bsend.c:971:		   int_t diag_procs[], int_t diag_len[],
./pdgstrs_Bglobal_Bsend.c:984:    for (p = 0; p < num_diag_procs; ++p) {
./pdgstrs_Bglobal_Bsend.c:985:	pkk = diag_procs[p];
./pdgstrs_Bglobal_Bsend.c:989:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pdgstrs_Bglobal_Bsend.c:1006:	for (k = p; k < nsupers; k += num_diag_procs) {
./pdgstrs_lsum.c:4:approvals from U.S. Dept. of Energy) 
./pdgstrs_lsum.c:16: * <pre>
./pdgstrs_lsum.c:24: * </pre>
./pdgstrs_lsum.c:40: * Function prototypes
./pdgstrs_lsum.c:55: * <pre>
./pdgstrs_lsum.c:59: * </pre>
./pdgstrs_lsum.c:85:    int    iam, iknsupc, myrow, nbrow, nsupr, nsupr1, p, pi;
./pdgstrs_lsum.c:94:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:98:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:107:    nsupr = lsub[1];
./pdgstrs_lsum.c:114:	      &alpha, &lusup[luptr], &nsupr, xk,
./pdgstrs_lsum.c:118:	       &alpha, &lusup[luptr], &nsupr, xk,
./pdgstrs_lsum.c:122:	       &alpha, &lusup[luptr], &nsupr, xk,
./pdgstrs_lsum.c:140:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:163:		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pdgstrs_lsum.c:166:	    } else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:176:		    nsupr1 = lsub1[1];
./pdgstrs_lsum.c:177:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:182:			  lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:185:			   lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);
./pdgstrs_lsum.c:188:			   lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:190:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:197:		    printf("(%2d) Solve X[%2d]\n", iam, ik);
./pdgstrs_lsum.c:201:		     * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:203:		    for (p = 0; p < grid->nprow; ++p) {
./pdgstrs_lsum.c:220:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_lsum.c:270:    int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
./pdgstrs_lsum.c:296:	gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pdgstrs_lsum.c:334:		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pdgstrs_lsum.c:337:	    } else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:348:		    nsupr = lsub[1];
./pdgstrs_lsum.c:351:			  lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:354:			   lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);
./pdgstrs_lsum.c:357:			   lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:361:		    printf("(%2d) Solve X[%2d]\n", iam, gik);
./pdgstrs_lsum.c:365:		     * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:367:		    for (p = 0; p < grid->nprow; ++p) {
./pdgstrs_lsum.c:384:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pdgstrs_lsum.c:409: * <pre>
./pdgstrs_lsum.c:413: * </pre>
./pdgstrs_lsum.c:443:	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r,m;
./pdgstrs_lsum.c:483:	// #if ( PROFlevel>=1 )
./pdgstrs_lsum.c:496:		nsupr = lsub[1];
./pdgstrs_lsum.c:498:		// printf("nlb: %5d lk: %5d\n",nlb,lk);
./pdgstrs_lsum.c:501:		krow = PROW( k, grid );
./pdgstrs_lsum.c:507:			m = nsupr-knsupc;
./pdgstrs_lsum.c:513:			m = nsupr;
./pdgstrs_lsum.c:528:#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j,nleaf_send_tmp) untied nogroup	
./pdgstrs_lsum.c:549:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:561:						  &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pdgstrs_lsum.c:565:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pdgstrs_lsum.c:569:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pdgstrs_lsum.c:588:							#pragma omp simd							
./pdgstrs_lsum.c:597:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:605:#pragma omp atomic capture
./pdgstrs_lsum.c:624:									#pragma omp simd							
./pdgstrs_lsum.c:630:#pragma omp atomic capture
./pdgstrs_lsum.c:636:							} else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:638:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:643:									#pragma omp simd							
./pdgstrs_lsum.c:651:									#pragma omp simd							
./pdgstrs_lsum.c:661:								nsupr1 = lsub1[1];
./pdgstrs_lsum.c:681:									#pragma omp simd							
./pdgstrs_lsum.c:690:											lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:693:											lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
./pdgstrs_lsum.c:696:											lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:701:								// printf("x_lsum: %f\n",x[ii+i]);
./pdgstrs_lsum.c:705:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:714:								printf("(%2d) Solve X[%2d]\n", iam, ik);
./pdgstrs_lsum.c:719:								 * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:724:#pragma omp atomic capture
./pdgstrs_lsum.c:735:								// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
./pdgstrs_lsum.c:754:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:760:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pdgstrs_lsum.c:764:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pdgstrs_lsum.c:768:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pdgstrs_lsum.c:792:					#pragma omp simd							
./pdgstrs_lsum.c:804:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:813:#pragma omp atomic capture
./pdgstrs_lsum.c:832:							#pragma omp simd							
./pdgstrs_lsum.c:838:#pragma omp atomic capture
./pdgstrs_lsum.c:843:					} else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:845:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:850:							#pragma omp simd							
./pdgstrs_lsum.c:858:							#pragma omp simd							
./pdgstrs_lsum.c:867:						nsupr1 = lsub1[1];
./pdgstrs_lsum.c:885:							#pragma omp simd							
./pdgstrs_lsum.c:893:									lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:896:									lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
./pdgstrs_lsum.c:899:									lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:904:							// printf("x_lsum: %f\n",x[ii+i]);
./pdgstrs_lsum.c:909:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:917:						printf("(%2d) Solve X[%2d]\n", iam, ik);
./pdgstrs_lsum.c:921:						 * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:927:#pragma omp atomic capture
./pdgstrs_lsum.c:930:							// printf("nleaf_send_tmp %5d lk %5d\n",nleaf_send_tmp);
./pdgstrs_lsum.c:940:						// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1) untied priority(1) 	
./pdgstrs_lsum.c:966: * <pre>
./pdgstrs_lsum.c:970: * </pre>
./pdgstrs_lsum.c:1000:	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r;
./pdgstrs_lsum.c:1031:	// #if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1042:		// printf("ya1 %5d k %5d lk %5d\n",thread_id,k,lk);
./pdgstrs_lsum.c:1047:		// printf("ya2 %5d k %5d lk %5d\n",thread_id,k,lk);
./pdgstrs_lsum.c:1054:		nsupr = lsub[1];
./pdgstrs_lsum.c:1056:		// printf("nlb: %5d lk: %5d\n",nlb,lk);
./pdgstrs_lsum.c:1059:		krow = PROW( k, grid );
./pdgstrs_lsum.c:1065:			m = nsupr-knsupc;
./pdgstrs_lsum.c:1071:			m = nsupr;
./pdgstrs_lsum.c:1084:#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied
./pdgstrs_lsum.c:1105:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1117:						  &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pdgstrs_lsum.c:1121:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pdgstrs_lsum.c:1125:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pdgstrs_lsum.c:1144:								#pragma omp simd lastprivate(irow)
./pdgstrs_lsum.c:1153:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1162:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1168:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pdgstrs_lsum.c:1172:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pdgstrs_lsum.c:1176:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pdgstrs_lsum.c:1200:						#pragma omp simd lastprivate(irow)
./pdgstrs_lsum.c:1209:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1221:			// #pragma omp atomic capture
./pdgstrs_lsum.c:1250:							#pragma omp simd
./pdgstrs_lsum.c:1259:				} else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:1265:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1271:							#pragma omp simd
./pdgstrs_lsum.c:1280:							#pragma omp simd 
./pdgstrs_lsum.c:1289:					nsupr1 = lsub1[1];
./pdgstrs_lsum.c:1307:							#pragma omp simd 
./pdgstrs_lsum.c:1315:								lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:1318:								lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
./pdgstrs_lsum.c:1321:								lusup1, &nsupr1, &x[ii], &iknsupc);
./pdgstrs_lsum.c:1325:					// printf("x_usum: %f\n",x[ii+i]);
./pdgstrs_lsum.c:1329:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1338:					printf("(%2d) Solve X[%2d]\n", iam, ik);
./pdgstrs_lsum.c:1342:					 * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:1353:					// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
./pdgstrs_lsum.c:1409:	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
./pdgstrs_lsum.c:1453:		// printf("Unnz: %5d nub: %5d knsupc: %5d\n",Llu->Unnz[lk],nub,knsupc);
./pdgstrs_lsum.c:1455:#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr) untied nogroup	
./pdgstrs_lsum.c:1480:				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pdgstrs_lsum.c:1485:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1498:							#pragma omp simd							
./pdgstrs_lsum.c:1508:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1515:		#pragma omp atomic capture
./pdgstrs_lsum.c:1526:							#pragma omp simd							
./pdgstrs_lsum.c:1532:#pragma omp atomic capture
./pdgstrs_lsum.c:1539:						printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pdgstrs_lsum.c:1542:					} else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:1544:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1551:							#pragma omp simd							
./pdgstrs_lsum.c:1561:							#pragma omp simd							
./pdgstrs_lsum.c:1571:							nsupr = lsub[1];
./pdgstrs_lsum.c:1589:								#pragma omp simd							
./pdgstrs_lsum.c:1597:										lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:1600:										lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
./pdgstrs_lsum.c:1603:										lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:1607:								// printf("x_usum: %f\n",x[ii+i]);
./pdgstrs_lsum.c:1611:		#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1618:							printf("(%2d) Solve X[%2d]\n", iam, gik);
./pdgstrs_lsum.c:1622:							 * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:1626:								// printf("xre: %f\n",x[ii+i]);
./pdgstrs_lsum.c:1631:#pragma omp atomic capture
./pdgstrs_lsum.c:1643:								// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
./pdgstrs_lsum.c:1668:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pdgstrs_lsum.c:1673:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1685:						#pragma omp simd							
./pdgstrs_lsum.c:1695:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1701:	#pragma omp atomic capture
./pdgstrs_lsum.c:1712:						#pragma omp simd							
./pdgstrs_lsum.c:1717:#pragma omp atomic capture
./pdgstrs_lsum.c:1724:					printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pdgstrs_lsum.c:1727:				} else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:1729:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1736:						#pragma omp simd							
./pdgstrs_lsum.c:1746:						#pragma omp simd							
./pdgstrs_lsum.c:1756:						nsupr = lsub[1];
./pdgstrs_lsum.c:1774:							#pragma omp simd							
./pdgstrs_lsum.c:1782:									lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:1785:									lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
./pdgstrs_lsum.c:1788:									lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:1792:	#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1798:						printf("(%2d) Solve X[%2d]\n", iam, gik);
./pdgstrs_lsum.c:1802:						 * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:1806:							// printf("xre: %f\n",x[ii+i]);
./pdgstrs_lsum.c:1811:#pragma omp atomic capture
./pdgstrs_lsum.c:1825:							// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
./pdgstrs_lsum.c:1879:	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
./pdgstrs_lsum.c:1920:	// printf("Urbs2[lk] %5d lk %5d nub %5d\n",Urbs2[lk],lk,nub);
./pdgstrs_lsum.c:1930:#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,gik,usub,uval,iknsupc,il,i,irow,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz) untied	
./pdgstrs_lsum.c:1941:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1959:				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pdgstrs_lsum.c:1973:							#pragma omp simd							
./pdgstrs_lsum.c:1983:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:1991:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:2001:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pdgstrs_lsum.c:2015:						#pragma omp simd							
./pdgstrs_lsum.c:2025:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:2036:		gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pdgstrs_lsum.c:2040:	// #pragma omp atomic capture
./pdgstrs_lsum.c:2051:					#pragma omp simd							
./pdgstrs_lsum.c:2058:				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pdgstrs_lsum.c:2061:			} else { /* Diagonal process: X[i] += lsum[i]. */
./pdgstrs_lsum.c:2063:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:2069:					#pragma omp simd							
./pdgstrs_lsum.c:2079:					#pragma omp simd							
./pdgstrs_lsum.c:2089:					nsupr = lsub[1];
./pdgstrs_lsum.c:2107:						#pragma omp simd							
./pdgstrs_lsum.c:2115:								lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:2118:								lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
./pdgstrs_lsum.c:2121:								lusup, &nsupr, &x[ii], &iknsupc);
./pdgstrs_lsum.c:2125:#if ( PROFlevel>=1 )
./pdgstrs_lsum.c:2131:					printf("(%2d) Solve X[%2d]\n", iam, gik);
./pdgstrs_lsum.c:2135:					 * Send Xk to process column Pc[k].
./pdgstrs_lsum.c:2139:						// printf("xre: %f\n",x[ii+i]);
./pdgstrs_lsum.c:2151:						// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
./pdlangs.c:4:approvals from U.S. Dept. of Energy) 
./pdlangs.c:16: * <pre>
./pdlangs.c:19: * </pre>
./pdlangs.c:26:<pre> 
./pdlangs.c:60:            The 2D process mesh.
./pdlangs.c:62:</pre>
./pdlangs.c:75:    double   *temprwork;
./pdlangs.c:114:	if ( !(temprwork = (double *) doubleCalloc_dist(A->ncol)) )
./pdlangs.c:115:	    ABORT("doubleCalloc_dist fails for temprwork.");
./pdlangs.c:116:	MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);
./pdlangs.c:119:	    value = SUPERLU_MAX(value, temprwork[j]);
./pdlangs.c:121:	SUPERLU_FREE (temprwork);
./pdlaqgs.c:4:approvals from U.S. Dept. of Energy) 
./pdlaqgs.c:16: * <pre>
./pdlaqgs.c:19: * </pre>
./pdlaqgs.c:26:<pre>
./pdlaqgs.c:61:            = 'R':  Row equilibration, i.e., A has been premultiplied by  
./pdlaqgs.c:81:</pre>
./pdlaqgs.c:108:    small = dmach_dist("Safe minimum") / dmach_dist("Precision");
./pdsymbfact_distdata.c:4:approvals from U.S. Dept. of Energy) 
./pdsymbfact_distdata.c:16: * <pre>
./pdsymbfact_distdata.c:26: * </pre>
./pdsymbfact_distdata.c:41: * <pre>
./pdsymbfact_distdata.c:54: * and the supernodes information.  This represents the arrays:
./pdsymbfact_distdata.c:79: *         of processors, stored by columns.
./pdsymbfact_distdata.c:82: *         Structure of L distributed on a 2D grid of processors, 
./pdsymbfact_distdata.c:87: *         of processors, stored by rows.
./pdsymbfact_distdata.c:90: *         Structure of U distributed on a 2D grid of processors, 
./pdsymbfact_distdata.c:94: *        The 2D process mesh.
./pdsymbfact_distdata.c:100: *        (an approximation).
./pdsymbfact_distdata.c:101: * </pre>
./pdsymbfact_distdata.c:111:  int   iam, nprocs, pc, pr, p, np, p_diag;
./pdsymbfact_distdata.c:119:  int_t maxszsn, maxNvtcsPProc;
./pdsymbfact_distdata.c:140:  nprocs = (int) grid->nprow * grid->npcol;
./pdsymbfact_distdata.c:143:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./pdsymbfact_distdata.c:154:  mem           = intCalloc_dist(12 * nprocs);
./pdsymbfact_distdata.c:157:  memAux     = (float) (12 * nprocs * sizeof(int_t));
./pdsymbfact_distdata.c:159:  nnzToSend     = nnzToRecv + 2*nprocs;
./pdsymbfact_distdata.c:160:  nnzToSend_l   = nnzToSend + 2 * nprocs;
./pdsymbfact_distdata.c:161:  nnzToSend_u   = nnzToSend_l + nprocs;
./pdsymbfact_distdata.c:162:  send_1        = nnzToSend_u + nprocs;
./pdsymbfact_distdata.c:163:  send_2        = send_1 + nprocs;
./pdsymbfact_distdata.c:164:  tmp_ptrToSend = send_2 + nprocs;
./pdsymbfact_distdata.c:165:  nnzToRecv_l   = tmp_ptrToSend + nprocs;
./pdsymbfact_distdata.c:166:  nnzToRecv_u   = nnzToRecv_l + nprocs;
./pdsymbfact_distdata.c:169:  ptrToRecv = nnzToSend + nprocs;
./pdsymbfact_distdata.c:171:  nvtcs = (int *) SUPERLU_MALLOC(5 * nprocs * sizeof(int));
./pdsymbfact_distdata.c:172:  intBuf1 = nvtcs + nprocs;
./pdsymbfact_distdata.c:173:  intBuf2 = nvtcs + 2 * nprocs;
./pdsymbfact_distdata.c:174:  intBuf3 = nvtcs + 3 * nprocs;
./pdsymbfact_distdata.c:175:  intBuf4 = nvtcs + 4 * nprocs;
./pdsymbfact_distdata.c:176:  memAux += 5 * nprocs * sizeof(int);
./pdsymbfact_distdata.c:182:    fprintf (stderr, "Malloc fails for supno_n[].");
./pdsymbfact_distdata.c:199:       each processor */
./pdsymbfact_distdata.c:200:    for (k = 0, p = 0; p < nprocs; p++) {
./pdsymbfact_distdata.c:206:  if (nprocs > 1) {
./pdsymbfact_distdata.c:210:	fprintf (stderr, "Malloc fails for temp[].");
./pdsymbfact_distdata.c:216:    for (p=0; p<nprocs; p++) {
./pdsymbfact_distdata.c:251:    if (nprocs > 1) {
./pdsymbfact_distdata.c:259:  for (p = 0; p < 2 *nprocs; p++)
./pdsymbfact_distdata.c:266:    fprintf (stderr, "Malloc fails for xsup_n[].");
./pdsymbfact_distdata.c:272:     COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
./pdsymbfact_distdata.c:286:  for (p = 0; p < nprocs; p++) {
./pdsymbfact_distdata.c:294:      pr = PROW( gb_n, grid );
./pdsymbfact_distdata.c:295:      p_diag = PNUM( pr, pc, grid);
./pdsymbfact_distdata.c:306:	  p = (int) PNUM( PROW(gb, grid), pc, grid );
./pdsymbfact_distdata.c:315:	  p = PNUM( pr, PCOL(gb, grid), grid);
./pdsymbfact_distdata.c:322:      for (p = pr * grid->npcol; p < (pr + 1) * grid->npcol; p++) {
./pdsymbfact_distdata.c:326:      for (p = pr * grid->npcol; p < (pr + 1) * grid->npcol; p++) 
./pdsymbfact_distdata.c:335:      for (p = pc; p < nprocs; p += grid->npcol) {
./pdsymbfact_distdata.c:339:      for (p = pc; p < nprocs; p += grid->npcol) 
./pdsymbfact_distdata.c:355:  for (p = 0; p < nprocs; p++) {
./pdsymbfact_distdata.c:371:  nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pdsymbfact_distdata.c:374:    fprintf (stderr, "Malloc fails for xlsub_n[].");
./pdsymbfact_distdata.c:380:    fprintf (stderr, "Malloc fails for xusub_n[].");
./pdsymbfact_distdata.c:389:      fprintf (stderr, "Malloc fails for rcv_luind[].");
./pdsymbfact_distdata.c:395:  if ( nprocs > 1 && (SendCnt_l || SendCnt_u) ) {
./pdsymbfact_distdata.c:397:      fprintf (stderr, "Malloc fails for index[].");
./pdsymbfact_distdata.c:418:    for (i = 0, j = 0, p = 0; p < nprocs; p++) {
./pdsymbfact_distdata.c:432:	pr = PROW( gb_n, grid );
./pdsymbfact_distdata.c:433:	p_diag = PNUM( pr, pc, grid );
./pdsymbfact_distdata.c:442:	  p = pc;                np = grid->nprow;	  
./pdsymbfact_distdata.c:444:	  p = pr * grid->npcol;  np = grid->npcol;
./pdsymbfact_distdata.c:467:	      p = PNUM( PROW(gb, grid), pc, grid );
./pdsymbfact_distdata.c:469:	      p = PNUM( pr, PCOL(gb, grid), grid);
./pdsymbfact_distdata.c:488:	  for (p = pc; p < nprocs; p += grid->npcol) {
./pdsymbfact_distdata.c:491:		if (PNUM(PROW(gb, grid), pc, grid) != p) {
./pdsymbfact_distdata.c:505:	  for (p = pr * grid->npcol; p < (pr + 1) * grid->npcol; p++) {
./pdsymbfact_distdata.c:509:		if(PNUM( pr, PCOL(gb, grid), grid) != p) {
./pdsymbfact_distdata.c:527:       each processor (structure needed in MPI_Alltoallv) */
./pdsymbfact_distdata.c:528:    for (i = 0, p = 0; p < nprocs; p++) {
./pdsymbfact_distdata.c:536:    if (nprocs > 1) {
./pdsymbfact_distdata.c:539:      for (p=0; p<nprocs; p++) {
./pdsymbfact_distdata.c:566:      if ( nprocs > 1 && (SendCnt_l || SendCnt_u) ) {
./pdsymbfact_distdata.c:576:    for (p = 0; p < nprocs; p ++) {
./pdsymbfact_distdata.c:608:	  fprintf (stderr, "Malloc fails for lsub_n[].");
./pdsymbfact_distdata.c:619:	  fprintf (stderr, "Malloc fails for usub_n[].");
./pdsymbfact_distdata.c:629:    for (p = 0; p < nprocs; p++) {
./pdsymbfact_distdata.c:634:	  printf ("Pe[%d] p %d gb " IFMT " nsupers " IFMT " i " IFMT " i-k " IFMT "\n",
./pdsymbfact_distdata.c:658:  memAux -= (float) (12 * nprocs * iword);
./pdsymbfact_distdata.c:660:  memAux -= (float) (5 * nprocs * sizeof(int));
./pdsymbfact_distdata.c:687: * <pre>
./pdsymbfact_distdata.c:690: *   Re-distribute A on the 2D process mesh.  The lower part is
./pdsymbfact_distdata.c:709: *        The 2D process mesh.
./pdsymbfact_distdata.c:713: *         of processors, stored by columns.
./pdsymbfact_distdata.c:717: *         2D grid of processors, stored by columns.
./pdsymbfact_distdata.c:721: *         2D grid of processors, stored by columns.
./pdsymbfact_distdata.c:725: *         of processors, stored by rows.
./pdsymbfact_distdata.c:729: *         2D grid of processors, stored by rows.
./pdsymbfact_distdata.c:733: *         2D grid of processors, stored by rows.
./pdsymbfact_distdata.c:747: *        (an approximation).
./pdsymbfact_distdata.c:748: * </pre>
./pdsymbfact_distdata.c:759:  int    iam, p, procs;
./pdsymbfact_distdata.c:795:  procs = grid->nprow * grid->npcol;
./pdsymbfact_distdata.c:800:  if (!(nnzToRecv = intCalloc_dist(2*procs))) {
./pdsymbfact_distdata.c:801:    fprintf (stderr, "Malloc fails for nnzToRecv[].");
./pdsymbfact_distdata.c:804:  memAux = (float) (2 * procs * iword);
./pdsymbfact_distdata.c:806:  nnzToSend = nnzToRecv + procs;
./pdsymbfact_distdata.c:810:     COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
./pdsymbfact_distdata.c:816:      irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pdsymbfact_distdata.c:820:      p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pdsymbfact_distdata.c:832:  for (p = 0; p < procs; ++p) {
./pdsymbfact_distdata.c:842:  k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
./pdsymbfact_distdata.c:847:    fprintf (stderr, "Malloc fails for ia[].");
./pdsymbfact_distdata.c:853:    fprintf (stderr, "Malloc fails for aij[].");
./pdsymbfact_distdata.c:859:  if ( procs > 1 ) {
./pdsymbfact_distdata.c:861:	   SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) ) {
./pdsymbfact_distdata.c:862:      fprintf (stderr, "Malloc fails for send_req[].");
./pdsymbfact_distdata.c:865:    memAux += (float) (2*procs *sizeof(MPI_Request));
./pdsymbfact_distdata.c:866:    if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) ) {
./pdsymbfact_distdata.c:867:      fprintf(stderr, "Malloc fails for ia_send[].");
./pdsymbfact_distdata.c:870:    memAux += (float) (procs*sizeof(int_t*));
./pdsymbfact_distdata.c:871:    if ( !(aij_send = (double **)SUPERLU_MALLOC(procs*sizeof(double*))) ) {
./pdsymbfact_distdata.c:872:      fprintf(stderr, "Malloc fails for aij_send[].");
./pdsymbfact_distdata.c:875:    memAux += (float) (procs*sizeof(double*));    
./pdsymbfact_distdata.c:877:      fprintf(stderr, "Malloc fails for index[].");
./pdsymbfact_distdata.c:882:      fprintf(stderr, "Malloc fails for nzval[].");
./pdsymbfact_distdata.c:886:    if ( !(ptr_to_send = intCalloc_dist(procs)) ) {
./pdsymbfact_distdata.c:887:      fprintf(stderr, "Malloc fails for ptr_to_send[].");
./pdsymbfact_distdata.c:890:    memAux += (float) (procs * iword);
./pdsymbfact_distdata.c:892:      fprintf(stderr, "Malloc fails for itemp[].");
./pdsymbfact_distdata.c:897:      fprintf(stderr, "Malloc fails for dtemp[].");
./pdsymbfact_distdata.c:902:    for (i = 0, j = 0, p = 0; p < procs; ++p) {
./pdsymbfact_distdata.c:910:  } /* if procs > 1 */
./pdsymbfact_distdata.c:912:  nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pdsymbfact_distdata.c:915:    fprintf (stderr, "Malloc fails for *ainf_colptr[].");
./pdsymbfact_distdata.c:920:    fprintf (stderr, "Malloc fails for *asup_rowptr[].");
./pdsymbfact_distdata.c:934:      irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pdsymbfact_distdata.c:938:      p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pdsymbfact_distdata.c:968:  for (p = 0; p < procs; ++p) {
./pdsymbfact_distdata.c:975:		 p, iam+procs, grid->comm, &send_req[procs+p] ); 
./pdsymbfact_distdata.c:979:  for (p = 0; p < procs; ++p) {
./pdsymbfact_distdata.c:984:      MPI_Recv( dtemp, it, MPI_DOUBLE, p, p+procs,
./pdsymbfact_distdata.c:1010:  for (p = 0; p < procs; ++p) {
./pdsymbfact_distdata.c:1013:      MPI_Wait( &send_req[procs+p], &status);
./pdsymbfact_distdata.c:1022:  memAux -= 2 * procs * iword;
./pdsymbfact_distdata.c:1023:  if ( procs > 1 ) {
./pdsymbfact_distdata.c:1032:    memAux -= 2*procs *sizeof(MPI_Request) + procs*sizeof(int_t*) +
./pdsymbfact_distdata.c:1033:      procs*sizeof(double*) + 2*SendCnt * iword +
./pdsymbfact_distdata.c:1034:      SendCnt* dword + procs*iword +
./pdsymbfact_distdata.c:1043:      fprintf (stderr, "Malloc fails for *ainf_rowind[].");
./pdsymbfact_distdata.c:1048:      fprintf (stderr, "Malloc fails for *ainf_val[].");
./pdsymbfact_distdata.c:1059:      fprintf (stderr, "Malloc fails for *asup_colind[].");
./pdsymbfact_distdata.c:1064:      fprintf (stderr, "Malloc fails for *asup_val[].");
./pdsymbfact_distdata.c:1136:  fprintf (stdout, "Size of allocated memory (MB) %.3f\n", memRet*1e-6);
./pdsymbfact_distdata.c:1144: * <pre>
./pdsymbfact_distdata.c:1147: *   Distribute the input matrix onto the 2D process mesh.
./pdsymbfact_distdata.c:1179: *        The 2D process mesh.
./pdsymbfact_distdata.c:1186: *        (an approximation).
./pdsymbfact_distdata.c:1187: * </pre>
./pdsymbfact_distdata.c:1200:    len, len1, nsupc, nsupc_gb, ii, nprocs;
./pdsymbfact_distdata.c:1207:  int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
./pdsymbfact_distdata.c:1209:  int iam, jbrow, jbcol, jcol, kcol, krow, mycol, myrow, pc, pr, ljb_i, ljb_j, p;
./pdsymbfact_distdata.c:1233:  double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pdsymbfact_distdata.c:1234:  int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pdsymbfact_distdata.c:1238:  RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
./pdsymbfact_distdata.c:1240:  RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
./pdsymbfact_distdata.c:1253:  int_t  **fsendx_plist; /* Column process list to send down Xk.   */
./pdsymbfact_distdata.c:1260:  int_t  **bsendx_plist; /* Column process list to send down Xk.   */
./pdsymbfact_distdata.c:1268:  int_t *Urb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./pdsymbfact_distdata.c:1273:  int_t *Lrb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./pdsymbfact_distdata.c:1304:#if ( PRNTlevel>=1 )
./pdsymbfact_distdata.c:1307:#if ( PROFlevel>=1 ) 
./pdsymbfact_distdata.c:1319:  nprocs = grid->npcol * grid->nprow;
./pdsymbfact_distdata.c:1340:  nsupers_i = CEILING( nsupers, grid->nprow );/* No of local row blocks */
./pdsymbfact_distdata.c:1344:    fprintf (stderr, "Malloc fails for ilsum[].");  
./pdsymbfact_distdata.c:1349:    fprintf (stderr, "Malloc fails for ilsum_j[].");
./pdsymbfact_distdata.c:1358:    if ( myrow == PROW( gb, grid ) ) {
./pdsymbfact_distdata.c:1388:   * propagate the values of A into them.
./pdsymbfact_distdata.c:1391:    fprintf(stderr, "Calloc fails for ToRecv[].");
./pdsymbfact_distdata.c:1399:    fprintf(stderr, "Malloc fails for ToSendR[].");
./pdsymbfact_distdata.c:1405:    fprintf(stderr, "Malloc fails for index[].");
./pdsymbfact_distdata.c:1416:    fprintf(stderr, "Calloc fails for LUb_length[].");
./pdsymbfact_distdata.c:1420:    fprintf(stderr, "Malloc fails for LUb_indptr[].");
./pdsymbfact_distdata.c:1424:    fprintf(stderr, "Calloc fails for LUb_number[].");
./pdsymbfact_distdata.c:1428:    fprintf(stderr, "Calloc fails for LUb_valptr[].");
./pdsymbfact_distdata.c:1433:  k = CEILING( nsupers, grid->nprow ); 
./pdsymbfact_distdata.c:1437:    fprintf(stderr, "Malloc fails for Unzval_br_ptr[].");
./pdsymbfact_distdata.c:1441:    fprintf(stderr, "Malloc fails for Ufstnz_br_ptr[].");
./pdsymbfact_distdata.c:1449:    fprintf(stderr, "Malloc fails for ToSendD[].");
./pdsymbfact_distdata.c:1456:    fprintf(stderr, "Calloc fails for rb_marker[].");
./pdsymbfact_distdata.c:1460:    fprintf(stderr, "Calloc fails for rb_marker[].");
./pdsymbfact_distdata.c:1470:    fprintf(stderr, "Calloc fails for SPA dense[].");
./pdsymbfact_distdata.c:1475:    fprintf(stderr, "Calloc fails for fmod[].");
./pdsymbfact_distdata.c:1479:    fprintf(stderr, "Calloc fails for bmod[].");
./pdsymbfact_distdata.c:1489:    fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[].");
./pdsymbfact_distdata.c:1493:    fprintf(stderr, "Malloc fails for Lrowind_bc_ptr[].");
./pdsymbfact_distdata.c:1499:	fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
./pdsymbfact_distdata.c:1504:	fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
./pdsymbfact_distdata.c:1508:    fprintf(stderr, "Malloc fails for Lindval_loc_bc_ptr[].");
./pdsymbfact_distdata.c:1513:    fprintf(stderr, "Malloc fails for Unnz[].");
./pdsymbfact_distdata.c:1525:  /* These lists of processes will be used for triangular solves. */
./pdsymbfact_distdata.c:1527:    fprintf(stderr, "Malloc fails for fsendx_plist[].");
./pdsymbfact_distdata.c:1530:  len = nsupers_j * grid->nprow;
./pdsymbfact_distdata.c:1532:    fprintf(stderr, "Malloc fails for fsendx_plist[0]");
./pdsymbfact_distdata.c:1536:  for (i = 0, j = 0; i < nsupers_j; ++i, j += grid->nprow)
./pdsymbfact_distdata.c:1539:    fprintf(stderr, "Malloc fails for bsendx_plist[].");
./pdsymbfact_distdata.c:1543:    fprintf(stderr, "Malloc fails for bsendx_plist[0]");
./pdsymbfact_distdata.c:1547:  for (i = 0, j = 0; i < nsupers_j; ++i, j += grid->nprow)
./pdsymbfact_distdata.c:1553:    PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
./pdsymbfact_distdata.c:1554:    THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
./pdsymbfact_distdata.c:1558:    jbrow = PROW( jb, grid );
./pdsymbfact_distdata.c:1564:    if ( myrow == jbrow ) { /* Block row jb in my process row */
./pdsymbfact_distdata.c:1569:	    printf ("ERR7\n");
./pdsymbfact_distdata.c:1572:	    printf ("Pe[%d] ERR distsn jb " IFMT " gb " IFMT " j " IFMT " jcol %d\n",
./pdsymbfact_distdata.c:1576:	  if (gb >= nsupers || lb >= nsupers_j) printf ("ERR8\n");
./pdsymbfact_distdata.c:1579:	    printf ("Pe[%d] ERR1 jb " IFMT " gb " IFMT " j " IFMT " jcol %d\n",
./pdsymbfact_distdata.c:1594:	if (i >= xusub[nsupers_i]) printf ("ERR10\n");
./pdsymbfact_distdata.c:1599:	  printf ("Pe[%d] [%d %d] elt [%d] jbcol %d pc %d\n",
./pdsymbfact_distdata.c:1603:	pc = PCOL( gb, grid ); /* Process col owning this block */
./pdsymbfact_distdata.c:1606:	pr = PROW( gb, grid );
./pdsymbfact_distdata.c:1607:	if ( pr != jbrow  && mycol == pc)
./pdsymbfact_distdata.c:1614:	    if (Urb_marker[lb] == FALSE && gb != jb && myrow != pr) nbrecvx ++;
./pdsymbfact_distdata.c:1618:	       printf ("Pe[%d] T1 [%d %d] nrbu %d \n",
./pdsymbfact_distdata.c:1624:#if ( PRNTlevel>=1 )
./pdsymbfact_distdata.c:1649:	  fprintf (stderr, "Malloc fails for Uindex[]");
./pdsymbfact_distdata.c:1655:	  fprintf (stderr, "Malloc fails for Unzval_br_ptr[*][]");
./pdsymbfact_distdata.c:1681:	/* Propagate the fstnz subscripts to Ufstnz_br_ptr[],
./pdsymbfact_distdata.c:1719:    if (mycol == jbcol) {  /* Block column jb in my process column */
./pdsymbfact_distdata.c:1724:	  if (irow >= n) printf ("Pe[%d] ERR1\n", iam);
./pdsymbfact_distdata.c:1726:	  if (gb >= nsupers) printf ("Pe[%d] ERR5\n", iam);
./pdsymbfact_distdata.c:1727:	  if ( myrow == PROW( gb, grid ) ) {
./pdsymbfact_distdata.c:1730:	    if (irow >= ldaspa) printf ("Pe[%d] ERR0\n", iam);
./pdsymbfact_distdata.c:1757:	pr = PROW( gb, grid ); /* Process row owning this block */
./pdsymbfact_distdata.c:1758:	if ( pr != jbrow && fsendx_plist[ljb_j][pr] == EMPTY &&
./pdsymbfact_distdata.c:1760:	  fsendx_plist[ljb_j][pr] = YES;
./pdsymbfact_distdata.c:1763:	if ( myrow == pr ) {
./pdsymbfact_distdata.c:1775:#if ( PRNTlevel>=1 )
./pdsymbfact_distdata.c:1802:	  fprintf (stderr, "Malloc fails for index[]");
./pdsymbfact_distdata.c:1808:	  fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[*][] col block " IFMT, jb);
./pdsymbfact_distdata.c:1848:	  /* Propagate the compressed row subscripts to Lindex[],
./pdsymbfact_distdata.c:1854:	    if ( myrow == PROW( gb, grid ) ) {
./pdsymbfact_distdata.c:1873:			krow = PROW( jb, grid );
./pdsymbfact_distdata.c:1952:  /* exchange information about bsendx_plist in between column of processors */
./pdsymbfact_distdata.c:1953:  k = SUPERLU_MAX( grid->nprow, grid->npcol);
./pdsymbfact_distdata.c:1955:    fprintf (stderr, "Malloc fails for recvBuf[].");
./pdsymbfact_distdata.c:1958:  if ( !(nnzToRecv = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pdsymbfact_distdata.c:1959:    fprintf (stderr, "Malloc fails for nnzToRecv[].");
./pdsymbfact_distdata.c:1962:  if ( !(ptrToRecv = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pdsymbfact_distdata.c:1963:    fprintf (stderr, "Malloc fails for ptrToRecv[].");
./pdsymbfact_distdata.c:1966:  if ( !(nnzToSend = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pdsymbfact_distdata.c:1967:    fprintf (stderr, "Malloc fails for nnzToRecv[].");
./pdsymbfact_distdata.c:1970:  if ( !(ptrToSend = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pdsymbfact_distdata.c:1971:    fprintf (stderr, "Malloc fails for ptrToRecv[].");
./pdsymbfact_distdata.c:1975:  if (memDist < (nsupers*k*iword +4*nprocs * sizeof(int)))
./pdsymbfact_distdata.c:1976:    memDist = nsupers*k*iword +4*nprocs * sizeof(int);
./pdsymbfact_distdata.c:1978:  for (p = 0; p < nprocs; p++)
./pdsymbfact_distdata.c:1983:    jbrow = PROW( jb, grid );
./pdsymbfact_distdata.c:1988:  for (p = 0; p < nprocs; p++) {
./pdsymbfact_distdata.c:2001:    jbrow = PROW( jb, grid );
./pdsymbfact_distdata.c:2015:    jbrow = PROW( jb, grid );
./pdsymbfact_distdata.c:2041:  /* exchange information about bsendx_plist in between column of processors */
./pdsymbfact_distdata.c:2042:  MPI_Allreduce ((*bsendx_plist), recvBuf, nsupers_j * grid->nprow, mpi_int_t,
./pdsymbfact_distdata.c:2047:    jbrow = PROW( jb, grid);
./pdsymbfact_distdata.c:2051:	for (k = ljb_j * grid->nprow; k < (ljb_j+1) * grid->nprow; k++) {
./pdsymbfact_distdata.c:2058:	for (k = ljb_j * grid->nprow; k < (ljb_j+1) * grid->nprow; k++) 
./pdsymbfact_distdata.c:2077:		nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
./pdsymbfact_distdata.c:2137:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pdsymbfact_distdata.c:2152:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2160:		if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pdsymbfact_distdata.c:2162:		if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pdsymbfact_distdata.c:2178:		if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pdsymbfact_distdata.c:2180:		for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
./pdsymbfact_distdata.c:2190:				pr = PROW( gb, grid );
./pdsymbfact_distdata.c:2191:				ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
./pdsymbfact_distdata.c:2197:		MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MIN,grid->cscp.comm);					  
./pdsymbfact_distdata.c:2207:			for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pdsymbfact_distdata.c:2208:			for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pdsymbfact_distdata.c:2209:			for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pdsymbfact_distdata.c:2213:			for (j=0;j<grid->nprow;++j){
./pdsymbfact_distdata.c:2216:				pr = PROW( gb, grid );
./pdsymbfact_distdata.c:2217:				if(gb==jb)Root=pr;
./pdsymbfact_distdata.c:2218:				if(myrow==pr)Iactive=1;		
./pdsymbfact_distdata.c:2223:			quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
./pdsymbfact_distdata.c:2226:				// printf("jb %5d damn\n",jb);
./pdsymbfact_distdata.c:2231:				for (j = 0; j < grid->nprow; ++j){
./pdsymbfact_distdata.c:2232:					if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pdsymbfact_distdata.c:2233:						ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pdsymbfact_distdata.c:2249:					// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pdsymbfact_distdata.c:2253:					// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb],'d'));
./pdsymbfact_distdata.c:2257:					// #if ( PRNTlevel>=1 )		
./pdsymbfact_distdata.c:2260:						for (j = 0; j < grid->nprow; ++j) {
./pdsymbfact_distdata.c:2267:						// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
./pdsymbfact_distdata.c:2269:						// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
./pdsymbfact_distdata.c:2270:						// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pdsymbfact_distdata.c:2271:						// // printf("\n");
./pdsymbfact_distdata.c:2286:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2288:	if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
./pdsymbfact_distdata.c:2292:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2297:		nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pdsymbfact_distdata.c:2305:			pr = PROW( k, grid );
./pdsymbfact_distdata.c:2306:			if ( myrow == pr ) {
./pdsymbfact_distdata.c:2313:		/* Every process receives the count, but it is only useful on the
./pdsymbfact_distdata.c:2314:		   diagonal processes.  */
./pdsymbfact_distdata.c:2319:		k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pdsymbfact_distdata.c:2361:					pr = PROW( ib, grid );
./pdsymbfact_distdata.c:2362:					if ( myrow == pr ) { /* Block row ib in my process row */
./pdsymbfact_distdata.c:2373:			ib = myrow+lib*grid->nprow;  /* not sure */
./pdsymbfact_distdata.c:2375:				pr = PROW( ib, grid );
./pdsymbfact_distdata.c:2407:							ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pdsymbfact_distdata.c:2419:						// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
./pdsymbfact_distdata.c:2423:						#if ( PRNTlevel>=1 )
./pdsymbfact_distdata.c:2426:						// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pdsymbfact_distdata.c:2427:						// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pdsymbfact_distdata.c:2428:						// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pdsymbfact_distdata.c:2429:						// printf("\n");
./pdsymbfact_distdata.c:2453:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2455:	if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
./pdsymbfact_distdata.c:2458:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2467:		if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pdsymbfact_distdata.c:2469:		if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pdsymbfact_distdata.c:2485:		if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pdsymbfact_distdata.c:2487:		for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
./pdsymbfact_distdata.c:2491:		for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
./pdsymbfact_distdata.c:2492:			ib = myrow+lib*grid->nprow;  /* not sure */
./pdsymbfact_distdata.c:2494:		// if(ib==0)printf("iam %5d ib %5d\n",iam,ib);
./pdsymbfact_distdata.c:2503:				  pr = PROW( ib, grid );
./pdsymbfact_distdata.c:2504:				  if ( mycol == pc ) { /* Block column ib in my process column */		
./pdsymbfact_distdata.c:2505:					ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);			  
./pdsymbfact_distdata.c:2508:				pr = PROW( ib, grid ); // take care of diagonal node stored as L
./pdsymbfact_distdata.c:2510:				if ( mycol == pc ) { /* Block column ib in my process column */					
./pdsymbfact_distdata.c:2512:					ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);					
./pdsymbfact_distdata.c:2513:					// if(pr+ljb*grid->nprow==0)printf("iam %5d ib %5d ActiveFlagAll %5d pr %5d ljb %5d\n",iam,ib,ActiveFlagAll[pr+ljb*grid->nprow],pr,ljb);
./pdsymbfact_distdata.c:2519:		// printf("iam %5d ActiveFlagAll %5d\n",iam,ActiveFlagAll[0]);
./pdsymbfact_distdata.c:2522:		MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MAX,grid->cscp.comm);					  
./pdsymbfact_distdata.c:2528:			// if ( mycol == pc ) { /* Block column jb in my process column */
./pdsymbfact_distdata.c:2530:			for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pdsymbfact_distdata.c:2531:			for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pdsymbfact_distdata.c:2532:			for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pdsymbfact_distdata.c:2536:			for (j=0;j<grid->nprow;++j){
./pdsymbfact_distdata.c:2539:				pr = PROW( gb, grid );
./pdsymbfact_distdata.c:2540:				if(gb==jb)Root=pr;
./pdsymbfact_distdata.c:2541:				if(myrow==pr)Iactive=1;		
./pdsymbfact_distdata.c:2545:			quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
./pdsymbfact_distdata.c:2546:		// printf("jb: %5d Iactive %5d\n",jb,Iactive);
./pdsymbfact_distdata.c:2549:				// if(jb==0)printf("root:%5d jb: %5d ActiveFlag %5d \n",Root,jb,ActiveFlag[0]);
./pdsymbfact_distdata.c:2554:				for (j = 0; j < grid->nprow; ++j){
./pdsymbfact_distdata.c:2555:					if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pdsymbfact_distdata.c:2556:						ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pdsymbfact_distdata.c:2560:		// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
./pdsymbfact_distdata.c:2572:					// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pdsymbfact_distdata.c:2577:					for (j = 0; j < grid->nprow; ++j) {
./pdsymbfact_distdata.c:2578:						// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
./pdsymbfact_distdata.c:2584:					// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
./pdsymbfact_distdata.c:2597:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2599:	if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
./pdsymbfact_distdata.c:2602:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2607:		nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pdsymbfact_distdata.c:2615:			pr = PROW( k, grid );
./pdsymbfact_distdata.c:2616:			if ( myrow == pr ) {
./pdsymbfact_distdata.c:2623:		/* Every process receives the count, but it is only useful on the
./pdsymbfact_distdata.c:2624:		   diagonal processes.  */
./pdsymbfact_distdata.c:2629:		k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pdsymbfact_distdata.c:2661:		for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
./pdsymbfact_distdata.c:2662:			ib = myrow+lib*grid->nprow;  /* not sure */
./pdsymbfact_distdata.c:2668:				  if ( mycol == pc ) { /* Block column ib in my process column */	
./pdsymbfact_distdata.c:2673:				if ( mycol == pc ) { /* Block column ib in my process column */						
./pdsymbfact_distdata.c:2682:			ib = myrow+lib*grid->nprow;  /* not sure */
./pdsymbfact_distdata.c:2684:				pr = PROW( ib, grid );
./pdsymbfact_distdata.c:2715:							ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pdsymbfact_distdata.c:2727:						// #if ( PRNTlevel>=1 )
./pdsymbfact_distdata.c:2729:						// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
./pdsymbfact_distdata.c:2732:						// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pdsymbfact_distdata.c:2733:						// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pdsymbfact_distdata.c:2734:						// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pdsymbfact_distdata.c:2735:						// printf("\n");
./pdsymbfact_distdata.c:2757:#if ( PROFlevel>=1 )
./pdsymbfact_distdata.c:2759:	if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
./pdsymbfact_distdata.c:2808:#if ( PRNTlevel>=1 )
./pdsymbfact_distdata.c:2809:  if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
./pdsymbfact_distdata.c:2813:  k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pdutil.c:4:approvals from U.S. Dept. of Energy) 
./pdutil.c:16: * <pre>
./pdutil.c:20: * </pre>
./pdutil.c:26:/*! \brief Gather A from the distributed compressed row format to global A in compressed column format.
./pdutil.c:28:int pdCompRow_loc_to_CompCol_global
./pdutil.c:56:    int   it, n_loc, procs;
./pdutil.c:59:    CHECK_MALLOC(grid->iam, "Enter pdCompRow_loc_to_CompCol_global");
./pdutil.c:74:       FIRST PHASE: TRANSFORM A INTO DISTRIBUTED COMPRESSED COLUMN.
./pdutil.c:76:    dCompRow_to_CompCol_dist(m_loc, n, nnz_loc, a, colind, rowptr, &a_loc,
./pdutil.c:82:    printf("Proc %d\n", grid->iam);
./pdutil.c:83:    PrintInt10("rowind_loc", nnz_loc, rowind_loc);
./pdutil.c:84:    PrintInt10("colptr_loc", n+1, colptr_loc);
./pdutil.c:87:    procs = grid->nprow * grid->npcol;
./pdutil.c:88:    if ( !(fst_rows = (int_t *) intMalloc_dist(2*procs)) )
./pdutil.c:90:    n_locs = fst_rows + procs;
./pdutil.c:93:    for (i = 0; i < procs-1; ++i) n_locs[i] = fst_rows[i+1] - fst_rows[i];
./pdutil.c:94:    n_locs[procs-1] = n - fst_rows[procs-1];
./pdutil.c:95:    if ( !(recvcnts = SUPERLU_MALLOC(5*procs * sizeof(int))) )
./pdutil.c:97:    sendcnts = recvcnts + procs;
./pdutil.c:98:    rdispls = sendcnts + procs;
./pdutil.c:99:    sdispls = rdispls + procs;
./pdutil.c:100:    itemp_32 = sdispls + procs;
./pdutil.c:104:    /* n column starts for each column, and procs column ends for each block */
./pdutil.c:105:    if ( !(colptr_send = intMalloc_dist(n + procs)) )
./pdutil.c:107:    if ( !(colptr_blk = intMalloc_dist( (((size_t) n_loc)+1)*procs)) )
./pdutil.c:109:    for (i = 0, j = 0; i < procs; ++i) {
./pdutil.c:128:    for (i = 0; i < procs; ++i) {
./pdutil.c:138:    /*assert(k == (n_loc+1)*procs);*/
./pdutil.c:140:    /* Now prepare to transfer row indices and values. */
./pdutil.c:142:    for (i = 0; i < procs-1; ++i) {
./pdutil.c:146:    sendcnts[procs-1] = colptr_loc[n] - colptr_loc[fst_rows[procs-1]];
./pdutil.c:147:    for (i = 0; i < procs; ++i) {
./pdutil.c:152:    for (i = 0; i < procs-1; ++i) rdispls[i+1] = rdispls[i] + recvcnts[i];
./pdutil.c:154:    k = rdispls[procs-1] + recvcnts[procs-1]; /* Total received */
./pdutil.c:174:	for (i = 0; i < procs; ++i) {
./pdutil.c:184:    for (i = 0; i < procs; ++i) {
./pdutil.c:196:        for (i = 0; i < procs; ++i) {
./pdutil.c:208:       SECOND PHASE: GATHER TO GLOBAL A IN COMPRESSED COLUMN FORMAT.
./pdutil.c:221:    for (i = 0, nnz = 0; i < procs; ++i) nnz += itemp[i];
./pdutil.c:231:    for (i = 0; i < procs-1; ++i) {
./pdutil.c:235:    itemp_32[procs-1] = itemp[procs-1];
./pdutil.c:248:    for (i = 0; i < procs-1; ++i) {
./pdutil.c:252:    itemp_32[procs-1] = n_locs[procs-1];
./pdutil.c:257:    for (i = 1; i < procs; ++i) {
./pdutil.c:260:	itemp[i] += itemp[i-1]; /* prefix sum */
./pdutil.c:266:        printf("After pdCompRow_loc_to_CompCol_global()\n");
./pdutil.c:267:	dPrint_CompCol_Matrix_dist(GA);
./pdutil.c:281:    if ( !grid->iam ) printf("sizeof(NCformat) %lu\n", sizeof(NCformat));
./pdutil.c:282:    CHECK_MALLOC(grid->iam, "Exit pdCompRow_loc_to_CompCol_global");
./pdutil.c:285:} /* pdCompRow_loc_to_CompCol_global */
./pdutil.c:294: int_t row_to_proc[],
./pdutil.c:303:    int p, procs;
./pdutil.c:314:    procs = grid->nprow * grid->npcol;
./pdutil.c:315:    if ( !(sendcnts = SUPERLU_MALLOC(10*procs * sizeof(int))) )
./pdutil.c:317:    sendcnts_nrhs = sendcnts + procs;
./pdutil.c:318:    recvcnts = sendcnts_nrhs + procs;
./pdutil.c:319:    recvcnts_nrhs = recvcnts + procs;
./pdutil.c:320:    sdispls = recvcnts_nrhs + procs;
./pdutil.c:321:    sdispls_nrhs = sdispls + procs;
./pdutil.c:322:    rdispls = sdispls_nrhs + procs;
./pdutil.c:323:    rdispls_nrhs = rdispls + procs;
./pdutil.c:324:    ptr_to_ibuf = rdispls_nrhs + procs;
./pdutil.c:325:    ptr_to_dbuf = ptr_to_ibuf + procs;
./pdutil.c:327:    for (i = 0; i < procs; ++i) sendcnts[i] = 0;
./pdutil.c:329:    /* Count the number of X entries to be sent to each process.*/
./pdutil.c:331:        p = row_to_proc[perm[i]];
./pdutil.c:339:    for (i = 1; i < procs; ++i) {
./pdutil.c:347:    k = sdispls[procs-1] + sendcnts[procs-1];/* Total number of sends */
./pdutil.c:348:    l = rdispls[procs-1] + recvcnts[procs-1];/* Total number of recvs */
./pdutil.c:358:    for (i = 0; i < procs; ++i) {
./pdutil.c:366:	p = row_to_proc[j];
./pdutil.c:408:    int_t *row_to_proc, *inv_perm_c, *itemp;
./pdutil.c:411:    int          procs;
./pdutil.c:416:    procs = grid->nprow * grid->npcol;
./pdutil.c:418:    if ( !(row_to_proc = intMalloc_dist(A->nrow)) )
./pdutil.c:419:	ABORT("Malloc fails for row_to_proc[]");
./pdutil.c:420:    SOLVEstruct->row_to_proc = row_to_proc;
./pdutil.c:427:       EVERY PROCESS NEEDS TO KNOW GLOBAL PARTITION.
./pdutil.c:428:       SET UP THE MAPPING BETWEEN ROWS AND PROCESSES.
./pdutil.c:430:       NOTE: For those processes that do not own any row, it must
./pdutil.c:433:    if ( !(itemp = intMalloc_dist(procs+1)) )
./pdutil.c:437:    itemp[procs] = A->nrow;
./pdutil.c:438:    for (p = 0; p < procs; ++p) {
./pdutil.c:439:        for (i = itemp[p] ; i < itemp[p+1]; ++i) row_to_proc[i] = p;
./pdutil.c:443:      printf("fst_row = %d\n", fst_row);
./pdutil.c:444:      PrintInt10("row_to_proc", A->nrow, row_to_proc);
./pdutil.c:445:      PrintInt10("inv_perm_c", A->ncol, inv_perm_c);
./pdutil.c:451:    /* Compute the mapping between rows and processes. */
./pdutil.c:452:    /* XSL NOTE: What happens if # of mapped processes is smaller
./pdutil.c:453:       than total Procs?  For the processes without any row, let
./pdutil.c:457:    itemp[procs] = n;
./pdutil.c:458:    for (p = 0; p < procs; ++p) {
./pdutil.c:463:	    for (i = j ; i < k; ++i) row_to_proc[i] = p;
./pdutil.c:468:    get_diag_procs(A->ncol, LUstruct->Glu_persist, grid,
./pdutil.c:469:		   &SOLVEstruct->num_diag_procs,
./pdutil.c:470:		   &SOLVEstruct->diag_procs,
./pdutil.c:502:    SUPERLU_FREE(SOLVEstruct->row_to_proc);
./pdutil.c:504:    SUPERLU_FREE(SOLVEstruct->diag_procs);
./pdutil.c:535:      if ( !iam ) printf("\tSol %2d: ||X-Xtrue||/||X|| = %e\n", j, err);
./pdutil.c:567: 	nb = CEILING(nsupers, grid->nprow);
./psymbfact.c:4:approvals from U.S. Dept. of Energy) 
./psymbfact.c:14: * <pre>
./psymbfact.c:31: * </pre>
./psymbfact.c:41: * Internal protypes
./psymbfact.c:105:allocPrune_domain
./psymbfact.c:109:allocPrune_lvl
./psymbfact.c:143: int         nprocs_num,  /* Input - no of processors */
./psymbfact.c:144: int         nprocs_symb, /* Input - no of processors for the symbolic
./psymbfact.c:160: * <pre> 
./psymbfact.c:169: *        o symmetric structure pruning
./psymbfact.c:175: * nprocs_num (input) int
./psymbfact.c:176: *         Number of processors SuperLU_DIST is executed on, and the input 
./psymbfact.c:179: * nprocs_symb (input) int
./psymbfact.c:180: *         Number of processors on which the symbolic factorization is
./psymbfact.c:183: *         previously and has to be a power of 2.  It corresponds to
./psymbfact.c:199: *         permutation matrix Pr; perm_r[i] = j means column i of A is 
./psymbfact.c:200: *         in position j in Pr*A.
./psymbfact.c:231: *  Distrbute the vertices on the processors using a subtree to
./psymbfact.c:235: *  subtree to subcube computed previously for the symbolic
./psymbfact.c:237: *  from nprocs_num processors to nprocs_symb processors.
./psymbfact.c:239: *  Perform symbolic factorization guided by the separator tree provided by
./psymbfact.c:241: *  combined left-looking, right-looking approach. 
./psymbfact.c:242: * </pre>
./psymbfact.c:253:  Llu_symbfact_t Llu_symbfact; /* local L and U and pruned L and U data structures */
./psymbfact.c:264:  int_t fstVtx, lstVtx, mark, fstVtx_lid, vtx_lid, maxNvtcsPProc;
./psymbfact.c:272:#if ( PRNTlevel >= 1)
./psymbfact.c:275:#if ( PROFlevel>=1 )
./psymbfact.c:287:  if (nprocs_symb != 1) {
./psymbfact.c:288:    if (!(commLvls = (MPI_Comm *) SUPERLU_MALLOC(2*nprocs_symb*sizeof(MPI_Comm)))) {
./psymbfact.c:289:      fprintf (stderr, "Malloc fails for commLvls[].");  
./psymbfact.c:292:    PS.allocMem += 2 * nprocs_symb * sizeof(MPI_Comm);
./psymbfact.c:295:  nlvls = (int) LOG2( nprocs_num ) + 1;
./psymbfact.c:296:#if ( PROFlevel>=1 )
./psymbfact.c:297:  time_lvlsT = (double *) SUPERLU_MALLOC(3*nprocs_symb*(nlvls+1) 
./psymbfact.c:301:    fprintf (stderr, "Malloc fails for time_lvls[].");  
./psymbfact.c:304:  PS.allocMem += (3*nprocs_symb*(nlvls+1) + 3*(nlvls+1)) * sizeof(double);
./psymbfact.c:322:    fprintf (stderr, "Malloc fails for tempArray[].\n");  
./psymbfact.c:327:#if ( PROFlevel>=1 )  
./psymbfact.c:331:  /* Distribute vertices on processors */
./psymbfact.c:333:       symbfact_mapVtcs (iam, nprocs_num, nprocs_symb, A, fstVtxSep, sizes, 
./psymbfact.c:337:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:339:  /* Redistribute matrix A on processors following the distribution found
./psymbfact.c:341:  symbfact_distributeMatrix (iam, nprocs_num, nprocs_symb,  A, 
./psymbfact.c:345:  /* THE REST OF THE SYMBOLIC FACTORIZATION IS EXECUTED ONLY BY NPROCS_SYMB
./psymbfact.c:346:     PROCESSORS */
./psymbfact.c:347:  if ( iam < nprocs_symb ) {
./psymbfact.c:349:#if ( PROFlevel>=1 )
./psymbfact.c:356:    if (iinfo = symbfact_alloc (n, nprocs_symb, Pslu_freeable, 
./psymbfact.c:381:    if (nprocs_symb != 1) {
./psymbfact.c:382:      createComm (iam, nprocs_symb, commLvls, symb_comm);    
./psymbfact.c:384:#if ( PROFlevel>=1 )
./psymbfact.c:387:      if ((flinfo = cntsVtcs (n, iam, nprocs_symb, Pslu_freeable, &Llu_symbfact, 
./psymbfact.c:391:#if ( PROFlevel>=1 )
./psymbfact.c:400:    szSep = nprocs_symb;
./psymbfact.c:405:      npNode = nprocs_symb / szSep; 
./psymbfact.c:412:	if (szSep == nprocs_symb) {
./psymbfact.c:415:	    /* allocate storage for the pruned structures */
./psymbfact.c:416:#if ( PROFlevel>=1 )
./psymbfact.c:419:	    if ((flinfo = allocPrune_domain (fstVtx, lstVtx, 
./psymbfact.c:432:	    if (nprocs_symb != 1) 
./psymbfact.c:433:	      if((flinfo = allocPrune_lvl (&Llu_symbfact, &VInfo, &PS)) > 0)
./psymbfact.c:435:#if ( PROFlevel>=1 )
./psymbfact.c:445:#if ( PROFlevel>=1 )
./psymbfact.c:452:#if ( PROFlevel>=1 )
./psymbfact.c:461:#if ( PROFlevel>=1 )
./psymbfact.c:471:#if ( PROFlevel>=1 )
./psymbfact.c:487:    if (PS.maxSzLPr < Llu_symbfact.indLsubPr)
./psymbfact.c:488:      PS.maxSzLPr = Llu_symbfact.indLsubPr;
./psymbfact.c:489:    if (PS.maxSzUPr < Llu_symbfact.indUsubPr)
./psymbfact.c:490:      PS.maxSzUPr = Llu_symbfact.indUsubPr;
./psymbfact.c:498:      fprintf (stderr, "Malloc fails for xsup_beg_loc, xsup_end_loc.");
./psymbfact.c:502:    maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:517:	printf ("PE[%d] ERR nnzL %lld\n", iam, nnzL); 
./psymbfact.c:520:	printf ("PE[%d] ERR nnzU %lld\n", iam, nnzU);
./psymbfact.c:556:#if ( PROFlevel>=1 )
./psymbfact.c:560:#if ( PRNTlevel>=1 )
./psymbfact.c:576:			    Llu_symbfact.no_expand_pr);
./psymbfact.c:579:    stat_loc[14] = (float) Llu_symbfact.no_expand_pr;
./psymbfact.c:603:#if ( PROFlevel>=1 )
./psymbfact.c:636:      printf("\tMax szBlk          %ld\n", (long) VInfo.maxSzBlk);
./psymbfact.c:637:#if ( PRNTlevel>=2 )
./psymbfact.c:638:      printf("\t relax_gen %.2f, relax_curSep %.2f, relax_seps %.2f\n",
./psymbfact.c:641:      printf("LONG_MAX %ld\n", LONG_MAX);
./psymbfact.c:642:      printf("\tParameters: fill mem %ld fill pelt %ld\n",
./psymbfact.c:644:      printf("\tNonzeros in L       %lld\n", nnzL);
./psymbfact.c:645:      printf("\tNonzeros in U       %lld\n", nnzU);
./psymbfact.c:647:      printf("\tnonzeros in L+U-I   %lld\n", nnzLU);
./psymbfact.c:648:      printf("\tNo of supers   %ld\n", (long) nsuper);
./psymbfact.c:649:      printf("\tSize of G(L)   %ld\n", (long) szLGr);
./psymbfact.c:650:      printf("\tSize of G(U)   %ld\n", (long) szUGr);
./psymbfact.c:651:      printf("\tSize of G(L+U) %ld\n", (long) szLGr+szUGr);
./psymbfact.c:653:      printf("\tParSYMBfact (MB)      :\tL\\U MAX %.2f\tAVG %.2f\n",
./psymbfact.c:655:	     stat_glob[5]/nprocs_symb*1e-6);
./psymbfact.c:656:#if ( PRNTlevel>=2 )
./psymbfact.c:657:      printf("\tRL overestim (MB):\tL\\U MAX %.2f\tAVG %.2f\n",
./psymbfact.c:659:	     stat_glob[6]/nprocs_symb*1e-6);
./psymbfact.c:660:      printf("\tsnd/rcv buffers (MB):\tL\\U MAX %.2f\tAVG %.2f\n",
./psymbfact.c:662:	     stat_glob[8]/nprocs_symb*1e-6);
./psymbfact.c:663:      printf("\tSYMBfact 2*n+4*nvtcs_loc+2*maxNvtcsNds_loc:\tL\\U %.2f\n",
./psymbfact.c:665:      printf("\tint_t %d, int %d, long int %d, short %d, float %d, double %d\n", 
./psymbfact.c:668:      printf("\tDNS ALLSEPS:\t MAX %d\tAVG %.2f\n",
./psymbfact.c:669:	     (int_t) mem_glob[4], stat_glob[9]/nprocs_symb);
./psymbfact.c:670:      printf("\tDNS CURSEP:\t MAX %d\tAVG %.2f\n\n",
./psymbfact.c:671:	     (int_t) mem_glob[5], stat_glob[10]/nprocs_symb);
./psymbfact.c:673:      printf("\t MAX FILL Mem(L+U) / Mem(A) per processor %ld\n", fill_rcmd);    
./psymbfact.c:674:      printf("\t      Per elt MAX %ld AVG %ld\n", 
./psymbfact.c:676:      printf("\t      Per elt RL MAX %ld AVG %ld\n",
./psymbfact.c:678:      printf("\tM Nops:\t MAX %.2f\tAVG %.2f\n",
./psymbfact.c:679:	     mem_glob[11]*1e-6, (stat_glob[16]/nprocs_symb)*1e-6);
./psymbfact.c:682:      printf("\tEXPANSIONS: MAX/AVG\n");
./psymbfact.c:683:      printf("\tTOTAL: %d / %.2f\n",
./psymbfact.c:684:	     (int_t) mem_glob[6], stat_glob[11]/nprocs_symb);
./psymbfact.c:685:      printf("\tREALLOC: %.f / %.2f RL_CP %.f / %.2f PR_CP %.f / %.2f\n",
./psymbfact.c:686:	     mem_glob[7], stat_glob[12]/nprocs_symb,
./psymbfact.c:687:	     mem_glob[8], stat_glob[13]/nprocs_symb,
./psymbfact.c:688:	     mem_glob[9], stat_glob[14]/nprocs_symb);
./psymbfact.c:690:      printf ("\n\tDATA MSGS  noMsgs*10^3 %.3f/%.3f size (MB) %.3f/%.3f \n",
./psymbfact.c:691:	      stat_msgs_g[2]*1e-3, stat_msgs_g[4]/nprocs_symb*1e-3,
./psymbfact.c:693:      printf ("\tTOTAL MSGS noMsgs*10^3 %.3f/%.3f size (MB) %.3f/%.3f \n",
./psymbfact.c:694:	      stat_msgs_g[3]*1e-3, stat_msgs_g[5]/nprocs_symb*1e-3,
./psymbfact.c:698:#if ( PROFlevel>=1 )
./psymbfact.c:699:      printf("Distribute matrix time = %8.3f\n", t_symbFact[0]);
./psymbfact.c:700:      printf("Count vertices time    = %8.3f\n", t_symbFact[2]);
./psymbfact.c:701:      printf("Symbfact DIST time     = %8.3f\n", t_symbFact[1]);
./psymbfact.c:703:      printf("\nLvl\t    Time\t    Init\t   Inter\t    Intra\n");
./psymbfact.c:708:	for (p = 0; p < nprocs_symb; p++) {
./psymbfact.c:726:	printf ("%d \t%.3f/%.3f\t%.3f/%.3f\t%.3f/%.3f\t%.3f/%.3f\n", i,
./psymbfact.c:727:		time_lvlsg[1], time_lvlsg[2] / nprocs_symb,
./psymbfact.c:728:		time_lvlsg[3], time_lvlsg[4] / nprocs_symb,
./psymbfact.c:729:		time_lvlsg[5], time_lvlsg[6] /nprocs_symb,
./psymbfact.c:730:		time_lvlsg[7], time_lvlsg[8] / nprocs_symb); 
./psymbfact.c:732:      printf("\t   %8.3f \n", time_lvlsg[0]);    
./psymbfact.c:736:#if ( PROFlevel>=1 )
./psymbfact.c:740:    symbfact_free (iam, nprocs_symb, &Llu_symbfact, &VInfo, &CS);
./psymbfact.c:741:  } /* if (iam < nprocs_symb) */  
./psymbfact.c:755:  if (iam < nprocs_symb && nprocs_symb != 1) 
./psymbfact.c:756:    freeComm (iam, nprocs_symb, commLvls, symb_comm);     
./psymbfact.c:774: * <pre> 
./psymbfact.c:778: * </pre>
./psymbfact.c:808:  PS->maxSzLPr = 0;
./psymbfact.c:809:  PS->maxSzUPr = 0;
./psymbfact.c:821: int    iam,         /* Input - my processor number */
./psymbfact.c:822: int    nprocs_symb, /* Input - no of processors for symbolic factorization */
./psymbfact.c:823: Pslu_freeable_t *Pslu_freeable, /* Input -globToLoc and maxNvtcsPProc */
./psymbfact.c:834: * <pre>
./psymbfact.c:841: * </pre>
./psymbfact.c:848:  int_t *xlsub, *lsub, *xusub, *usub, *globToLoc, maxNvtcsPProc;
./psymbfact.c:857:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:863:      fprintf(stderr, "Malloc fails for minElt_vtx[].");
./psymbfact.c:874:  szSep = nprocs_symb;
./psymbfact.c:880:    npNode = nprocs_symb / szSep; 
./psymbfact.c:911:	if (szSep == nprocs_symb) 
./psymbfact.c:916:#if ( PRNTlevel>=1 )
./psymbfact.c:956:	} /* if (szSep != nprocs_symb) */
./psymbfact.c:975: int iam,             /* Input -process number */
./psymbfact.c:976: int nprocs_num,      /* Input -number of processors */
./psymbfact.c:977: int nprocs_symb,     /* Input -number of procs for symbolic factorization */
./psymbfact.c:981: Pslu_freeable_t *Pslu_freeable, /* Output -globToLoc and maxNvtcsPProc 
./psymbfact.c:991: * <pre>
./psymbfact.c:996: *  matrix A on nprocs_symb processors, using the separator tree
./psymbfact.c:997: *  returned by a graph partitioning algorithm from the previous step
./psymbfact.c:998: *  of the symbolic factorization.  The number of processors
./psymbfact.c:999: *  nprocs_symb must be a power of 2.
./psymbfact.c:1004: *  A subtree to subcube algorithm is used first to map the processors
./psymbfact.c:1008: *  are distributed on the processors affected to this node, using a
./psymbfact.c:1012: *  computed.  The array globToLoc and maxNvtcsPProc of Pslu_freeable
./psymbfact.c:1014: * </pre>
./psymbfact.c:1020:  int_t noVtcsProc, noBlk;
./psymbfact.c:1021:  int_t nvtcs_loc; /* number of vertices owned by process iam */
./psymbfact.c:1022:  int_t nblks_loc; /* no of blocks owned by process iam */
./psymbfact.c:1024:  int_t maxNvtcsPProc, maxNvtcsNds_loc, nvtcsNds_loc, maxNeltsVtx;
./psymbfact.c:1026:  int_t *vtcs_pe;  /* contains the number of vertices on each processor */
./psymbfact.c:1027:  int   *avail_pes; /* contains the processors to be used at each level */
./psymbfact.c:1032:    fprintf (stderr, "Malloc fails for globToLoc[].");
./psymbfact.c:1036:  if (!(avail_pes = (int *) SUPERLU_MALLOC(nprocs_symb*sizeof(int)))) {
./psymbfact.c:1037:    fprintf (stderr, "Malloc fails for avail_pes[].");
./psymbfact.c:1040:  PS->allocMem += nprocs_symb*sizeof(int);
./psymbfact.c:1041:  if (!(vtcs_pe = (int_t *) SUPERLU_MALLOC(nprocs_symb*sizeof(int_t)))) {
./psymbfact.c:1042:    fprintf (stderr, "Malloc fails for vtcs_pe[].");
./psymbfact.c:1045:  PS->allocMem += nprocs_symb*sizeof(int_t);
./psymbfact.c:1049:  for (p = 0; p < nprocs_symb; p++) {
./psymbfact.c:1058:  /* distribute data among processors */
./psymbfact.c:1059:  szSep = nprocs_symb;
./psymbfact.c:1063:    npNode = nprocs_symb / szSep; 
./psymbfact.c:1074:      if (szSep == nprocs_symb) {
./psymbfact.c:1089:	noVtcsProc = maxSzBlk;
./psymbfact.c:1092:	/* first allocate processors from previous levels */	
./psymbfact.c:1099:	    while (kk < noVtcsProc && k < lstVtx) {
./psymbfact.c:1124:	  while (kk < noVtcsProc && k < lstVtx) {
./psymbfact.c:1137:	/* Add the unused processors to the avail_pes list of pes */
./psymbfact.c:1144:    if (maxNvtcsNds_loc < nvtcsNds_loc && szSep != nprocs_symb)
./psymbfact.c:1150:#if ( PRNTlevel>=2 )
./psymbfact.c:1152:    PrintInt10 (" novtcs_pe", nprocs_symb, vtcs_pe);
./psymbfact.c:1154:  /* determine maximum number of vertices among processors */
./psymbfact.c:1155:  maxNvtcsPProc = vtcs_pe[0];
./psymbfact.c:1157:  for (p = 1; p < nprocs_symb; p++) {
./psymbfact.c:1158:    if (maxNvtcsPProc < vtcs_pe[p])
./psymbfact.c:1159:      maxNvtcsPProc = vtcs_pe[p];
./psymbfact.c:1162:#if ( PRNTlevel>=2 )
./psymbfact.c:1164:    printf ("  MaxNvtcsPerProc %d MaxNvtcs/Avg %e\n\n", 
./psymbfact.c:1165:	    maxNvtcsPProc, ((float) maxNvtcsPProc * nprocs_symb)/(float)n);
./psymbfact.c:1168:  if (iam < nprocs_symb)
./psymbfact.c:1179:      globToLoc[k] = globToLoc[k] * maxNvtcsPProc + vtcs_pe[p];
./psymbfact.c:1188:  if (iam < nprocs_symb)
./psymbfact.c:1194:  Pslu_freeable->maxNvtcsPProc   = maxNvtcsPProc;
./psymbfact.c:1196:  if (iam < nprocs_symb) {
./psymbfact.c:1214: int   iam,             /* Input - my processor number */  
./psymbfact.c:1215: int   nprocs_num,      /* Input - number of processors */
./psymbfact.c:1216: int   nprocs_symb,     /* Input - number of processors for the
./psymbfact.c:1228: MPI_Comm    *num_comm  /* Input - communicator for nprocs_num procs */
./psymbfact.c:1233: * <pre>
./psymbfact.c:1241: * </pre>
./psymbfact.c:1255:  /* number of nonzeros to send/receive per processor */
./psymbfact.c:1259:  int_t *globToLoc, *begEndBlks_loc, nblks_loc, nvtcs_loc, maxNvtcsPProc;
./psymbfact.c:1276:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:1277:  nnzToRecv = intCalloc_symbfact(3 * (int_t)nprocs_num);
./psymbfact.c:1278:  nnzToSend = nnzToRecv + nprocs_num;
./psymbfact.c:1279:  nnzAinf_toSnd = nnzToRecv + 2 * nprocs_num;
./psymbfact.c:1282:    COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS, THEN ALLOCATE
./psymbfact.c:1289:    irow   = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./psymbfact.c:1317:  for (p = 0; p < nprocs_num; p++)
./psymbfact.c:1326:  for (p = 0; p < nprocs_num; p++) {
./psymbfact.c:1335:  nnz_iam = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
./psymbfact.c:1342:  if ( !(ptr_toSnd = intCalloc_symbfact((int_t) nprocs_num)) )
./psymbfact.c:1344:  if ( !(ptr_toRcv = intCalloc_symbfact((int_t) nprocs_num)) )
./psymbfact.c:1348:   processor p */
./psymbfact.c:1350:  /* for (i = 0, j = 0, p = 0; p < nprocs_num; p++) { */
./psymbfact.c:1360:    if (p >= nprocs_num) break;
./psymbfact.c:1365:      /* column i of Ainf will be send to a processor  */
./psymbfact.c:1381:  for (i = 0, j = 0, p = 0; p < nprocs_num; p++) {
./psymbfact.c:1401:     For each processor, we store first the columns to be sent,
./psymbfact.c:1455:     each processor (structure needed in MPI_Alltoallv */
./psymbfact.c:1456:  for (i = 0, j = 0, p = 0; p < nprocs_num; p++) {
./psymbfact.c:1467:  if (nprocs_num > 1) {
./psymbfact.c:1469:    intBuf1 = (int *) SUPERLU_MALLOC(4 * nprocs_num * sizeof(int));
./psymbfact.c:1470:    intBuf2 = intBuf1 + nprocs_num;
./psymbfact.c:1471:    intBuf3 = intBuf1 + 2 * nprocs_num;
./psymbfact.c:1472:    intBuf4 = intBuf1 + 3 * nprocs_num;
./psymbfact.c:1474:    for (p=0; p<nprocs_num; p++) {
./psymbfact.c:1512:     THIS IS PERFORMED ONLY BY NPROCS_SYMB PROCESSORS
./psymbfact.c:1514:  if (iam < nprocs_symb) {
./psymbfact.c:1527:    for (i = 0, p = 0; p < nprocs_num; p++) {
./psymbfact.c:1573:    for (i = 0, p = 0; p < nprocs_num; p++) {
./psymbfact.c:1623:float allocPrune_lvl
./psymbfact.c:1633: * <pre>
./psymbfact.c:1634: * Allocate storage for data structures necessary for pruned graphs.
./psymbfact.c:1635: * For those unpredictable size, make a guess as FILL * n.
./psymbfact.c:1640: * </pre>
./psymbfact.c:1644:  int_t  nzlmaxPr, nzumaxPr, *xlsubPr, *xusubPr, *lsubPr, *usubPr;
./psymbfact.c:1645:  int_t  nvtcs_loc, no_expand_pr, x_sz;
./psymbfact.c:1651:  no_expand_pr = 0;
./psymbfact.c:1655:  if (Llu_symbfact->szLsubPr)
./psymbfact.c:1656:    SUPERLU_FREE( Llu_symbfact->lsubPr );
./psymbfact.c:1657:  if (Llu_symbfact->szUsubPr)
./psymbfact.c:1658:    SUPERLU_FREE( Llu_symbfact->usubPr );
./psymbfact.c:1659:  if (Llu_symbfact->xlsubPr)
./psymbfact.c:1660:    SUPERLU_FREE( Llu_symbfact->xlsubPr );
./psymbfact.c:1661:  if (Llu_symbfact->xusubPr)
./psymbfact.c:1662:    SUPERLU_FREE( Llu_symbfact->xusubPr );
./psymbfact.c:1669:  nzlmaxPr = 2 * FILL * VInfo->maxNvtcsNds_loc;
./psymbfact.c:1670:  nzumaxPr = 2 * FILL * VInfo->maxSzBlk;  
./psymbfact.c:1674:    xlsubPr   = intMalloc_symbfact(VInfo->maxNvtcsNds_loc + 1);
./psymbfact.c:1675:    xusubPr   = intMalloc_symbfact(VInfo->maxNvtcsNds_loc + 1);
./psymbfact.c:1677:    lsubPr = (int_t *) SUPERLU_MALLOC (nzlmaxPr * lword);
./psymbfact.c:1678:    usubPr = (int_t *) SUPERLU_MALLOC (nzumaxPr * lword);
./psymbfact.c:1680:    while ( !lsubPr || !usubPr ) {
./psymbfact.c:1681:      if ( lsubPr ) SUPERLU_FREE( lsubPr ); 
./psymbfact.c:1682:      if ( usubPr ) SUPERLU_FREE( usubPr );
./psymbfact.c:1684:      nzlmaxPr /= 2;     nzlmaxPr = alpha * (float) nzlmaxPr;
./psymbfact.c:1685:      nzumaxPr /= 2;     nzumaxPr = alpha * (float) nzumaxPr;
./psymbfact.c:1687:      if ( nzumaxPr < x_sz ) {
./psymbfact.c:1688:	fprintf(stderr, "Not enough memory to perform factorization.\n");
./psymbfact.c:1691:      lsubPr  = (int_t *) SUPERLU_MALLOC(nzlmaxPr * lword);
./psymbfact.c:1692:      usubPr  = (int_t *) SUPERLU_MALLOC(nzumaxPr * lword);
./psymbfact.c:1693:      ++no_expand_pr;
./psymbfact.c:1697:    xlsubPr = NULL; lsubPr = NULL;
./psymbfact.c:1698:    xusubPr = NULL; usubPr = NULL;
./psymbfact.c:1699:    nzlmaxPr = 0; nzumaxPr = 0;
./psymbfact.c:1706:  if (PS->maxSzLPr < Llu_symbfact->indLsubPr)
./psymbfact.c:1707:    PS->maxSzLPr = Llu_symbfact->indLsubPr;
./psymbfact.c:1708:  if (PS->maxSzUPr < Llu_symbfact->indUsubPr)
./psymbfact.c:1709:    PS->maxSzUPr = Llu_symbfact->indUsubPr;
./psymbfact.c:1711:  Llu_symbfact->lsubPr   = lsubPr;
./psymbfact.c:1712:  Llu_symbfact->xlsubPr  = xlsubPr;
./psymbfact.c:1713:  Llu_symbfact->usubPr   = usubPr;
./psymbfact.c:1714:  Llu_symbfact->xusubPr  = xusubPr;
./psymbfact.c:1715:  Llu_symbfact->szLsubPr = nzlmaxPr;
./psymbfact.c:1716:  Llu_symbfact->szUsubPr = nzumaxPr;
./psymbfact.c:1717:  Llu_symbfact->indLsubPr = 0;
./psymbfact.c:1718:  Llu_symbfact->indUsubPr = 0;
./psymbfact.c:1720:  Llu_symbfact->no_expand_pr += no_expand_pr;
./psymbfact.c:1725:allocPrune_domain
./psymbfact.c:1737: * <pre>
./psymbfact.c:1738: * Allocate storage for data structures necessary for pruned graphs.
./psymbfact.c:1739: * For those unpredictable size, make a guess as FILL * n.
./psymbfact.c:1744: * </pre>
./psymbfact.c:1748:  int_t  nzlmaxPr, nzumaxPr, *xlsubPr, *xusubPr, *lsubPr, *usubPr;
./psymbfact.c:1749:  int_t  nvtcs_loc, no_expand_pr, x_sz;
./psymbfact.c:1755:  no_expand_pr = 0;
./psymbfact.c:1759:  /* Guess for prune graph */
./psymbfact.c:1761:  nzlmaxPr = nzumaxPr = 2*FILL * x_sz;
./psymbfact.c:1765:    xlsubPr   = intMalloc_symbfact(x_sz+1);
./psymbfact.c:1766:    xusubPr   = intMalloc_symbfact(x_sz+1);
./psymbfact.c:1768:    lsubPr = (int_t *) SUPERLU_MALLOC (nzlmaxPr * lword);
./psymbfact.c:1769:    usubPr = (int_t *) SUPERLU_MALLOC (nzumaxPr * lword);
./psymbfact.c:1771:    while ( !lsubPr || !usubPr ) {
./psymbfact.c:1772:      if ( lsubPr ) SUPERLU_FREE(lsubPr); 
./psymbfact.c:1773:      if ( usubPr ) SUPERLU_FREE(usubPr);
./psymbfact.c:1775:      nzlmaxPr /= 2;     nzlmaxPr = alpha * (float) nzlmaxPr;
./psymbfact.c:1776:      nzumaxPr /= 2;     nzumaxPr = alpha * (float) nzumaxPr;
./psymbfact.c:1778:      if ( nzumaxPr < x_sz ) {
./psymbfact.c:1779:	fprintf(stderr, "Not enough memory to perform factorization.\n");
./psymbfact.c:1782:      lsubPr  = (void *) SUPERLU_MALLOC(nzlmaxPr * lword);
./psymbfact.c:1783:      usubPr  = (void *) SUPERLU_MALLOC(nzumaxPr * lword);
./psymbfact.c:1784:      ++no_expand_pr;
./psymbfact.c:1788:    xlsubPr = NULL;
./psymbfact.c:1789:    xusubPr = NULL;
./psymbfact.c:1792:  Llu_symbfact->lsubPr   = lsubPr;
./psymbfact.c:1793:  Llu_symbfact->xlsubPr  = xlsubPr;
./psymbfact.c:1794:  Llu_symbfact->usubPr   = usubPr;
./psymbfact.c:1795:  Llu_symbfact->xusubPr  = xusubPr;
./psymbfact.c:1796:  Llu_symbfact->szLsubPr = nzlmaxPr;
./psymbfact.c:1797:  Llu_symbfact->szUsubPr = nzumaxPr;
./psymbfact.c:1798:  Llu_symbfact->indLsubPr = 0;
./psymbfact.c:1799:  Llu_symbfact->indUsubPr = 0;
./psymbfact.c:1804:  PS->maxSzLPr = Llu_symbfact->indLsubPr;
./psymbfact.c:1805:  PS->maxSzUPr = Llu_symbfact->indUsubPr;
./psymbfact.c:1807:  Llu_symbfact->no_expand_pr = no_expand_pr;
./psymbfact.c:1818: int   nprocs,  /* Input - number of processors for the symbolic
./psymbfact.c:1829: * <pre>
./psymbfact.c:1831: * routines. For those unpredictable size, make a guess as FILL * nnz(A).
./psymbfact.c:1836: * </pre>
./psymbfact.c:1851:  nlvls = (int) LOG2( nprocs ) + 1;
./psymbfact.c:1874:      fprintf(stderr, "Not enough memory to perform factorization.\n");
./psymbfact.c:1882:  if (nprocs == 1)
./psymbfact.c:1888:  CS->rcv_interLvl = intMalloc_symbfact (2 * (int_t) nprocs + 1);
./psymbfact.c:1889:  CS->snd_interLvl = intMalloc_symbfact (2 * (int_t) nprocs + 1);
./psymbfact.c:1890:  CS->ptr_rcvBuf   = intMalloc_symbfact (2 * (int_t) nprocs );
./psymbfact.c:1891:  CS->rcv_intraLvl = intMalloc_symbfact ((int_t) nprocs + 1);
./psymbfact.c:1892:  CS->snd_intraLvl = intMalloc_symbfact ((int_t) nprocs + 1);
./psymbfact.c:1902:  for (p = 0; p < nprocs; p++) {
./psymbfact.c:1933: int   iam,       /* Input - my processor number */
./psymbfact.c:1936: int_t vtx_prid,  /* Input - */
./psymbfact.c:1943: int_t snrep_lid,    /* local index of current supernode reprezentative */
./psymbfact.c:1944: int_t szSn,         /* size of supernode with snrep_lid reprezentative */
./psymbfact.c:1959: int_t *p_prval_curvtx,
./psymbfact.c:1965:  int_t k, vtx_elt, ind, pr, pr_lid, mem_error, ii, jj, compRcvd;
./psymbfact.c:1966:  int_t *xsub, *sub, *xsubPr, *subPr, *xsub_rcvd, *xsub_src, *sub_src;
./psymbfact.c:1967:  int_t pr_elt, next, prval_curvtx, maxNvtcsPProc;
./psymbfact.c:1972:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:1975:  prval_curvtx  = *p_prval_curvtx;
./psymbfact.c:1981:    xsubPr = Llu_symbfact->xusubPr; subPr = Llu_symbfact->usubPr;
./psymbfact.c:1986:    xsubPr = Llu_symbfact->xlsubPr; subPr = Llu_symbfact->lsubPr;
./psymbfact.c:2001:	  if (vtx_elt < prval_curvtx)
./psymbfact.c:2002:	    prval_curvtx = vtx_elt;
./psymbfact.c:2007:	printf ("Pe[%d] ERROR diag elt in U part vtx " IFMT " dom_s %d fstV "
./psymbfact.c:2029:  printf ("compL %d vtx %d vtx_lid %d vtx_prid %d vtx_bel_othSn %d\n", 
./psymbfact.c:2030:	  computeL, vtx, vtx_lid, vtx_prid, vtx_bel_othSn);
./psymbfact.c:2031:  PrintInt10 ("A(:, v)", x_aind_end - x_aind_beg, &(sub[xsub[vtx_lid]]));
./psymbfact.c:2034:  ind = xsubPr[vtx_prid];
./psymbfact.c:2041:      pr_lid = snrep_lid;
./psymbfact.c:2044:      pr_lid = subPr[ind];
./psymbfact.c:2045:      ind = subPr[ind - 1];
./psymbfact.c:2050:    if (pr_lid >= VInfo->nvtcs_loc) {
./psymbfact.c:2051:      compRcvd = TRUE;
./psymbfact.c:2053:      pr_lid -= VInfo->nvtcs_loc;
./psymbfact.c:2054:      k = xsub_src[pr_lid] + RCVD_IND;
./psymbfact.c:2057:      compRcvd = FALSE;
./psymbfact.c:2059:      k = xsub_src[pr_lid];
./psymbfact.c:2062:    PS->nops += xsub_src[pr_lid+1] - xsub_src[pr_lid];
./psymbfact.c:2063:    for (; k < xsub_src[pr_lid+1]; k++) {
./psymbfact.c:2064:      pr_elt = sub_src[k];
./psymbfact.c:2065:      if (pr_elt >= vtx && marker[pr_elt] != mark_vtx) {
./psymbfact.c:2084:	  if (!compRcvd) 
./psymbfact.c:2088:	sub[next] = pr_elt; next ++;
./psymbfact.c:2090:	if (pr_elt < lstVtx) neltsVtx_CSep ++;
./psymbfact.c:2091:	if (computeL && pr_elt == vtx)
./psymbfact.c:2094:	  if (marker[pr_elt] == mark_vtx - 2)
./psymbfact.c:2095:	    if (pr_elt < prval_curvtx)
./psymbfact.c:2096:	      prval_curvtx = pr_elt;
./psymbfact.c:2097:	marker[pr_elt] = mark_vtx;	
./psymbfact.c:2104:    printf("Pe[%d] At column " IFMT ", ", iam, vtx);
./psymbfact.c:2148:  *p_prval_curvtx  = prval_curvtx;
./psymbfact.c:2153:updateRcvd_prGraph
./psymbfact.c:2156: int   iam,       /* Input - my processor number */
./psymbfact.c:2161: int_t pr_offset,
./psymbfact.c:2172:  int_t i, k, nelts, prVal, vtx_elt, vtx_elt_lid, ind;
./psymbfact.c:2174:  int_t *xsub, *sub, *xsub_rcvd, *xsubPr, *subPr, szsubPr, *p_indsubPr;
./psymbfact.c:2175:  int_t maxNvtcsPProc, *globToLoc, mem_error;
./psymbfact.c:2178:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:2186:    xsubPr = Llu_symbfact->xlsubPr; subPr = Llu_symbfact->lsubPr;
./psymbfact.c:2187:    p_indsubPr = &(Llu_symbfact->indLsubPr);
./psymbfact.c:2188:    szsubPr = Llu_symbfact->szLsubPr;
./psymbfact.c:2193:    xsubPr = Llu_symbfact->xusubPr; subPr = Llu_symbfact->usubPr;
./psymbfact.c:2194:    p_indsubPr = &(Llu_symbfact->indUsubPr);
./psymbfact.c:2195:    szsubPr = Llu_symbfact->szUsubPr;
./psymbfact.c:2198:  /* count number of elements in transpose representation of sub_rcvd */
./psymbfact.c:2211:    prVal = sub_rcvd[i];
./psymbfact.c:2217:      if (vtx_elt > prVal)
./psymbfact.c:2232:  vtx_lid = fstVtx_toUpd_lid - pr_offset;
./psymbfact.c:2236:      xsubPr[vtx_lid] = ind + 1;
./psymbfact.c:2238:      marker[i] = xsubPr[vtx_lid] - 1;
./psymbfact.c:2247:  /* test if enough memory in usubPr array */
./psymbfact.c:2248:  if (ind >= szsubPr) {
./psymbfact.c:2250:	psymbfact_prLUXpand (iam, ind, computeL, Llu_symbfact, PS))
./psymbfact.c:2253:      subPr = Llu_symbfact->lsubPr;  
./psymbfact.c:2255:      subPr = Llu_symbfact->usubPr;
./psymbfact.c:2257:  *p_indsubPr = ind;
./psymbfact.c:2264:    prVal = sub_rcvd[i];
./psymbfact.c:2267:      if (vtx_elt > prVal)
./psymbfact.c:2273:	    vtx_lid_p = vtx_elt_lid - pr_offset;
./psymbfact.c:2275:	    /* add vtx to structure of pruned graph */
./psymbfact.c:2276:	    if (marker[vtx_elt_lid] != xsubPr[vtx_lid_p] - 1) 
./psymbfact.c:2277:	      subPr[marker[vtx_elt_lid] - 2] = marker[vtx_elt_lid] + 1;
./psymbfact.c:2278:	    subPr[marker[vtx_elt_lid] + 1] = vtx - fstVtx_srcUpd + VInfo->nvtcs_loc;
./psymbfact.c:2279:	    subPr[marker[vtx_elt_lid]] = EMPTY;
./psymbfact.c:2294:update_prGraph 
./psymbfact.c:2300: int_t snrep_lid,   /* local index of current supernode reprezentative */
./psymbfact.c:2301: int_t pr_offset,   /* offset in the indexing of prune structure */
./psymbfact.c:2302: int_t prval_cursn, /* prune value of current supernode reprezentative */
./psymbfact.c:2313:  int_t sn_elt, sn_elt_prid;
./psymbfact.c:2314:  int_t *globToLoc, maxNvtcsPProc;
./psymbfact.c:2315:  int_t *xsub, *sub, *xsubPr, *subPr;
./psymbfact.c:2316:  int_t *p_indsubPr, szsubPr;
./psymbfact.c:2319:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:2323:    xsubPr = Llu_symbfact->xlsubPr; subPr = Llu_symbfact->lsubPr;
./psymbfact.c:2324:    p_indsubPr = &(Llu_symbfact->indLsubPr);
./psymbfact.c:2325:    szsubPr = Llu_symbfact->szLsubPr;
./psymbfact.c:2329:    xsubPr = Llu_symbfact->xusubPr; subPr = Llu_symbfact->usubPr;
./psymbfact.c:2330:    p_indsubPr = &(Llu_symbfact->indUsubPr);
./psymbfact.c:2331:    szsubPr = Llu_symbfact->szUsubPr;
./psymbfact.c:2336:  if (prval_cursn != n)
./psymbfact.c:2337:    maxElt = prval_cursn;
./psymbfact.c:2341:    if (prval_cursn == n) {
./psymbfact.c:2349:      if (sub[kmax] > prval_cursn) 
./psymbfact.c:2351:      else if (sub[kmin] <= prval_cursn)
./psymbfact.c:2365:  while (sub[k] <= prval_cursn && k < xsub_snp1) {
./psymbfact.c:2368:      sn_elt_prid = LOCAL_IND( globToLoc[sn_elt] ) - pr_offset;
./psymbfact.c:2369:      if ((*p_indsubPr) + 2 >= szsubPr) {
./psymbfact.c:2371:	    psymbfact_prLUXpand (iam, 0, computeL, Llu_symbfact, PS))
./psymbfact.c:2374:	  subPr = Llu_symbfact->lsubPr;  szsubPr = Llu_symbfact->szLsubPr;
./psymbfact.c:2377:	  subPr = Llu_symbfact->usubPr;  szsubPr = Llu_symbfact->szUsubPr;
./psymbfact.c:2380:      /* add krow to structure of pruned graph */
./psymbfact.c:2381:      subPr[(*p_indsubPr) + 1] = snrep_lid;
./psymbfact.c:2382:      subPr[(*p_indsubPr)] = xsubPr[sn_elt_prid];
./psymbfact.c:2383:      xsubPr[sn_elt_prid] = (*p_indsubPr) + 1;
./psymbfact.c:2384:      (*p_indsubPr) += 2;
./psymbfact.c:2387:      /* move prune val in the first position */
./psymbfact.c:2429:  int_t  *xlsubPr, *xusubPr; 
./psymbfact.c:2431:  int_t  vtx_lid, vtx_prid, vtx, vtx_super, vtx_elt, maxNvtcsPProc;
./psymbfact.c:2432:  int_t  ind, pr, pr_elt, newnext, k, vtx_elt_lid;
./psymbfact.c:2442:  /* next vertex belongs to current supernode pruned structure */
./psymbfact.c:2446:  /* prune structure variables */
./psymbfact.c:2447:  int_t prval_cursn, prval_curvtx, pr_offset;
./psymbfact.c:2459:  xusubPr  = Llu_symbfact->xusubPr; 
./psymbfact.c:2460:  xlsubPr  = Llu_symbfact->xlsubPr;   
./psymbfact.c:2461:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:2478:  prval_cursn = n;
./psymbfact.c:2481:  /* set up to EMPTY xlsubPr[], xusubPr[] */
./psymbfact.c:2482:  if (PS->maxSzLPr < Llu_symbfact->indLsubPr)
./psymbfact.c:2483:    PS->maxSzLPr = Llu_symbfact->indLsubPr;
./psymbfact.c:2484:  if (PS->maxSzUPr < Llu_symbfact->indUsubPr)
./psymbfact.c:2485:    PS->maxSzUPr = Llu_symbfact->indUsubPr;
./psymbfact.c:2487:    xlsubPr[i] = EMPTY;
./psymbfact.c:2488:    xusubPr[i] = EMPTY;
./psymbfact.c:2490:  Llu_symbfact->indLsubPr = 0;
./psymbfact.c:2491:  Llu_symbfact->indUsubPr = 0;
./psymbfact.c:2500:  vtx_prid = 0;
./psymbfact.c:2502:  pr_offset = vtx_lid;
./psymbfact.c:2505:    updateRcvd_prGraph (n, iam, lsub_rcvd, lsub_rcvd_sz,
./psymbfact.c:2506:			fstVtx_blk, lstVtx_blk, pr_offset, 1, marker,
./psymbfact.c:2508:    updateRcvd_prGraph (n, iam, usub_rcvd, usub_rcvd_sz,
./psymbfact.c:2509:			fstVtx_blk, lstVtx_blk, pr_offset, 0, marker,
./psymbfact.c:2513:  for (vtx = fstVtx_blk; vtx < lstVtx_blk; vtx++, vtx_lid ++, vtx_prid ++) {
./psymbfact.c:2524:    prval_curvtx   = n;
./psymbfact.c:2527:	symbfact_vtx (n, iam, vtx, vtx_lid, vtx_prid, 1, domain_symb, 
./psymbfact.c:2534:		      &neltsMatched_L, markl1_vtx, &prval_curvtx, 
./psymbfact.c:2540:    PrintInt10 ("L(:, %d)", nextl - xlsub[vtx_lid], &(lsub[xlsub[vtx_lid]]));
./psymbfact.c:2545:	symbfact_vtx (n, iam, vtx, vtx_lid, vtx_prid, 0, domain_symb, 
./psymbfact.c:2552:		      &neltsMatched_U, marku1_vtx, &prval_curvtx,
./psymbfact.c:2558:    PrintInt10 ("U(%d, :)", nextu - xusub[vtx_lid], &(usub[xusub[vtx_lid]])); 
./psymbfact.c:2591:    printf ("[%d] vtx %d pr %d szsn %d nVtx_L %d nZrSn_L %d nZrVtx_L %d\n",
./psymbfact.c:2592:	    iam, vtx, prval_curvtx, szsn,neltsVtx_L, neltsZrSn_L, neltsZrVtx_L);
./psymbfact.c:2593:    printf ("  [%d] nVtx_U %d, nZrSn_U %d nZrVtx_U %d nextl %d nextu %d\n",
./psymbfact.c:2595:    printf ("  [%d] nZr %d nZr_tmp %d nTot %d nTot_tmp %d rel %f test %d\n\n", 
./psymbfact.c:2603:      prval_cursn = prval_curvtx;
./psymbfact.c:2634:	  if (prval_cursn > prval_curvtx || prval_cursn <= vtx)
./psymbfact.c:2635:	    prval_cursn = prval_curvtx;
./psymbfact.c:2667:		  if (prval_cursn > vtx_elt && vtx_elt != vtx)
./psymbfact.c:2668:		    prval_cursn = vtx_elt;
./psymbfact.c:2678:		  if (prval_cursn > vtx_elt && vtx_elt != vtx)
./psymbfact.c:2679:		    prval_cursn = vtx_elt;
./psymbfact.c:2702:      /* build the pruned structure */
./psymbfact.c:2719:	printf ("End sn %d szsn %d\n", nsuper_loc, szsn);
./psymbfact.c:2720:	printf ("BLD pr vtx %d snrep %d prval %d szLp %d\n",
./psymbfact.c:2721:		vtx, snrep, prval_cursn, szLp);
./psymbfact.c:2724:	update_prGraph (iam, n, fstVtx_blk, lstVtx_blk,
./psymbfact.c:2725:			snrep_lid, pr_offset, prval_cursn,
./psymbfact.c:2728:	update_prGraph (iam, n, fstVtx_blk, lstVtx_blk,
./psymbfact.c:2729:			snrep_lid, pr_offset, prval_cursn,
./psymbfact.c:2734:	printf ("Adr lsub %p usub %p lsub %p pos %d usub %p pos %d\n", 
./psymbfact.c:2737:	PrintInt10 ("Lsn", xlsub_snp1 - xlsub[snrep_lid],
./psymbfact.c:2739:	PrintInt10 ("Usn", xusub_snp1 - xusub[snrep_lid],
./psymbfact.c:2743:	if (prval_cursn >= lstVtx_blk) {
./psymbfact.c:2750:	  if (prval_cursn >= lstVtx) {
./psymbfact.c:2757:	    while (prval_cursn >= lstVtx_tmp && szSep_tmp != 1) {
./psymbfact.c:2772:	prval_cursn = prval_curvtx;
./psymbfact.c:2813: int   iam,        /* Input - my processor number */  
./psymbfact.c:2837:  int_t lstVtx_lid, maxNvtcsPProc; 
./psymbfact.c:2850:    maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:2866: * <pre>
./psymbfact.c:2872: * </pre>
./psymbfact.c:2878: int   iam,     /* Input - my processor number */
./psymbfact.c:2898:  int_t nelts, nelts_fill_l, nelts_fill_u, nelts_cnts, maxNvtcsPProc, *globToLoc;
./psymbfact.c:2906:  maxNvtcsPProc  = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:3075: int   iam,       /* process number */
./psymbfact.c:3081: int_t vtx_upd_pr,    /* ind in pruned structure of upd vertex which 
./psymbfact.c:3083: int_t lstVtx_upd_pr, /* ind in pruned structure of lst vtx to update */
./psymbfact.c:3100:  int_t *xusubPr, *usubPr, *xlsub, *lsub, *xusub, *usub;
./psymbfact.c:3101:  int_t markl, *globToLoc, maxNvtcsPProc;
./psymbfact.c:3104:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:3107:  xusubPr = Llu_symbfact->xlsubPr; usubPr  = Llu_symbfact->lsubPr;
./psymbfact.c:3127:  vtx_lid += vtx_upd_pr;
./psymbfact.c:3129:  for (i = vtx_upd_pr; i < lstVtx_upd_pr; i++, vtx_lid ++) { 
./psymbfact.c:3131:    if (xusubPr[i] != xusubPr[i+1]) {
./psymbfact.c:3132:      j = xusubPr[i]; 
./psymbfact.c:3133:      vtx = usubPr[j];
./psymbfact.c:3141:      for (j = xusubPr[i] + 1; j < xusubPr[i+1]; j++) {
./psymbfact.c:3142:	vtx_elt = usubPr[j];
./psymbfact.c:3219: int   iam,       /* process number */
./psymbfact.c:3241:  int_t i, j, k, prVal, nelts, ind, nextl, ii, mpnelts, mem_error;
./psymbfact.c:3244:  int_t *xusubPr, *usubPr, *xlsub, *lsub, *xusub, *usub;
./psymbfact.c:3245:  int_t fstVtx_upd, lstVtx_upd, maxNvtcsPProc, *globToLoc;
./psymbfact.c:3252:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:3257:  xusubPr = Llu_symbfact->xlsubPr; usubPr  = Llu_symbfact->lsubPr;
./psymbfact.c:3269:  /* count number of elements in transpose representation of usub_rcvd */
./psymbfact.c:3296:    prVal = usub_rcvd[i];
./psymbfact.c:3299:      if (vtx_elt > prVal)
./psymbfact.c:3322:    xusubPr[i] = ind;
./psymbfact.c:3324:    marker[i] = xusubPr[i];
./psymbfact.c:3326:  xusubPr[i] = ind;
./psymbfact.c:3333:  /* test if enough memory in usubPr array */
./psymbfact.c:3334:  if (ind > Llu_symbfact->szLsubPr) {
./psymbfact.c:3336:	psymbfact_prLUXpand (iam, ind, LSUB_PR, Llu_symbfact, PS))
./psymbfact.c:3338:    usubPr  = Llu_symbfact->lsubPr;
./psymbfact.c:3368:    prVal = usub_rcvd[i];
./psymbfact.c:3371:      if (vtx_elt > prVal)
./psymbfact.c:3377:	    /* add vtx_elt to the pruned structure */
./psymbfact.c:3378:	    if (marker[vtx_elt_lid] == xusubPr[vtx_elt_lid]) {
./psymbfact.c:3379:	      usubPr[marker[vtx_elt_lid]] = vtx_elt;
./psymbfact.c:3382:	    usubPr[marker[vtx_elt_lid]] = vtx;
./psymbfact.c:3424:  /* use the pruned structure to update symbolic factorization */
./psymbfact.c:3428:    if (xusubPr[i] != xusubPr[i+1]) {
./psymbfact.c:3429:      j = xusubPr[i]; 
./psymbfact.c:3430:      vtx = usubPr[j];
./psymbfact.c:3439:      for (j = xusubPr[i] + 1; j < xusubPr[i+1]; j++) {
./psymbfact.c:3440:	vtx_elt = usubPr[j];
./psymbfact.c:3511: int   iam,      /* my processor number */
./psymbfact.c:3532:  int_t *globToLoc, maxNvtcsPProc;
./psymbfact.c:3539:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:3658: int   iam,        /* Input - my processor number */
./psymbfact.c:3686:  int_t *globToLoc, maxNvtcsPProc, lvl;
./psymbfact.c:3687:  int_t prval, kmin, kmax, maxElt, ktemp, prpos;
./psymbfact.c:3696:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:3768:#if ( PRNTlevel>=1 )
./psymbfact.c:3851:  prval = n; 	    
./psymbfact.c:3914:	    if (computeU && vtx_elt < prval 
./psymbfact.c:3916:	      prval = vtx_elt;
./psymbfact.c:3944:      /* if iam the last processor owning a block of this level */
./psymbfact.c:3946:      /* prune the structure */
./psymbfact.c:3961:	if (prval != n) {
./psymbfact.c:3962:	  maxElt = prval;
./psymbfact.c:3965:	    if (sub[kmax] > prval) 
./psymbfact.c:3967:	    else if (sub[kmin] <= prval) {
./psymbfact.c:3979:	    if (sub[kmin-1] == prval)
./psymbfact.c:3980:	      prpos = kmin - 1;
./psymbfact.c:3989:	      prpos = kmin;
./psymbfact.c:3996:	sub[prpos] = ktemp;
./psymbfact.c:4000:      prval = Llu_symbfact->lsub[Llu_symbfact->xlsub[snlid]];
./psymbfact.c:4001:      if (prval >= lstVtx) {
./psymbfact.c:4003:	while (prval >= lstVtx && szSep != 1) {
./psymbfact.c:4035:<pre>
./psymbfact.c:4036:   All processors affected to current node must call this routine
./psymbfact.c:4039:   MPI_allreduce among all processors affected to curent node
./psymbfact.c:4040:</pre>
./psymbfact.c:4046: int   rcvd_dnsSep, /* =1 if processor received info that the separator
./psymbfact.c:4050: int   iam,         /* Input - my processor number */
./psymbfact.c:4075:  int   nprocsLvl, p, prvP, tag;
./psymbfact.c:4080:  int_t *globToLoc, maxNvtcsPProc;
./psymbfact.c:4084:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:4089:  nprocsLvl = lstP - fstP;
./psymbfact.c:4101:  /* first exchange msgs with all processors affected to current node */
./psymbfact.c:4109:	MPI_Send (&(rcv_intraLvl[fstP]), nprocsLvl, mpi_int_t, p,
./psymbfact.c:4111:#if ( PRNTlevel>=1 )
./psymbfact.c:4123:      prvP = OWNER( globToLoc[fstVtx_blk - 1]);
./psymbfact.c:4124:      MPI_Recv (&(rcv_intraLvl[fstP]), nprocsLvl, mpi_int_t, prvP,
./psymbfact.c:4126:#if ( PRNTlevel>=1 )
./psymbfact.c:4156: int   iam,      /* Input - my processor number */  
./psymbfact.c:4159: int   fstP,     /* Input - first processor assigned to current node */
./psymbfact.c:4160: int   lstP,     /* Input - last processor assigned to current node */
./psymbfact.c:4182:  int   nprocsLvl, rcvdP, p, filledSep_lvl;
./psymbfact.c:4193:  int_t fstVtx_blk, lstVtx_blk, i, j, vtx, prElt_L, prElt_U, 
./psymbfact.c:4194:    snd_indBlk, prElt_ind;
./psymbfact.c:4195:  int_t fstVtxLvl_loc, nvtcsLvl_loc, maxNvtcsPProc, *globToLoc, 
./psymbfact.c:4197:  int  ind1, nprocsToRcv, nprocsToSnd, ind2, ind_l, ind_u, ij, ik;
./psymbfact.c:4205:  maxNvtcsPProc   = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:4207:  nprocsLvl       = lstP - fstP;
./psymbfact.c:4241:#if ( PRNTlevel>=1 )
./psymbfact.c:4242:  PS->no_msgsCol += (float) (2 * (int_t) LOG2( nprocsLvl ));
./psymbfact.c:4281:  /* snd_interLvl : to which processors the data need to be send 
./psymbfact.c:4283:   * rcv_interLvl : from which processors iam receives update data  */
./psymbfact.c:4291:#if ( PRNTlevel>=1 )
./psymbfact.c:4292:    PS->no_msgsCol += (float) (2 * (int_t) LOG2( nprocsLvl ));
./psymbfact.c:4301:   * obtain from which processors iam receives update information */
./psymbfact.c:4324:	  prElt_L = lsub[k];
./psymbfact.c:4326:	  prElt_U = usub[j];
./psymbfact.c:4327:	  if (prElt_L >= fstVtx || prElt_U >= fstVtx) {
./psymbfact.c:4328:	    if (prElt_L >= fstVtx)
./psymbfact.c:4329:	      while (lsub[k] <= prElt_L && k < xlsub[vtx_lid + 1]) {
./psymbfact.c:4334:		    /* vtx will be send to another processor */
./psymbfact.c:4343:	    if (prElt_U >= fstVtx)
./psymbfact.c:4344:	      while (usub[j] <= prElt_U && j < xusub[vtx_lid + 1]) {
./psymbfact.c:4349:		    /* vtx will be send to another processor */
./psymbfact.c:4359:	      /* L(:, vtx) and U(vtx, :) will be send to processors */
./psymbfact.c:4388:  nprocsToSnd = 0;
./psymbfact.c:4393:      nprocsToSnd ++;
./psymbfact.c:4399:#if ( PRNTlevel>=1 )
./psymbfact.c:4400:  PS->no_msgsCol += (float) (2 * (int_t) LOG2( nprocsLvl ));
./psymbfact.c:4401:  PS->sz_msgsCol += 2 * nprocsLvl;
./psymbfact.c:4402:  if (PS->maxsz_msgCol < 2 * nprocsLvl) 
./psymbfact.c:4403:    PS->maxsz_msgCol = 2 * nprocsLvl;      
./psymbfact.c:4407:  nprocsToRcv = 0;
./psymbfact.c:4416:      nprocsToRcv ++;
./psymbfact.c:4431:  if (nprocsToSnd)
./psymbfact.c:4433:	   SUPERLU_MALLOC(2 * nprocsToSnd * sizeof(MPI_Request))))
./psymbfact.c:4435:  if (nprocsToRcv)
./psymbfact.c:4437:	   SUPERLU_MALLOC(2 * nprocsToRcv * sizeof(MPI_Request))))
./psymbfact.c:4439:  if (nprocsToRcv || nprocsToSnd)
./psymbfact.c:4454:#if ( PRNTlevel>=1 )
./psymbfact.c:4501:#if ( PRNTlevel>=1 )
./psymbfact.c:4512:  for (i = 0; i < nprocsToRcv; i++) {
./psymbfact.c:4513:    MPI_Waitany (2*nprocsToRcv, request_rcv, &ind1, status);
./psymbfact.c:4553:  if (nprocsToSnd)
./psymbfact.c:4554:    MPI_Waitall (2*nprocsToSnd, request_snd, status);
./psymbfact.c:4567: int   iam,          /* Input -my processor number */
./psymbfact.c:4568: int   nprocs,       /* Input -number of procs for the symbolic fact. */
./psymbfact.c:4576:  i = 2 * nprocs - 2;
./psymbfact.c:4584:    npNode = nprocs / szSep; 
./psymbfact.c:4603: int   iam,          /* Input -my processor number */
./psymbfact.c:4604: int   nprocs,       /* Input -number of procs for the symbolic factorization */
./psymbfact.c:4609:  int szSep, i, j, jj, k, *pranks;
./psymbfact.c:4612:  for (i=0; i < 2*nprocs; i++)
./psymbfact.c:4615:  /* Make a list of the processes in the new communicator. */
./psymbfact.c:4616:  pranks = (int *) SUPERLU_MALLOC( nprocs * sizeof(int) );
./psymbfact.c:4618:  i = 2 * nprocs - 2;
./psymbfact.c:4625:    npNode = nprocs / szSep; 
./psymbfact.c:4643:  SUPERLU_FREE (pranks);
./psymbfact.c:4650: int   iam,      /* Input - my processor number */
./psymbfact.c:4657: int   fstP,     /* Input - first processor assigned to current node */
./psymbfact.c:4658: int   lstP,     /* Input - last processor assigned to current node */
./psymbfact.c:4678:  int nprocsLvl, p, prvP, rcvP;
./psymbfact.c:4679:  int toSend, rcvd_prvP, index_req[2];
./psymbfact.c:4682:  int_t nvtcs_blk, xusub_end, xlsub_end, prv_fstVtx_blk;
./psymbfact.c:4689:  int_t fstVtx_blk_loc, fstBlk, vtx_lid, prElt, nelts, j, nvtcs_toUpd;
./psymbfact.c:4690:  int_t snd_LinterLvlSz, fstVtx_blk_loc_lid, prElt_ind, maxNmsgsToRcv;
./psymbfact.c:4692:  int_t *globToLoc, maxNvtcsPProc, nblk_loc, upd_myD, r, fstVtx_blkCyc;
./psymbfact.c:4693:  int_t k, prElt_L, prElt_U, vtx_elt, fstVtx_toUpd;
./psymbfact.c:4703:  /* max number of msgs this processor can receive during 
./psymbfact.c:4708:  maxNvtcsPProc  = Pslu_freeable->maxNvtcsPProc;
./psymbfact.c:4710:  nprocsLvl       = lstP - fstP;
./psymbfact.c:4747:    prv_fstVtx_blk = fstVtx_blk;
./psymbfact.c:4756:    VInfo->maxNeltsVtx -= fstVtx_blk - prv_fstVtx_blk;
./psymbfact.c:4763:      prvP           = OWNER( globToLoc[fstVtx_blk - 1] );
./psymbfact.c:4764:      rcvd_prvP      = FALSE;
./psymbfact.c:4769:      MPI_Irecv (&(rcv_intraLvl[fstP]), nprocsLvl, mpi_int_t, prvP,
./psymbfact.c:4772:      while (!rcvd_prvP || nmsgsRcvd < nmsgsTRcv) {
./psymbfact.c:4783:#if ( PRNTlevel>=1 )
./psymbfact.c:4786:	  rcvd_prvP = TRUE;
./psymbfact.c:4789:	     previous processors ... */
./psymbfact.c:4831:#if ( PRNTlevel>=1 )
./psymbfact.c:4887:	      ((lstVtx - lstVtx_blk > VInfo->maxSzBlk * nprocsLvl && nblk_loc > 0) ||
./psymbfact.c:4888:	       (lstVtx - fstVtx_blkCyc > VInfo->maxSzBlk * nprocsLvl && nblk_loc == 0))))
./psymbfact.c:4906:	  /* send blk to next procs and update the rest of my own blocks */
./psymbfact.c:4925:	  /* determine processors to which send this block
./psymbfact.c:4948:	    prElt_L = lsub[k];
./psymbfact.c:4950:	    prElt_U = usub[j];
./psymbfact.c:4952:	    if (prElt_L >= lstVtx_blk || prElt_U >= lstVtx_blk) {
./psymbfact.c:4961:	      if (prElt_L >= lstVtx_blk) {
./psymbfact.c:4962:		while (lsub[k] <= prElt_L && k < xlsub_end) {
./psymbfact.c:4967:		      /* vtx will be send to another processor */
./psymbfact.c:4978:	      if (prElt_U >= lstVtx_blk) {
./psymbfact.c:4979:		while (usub[j] <= prElt_U && j < xusub_end) {
./psymbfact.c:4984:		      /* vtx will be send to another processor */
./psymbfact.c:4996:		/* L(:, vtx) and U(vtx, :) will be send to processors */
./psymbfact.c:5029:	    MPI_Isend (&(rcv_intraLvl[fstP]), nprocsLvl, mpi_int_t, p,
./psymbfact.c:5031:#if ( PRNTlevel>=1 )
./psymbfact.c:5048:#if ( PRNTlevel>=1 )
./psymbfact.c:5110: int   iam,    /* Input - my processor number */
./psymbfact.c:5111: int   nprocs, /* Input - number of processors for the symbolic factorization */
./psymbfact.c:5117:  /* free memory corresponding to prune structure */
./psymbfact.c:5118:  if (Llu_symbfact->szLsubPr != 0)
./psymbfact.c:5119:    SUPERLU_FREE( Llu_symbfact->lsubPr );
./psymbfact.c:5120:  if (Llu_symbfact->szUsubPr != 0)
./psymbfact.c:5121:    SUPERLU_FREE( Llu_symbfact->usubPr );
./psymbfact.c:5122:  if (Llu_symbfact->xlsubPr != NULL)
./psymbfact.c:5123:    SUPERLU_FREE( Llu_symbfact->xlsubPr );
./psymbfact.c:5124:  if (Llu_symbfact->xusubPr != NULL)
./psymbfact.c:5125:    SUPERLU_FREE( Llu_symbfact->xusubPr );
./psymbfact.c:5157: int iam,  /* Input - my processor number */
./psymbfact.c:5178:  /* memory for xlsubPr, xusubPr */
./psymbfact.c:5188:  lu_mem += (float) PS->maxSzLPr * lword;
./psymbfact.c:5189:  lu_mem += (float) PS->maxSzUPr * lword;
./psymbfact.h:4:approvals from U.S. Dept. of Energy) 
./psymbfact.h:14: * <pre>
./psymbfact.h:18: * </pre>
./psymbfact.h:31: * <pre>
./psymbfact.h:37: * (xlsub,lsub): lsub[*] contains the compressed subscript of
./psymbfact.h:43: * (xusub,usub): lsub[*] contains the compressed subscript of
./psymbfact.h:50: *      supernode and column, information local to each processor:
./psymbfact.h:54: * </pre>
./psymbfact.h:59:  int_t     *lsub;   /* compressed L subscripts, stored by columns */
./psymbfact.h:63:  int_t     *usub;   /* compressed U subscripts, stored by rows */
./psymbfact.h:71:  int_t     maxNvtcsPProc;   /* max number of vertices on the processors */
./psymbfact.h:78: * <pre>
./psymbfact.h:81: * (xlsub,lsub): lsub[*] contains the compressed subscript of L, as described above
./psymbfact.h:83: * (xusub,usub): usub[*] contains the compressed subscript of U, as described above
./psymbfact.h:86: * (xlsubPr,lsubPr): contains the pruned structure of the graph of
./psymbfact.h:88: *	xlsubPr[j] points to the starting location of the j-th 
./psymbfact.h:92: *      In each independent domain formed by x vertices, xlsubPr is of size x.
./psymbfact.h:95: *      maxNvtcsNds_loc, xlsubPr is of size maxNvtcsNds_loc. 
./psymbfact.h:98: * (xusubPr,usubPr): contains the pruned structure of the graph of
./psymbfact.h:99: *      U, stored by columns as a linked list.  Similar to (xlsubPr,lsubPr),
./psymbfact.h:103: * </pre>
./psymbfact.h:107:  int_t     *xlsubPr;  /* pointer to pruned structure of L */
./psymbfact.h:108:  int_t     *lsubPr;   /* pruned structure of L */
./psymbfact.h:109:  int_t     szLsubPr;  /* size of lsubPr array */
./psymbfact.h:110:  int_t     indLsubPr; /* current index in lsubPr */
./psymbfact.h:111:  int_t     *xusubPr;  /* pointer to pruned structure of U */
./psymbfact.h:112:  int_t     *usubPr;   /* pruned structure of U */
./psymbfact.h:113:  int_t     szUsubPr;  /* size of usubPr array */
./psymbfact.h:114:  int_t     indUsubPr; /* current index in usubPr */
./psymbfact.h:132:  LU_space_t MemModel; /* 0 - system malloc'd; 1 - user provided */
./psymbfact.h:134:  int_t  no_expand_pr; /* Number of memory expansions of the pruned structures */
./psymbfact.h:136:			  overestimation approach */
./psymbfact.h:143:			     processor.  The maximum is computed among all the nodes 
./psymbfact.h:144:			     of the sep arator tree and among all the processors */
./psymbfact.h:152:  int_t  nvtcs_loc;       /* Number of local vertices distributed on a processor */
./psymbfact.h:174:  int_t  *rcv_interLvl; /* from which processors iam receives data */
./psymbfact.h:175:  int_t  *snd_interLvl; /* to which processors iam sends data */
./psymbfact.h:181:  int_t  *snd_intraLvl; /* to which processors iam sends data */
./psymbfact.h:184:  int_t  *rcv_intraLvl; /* from which processors iam receives data */
./psymbfact.h:203:  /* no of dense current separators per proc */
./psymbfact.h:205:  /* no of dense separators up per proc */
./psymbfact.h:237:  int_t  maxSzLPr; /* maximum size of pruned L */
./psymbfact.h:238:  int_t  maxSzUPr; /* maximum size of pruned U */
./psymbfact.h:250:#define OWNER(x)      ((x) / maxNvtcsPProc)
./psymbfact.h:251:#define LOCAL_IND(x)  ((x) % maxNvtcsPProc)
./psymbfact.h:267: * Index of diagonal element, no of elements preceding each column/row
./psymbfact.h:268: * of L/U send to another processor 
./psymbfact.h:281:#define USUB_PR 0
./psymbfact.h:282:#define LSUB_PR 1
./psymbfact_util.c:4:approvals from U.S. Dept. of Energy) 
./psymbfact_util.c:14: * <pre>
./psymbfact_util.c:21: * </pre>
./psymbfact_util.c:41: int_t prev_len,    /* length used from previous call */
./psymbfact_util.c:43: int_t *prev_mem,    /* pointer to the previous memory */
./psymbfact_util.c:60:  new_len = alpha * prev_len;
./psymbfact_util.c:69:      copy_mem_int(len_tcopy_fbeg, prev_mem, new_mem);
./psymbfact_util.c:71:      copy_mem_int(len_tcopy_fend, &(prev_mem[prev_len-len_tcopy_fend]), 
./psymbfact_util.c:82: * <pre>
./psymbfact_util.c:86: * </pre>
./psymbfact_util.c:99: int_t free_prev_mem, /* =1 if prev_mem has to be freed */
./psymbfact_util.c:106:  int_t  *new_mem, *prev_mem, *xsub;
./psymbfact_util.c:110:  int_t exp, prev_xsub_nextLvl, vtxXp_lid;
./psymbfact_util.c:111:  int_t *globToLoc, maxNvtcsPProc, nvtcs_loc;
./psymbfact_util.c:113:  int_t len_tcopy_fbeg, len_tcopy_fend, new_len, prev_len;  
./psymbfact_util.c:118:  maxNvtcsPProc  = Pslu_freeable->maxNvtcsPProc;
./psymbfact_util.c:128:    prev_mem = Llu_symbfact->lsub;
./psymbfact_util.c:129:    prev_len = Llu_symbfact->szLsub;
./psymbfact_util.c:132:      prev_xsub_nextLvl = xsub[vtxXp_lid+1];
./psymbfact_util.c:134:      prev_xsub_nextLvl = VInfo->xlsub_nextLvl;
./psymbfact_util.c:136:    prev_mem = Llu_symbfact->usub;
./psymbfact_util.c:137:    prev_len = Llu_symbfact->szUsub;
./psymbfact_util.c:140:      prev_xsub_nextLvl = xsub[vtxXp_lid+1];
./psymbfact_util.c:142:      prev_xsub_nextLvl = VInfo->xusub_nextLvl;
./psymbfact_util.c:145:  len_tcopy_fend = prev_len - prev_xsub_nextLvl;  
./psymbfact_util.c:153:  printf ("Pe[" IFMT "] LUXpand mem_t " IFMT " vtxXp " IFMT "\n",
./psymbfact_util.c:156:  new_mem = expand (prev_len, min_new_len, prev_mem,
./psymbfact_util.c:159:    fprintf(stderr, "Pe[" IFMT "] Can't exp MemType " IFMT ": prv_len " IFMT
./psymbfact_util.c:161:	   iam, mem_type, prev_len, min_new_len, new_len);
./psymbfact_util.c:174:    i = xsub_nextLvl + xsub[vtx_lid] - prev_xsub_nextLvl;
./psymbfact_util.c:183:  if (free_prev_mem) {
./psymbfact_util.c:184:    SUPERLU_FREE (prev_mem);
./psymbfact_util.c:204: * <pre>
./psymbfact_util.c:208: * </pre>
./psymbfact_util.c:222: int_t free_prev_mem, /* =1 if free prev_mem memory */
./psymbfact_util.c:230:  int_t  *new_mem, *prev_mem, *xsub, sz_prev_mem;
./psymbfact_util.c:233:  int_t exp, prev_xsub_nextLvl, vtxXp_lid, xsub_nextLvl;
./psymbfact_util.c:234:  int_t *globToLoc, nvtcs_loc, maxNvtcsPProc;
./psymbfact_util.c:237:  int_t fstVtxLvl_loc_lid, prev_len, next;
./psymbfact_util.c:243:  maxNvtcsPProc  = Pslu_freeable->maxNvtcsPProc;
./psymbfact_util.c:256:    prev_mem = Llu_symbfact->lsub;
./psymbfact_util.c:257:    prev_xsub_nextLvl = VInfo->xlsub_nextLvl;
./psymbfact_util.c:258:    sz_prev_mem = Llu_symbfact->szLsub;
./psymbfact_util.c:261:    prev_mem = Llu_symbfact->usub;
./psymbfact_util.c:262:    prev_xsub_nextLvl = VInfo->xusub_nextLvl;
./psymbfact_util.c:263:    sz_prev_mem = Llu_symbfact->szUsub;
./psymbfact_util.c:266:  printf ("Pe[%d] Expand LU mem_t %d vtxXp %d\n", 
./psymbfact_util.c:281:    prev_len = xsub[fstVtxLvl_loc_lid];
./psymbfact_util.c:282:    next = prev_len;
./psymbfact_util.c:289:    prev_len = xsub[vtxXp_lid];
./psymbfact_util.c:292:  if (prev_len + len_texp >= prev_xsub_nextLvl) {
./psymbfact_util.c:294:    min_new_len = prev_len + len_texp + (sz_prev_mem - prev_xsub_nextLvl);
./psymbfact_util.c:306:    new_mem = prev_mem;
./psymbfact_util.c:308:  if (mem_type == LSUB && PS->estimLSz < (prev_len + len_texp))
./psymbfact_util.c:309:    PS->estimLSz = prev_len + len_texp;
./psymbfact_util.c:310:  if (mem_type == USUB && PS->estimUSz < (prev_len + len_texp))
./psymbfact_util.c:311:    PS->estimUSz = prev_len;
./psymbfact_util.c:320:      while (j < xsub[vtx_lid+1] && prev_mem[j] != EMPTY) {
./psymbfact_util.c:327:	new_mem[k] = prev_mem[j]; k--; j--;
./psymbfact_util.c:350:      while (j < xsub[vtx_lid+1] && prev_mem[j] != EMPTY) {
./psymbfact_util.c:357:	new_mem[k] = prev_mem[j]; k--; j--;
./psymbfact_util.c:366:  if (free_prev_mem && new_mem != prev_mem)
./psymbfact_util.c:367:    SUPERLU_FREE (prev_mem);
./psymbfact_util.c:375: * <pre>
./psymbfact_util.c:379: * </pre>
./psymbfact_util.c:397:  int_t  *new_mem, *prev_mem, *xsub, mem_error, sz_prev_mem;
./psymbfact_util.c:400:  int_t exp, prev_xsub_nextLvl, vtxXp_lid, xsub_nextLvl;
./psymbfact_util.c:401:  int_t *globToLoc, nvtcs_loc, maxNvtcsPProc;
./psymbfact_util.c:404:  int_t fstVtxLvl_loc_lid, prev_len, min_new_len;
./psymbfact_util.c:407:  printf ("Pe[%d] Expand LU_RL mem_t %d vtxXp %d\n", 
./psymbfact_util.c:412:  maxNvtcsPProc  = Pslu_freeable->maxNvtcsPProc;
./psymbfact_util.c:423:    prev_mem = Llu_symbfact->lsub;
./psymbfact_util.c:424:    prev_xsub_nextLvl = VInfo->xlsub_nextLvl;
./psymbfact_util.c:425:    sz_prev_mem = Llu_symbfact->szLsub;
./psymbfact_util.c:428:    prev_mem = Llu_symbfact->usub;
./psymbfact_util.c:429:    prev_xsub_nextLvl = VInfo->xusub_nextLvl;
./psymbfact_util.c:430:    sz_prev_mem = Llu_symbfact->szUsub;
./psymbfact_util.c:435:  prev_len = xsub[vtxXp_lid];
./psymbfact_util.c:437:  if (prev_len + len_texp >= prev_xsub_nextLvl) {
./psymbfact_util.c:439:    min_new_len = prev_len + len_texp + (sz_prev_mem - prev_xsub_nextLvl);
./psymbfact_util.c:451:    new_mem = prev_mem;
./psymbfact_util.c:454:  if (mem_type == LSUB && PS->estimLSz < (prev_len + len_texp))
./psymbfact_util.c:455:    PS->estimLSz = prev_len + len_texp;
./psymbfact_util.c:456:  if (mem_type == USUB && PS->estimUSz < (prev_len + len_texp))
./psymbfact_util.c:457:    PS->estimUSz = prev_len;
./psymbfact_util.c:464:    while (j < xsub[vtx_lid+1] && prev_mem[j] != EMPTY) {
./psymbfact_util.c:472:      new_mem[k] = prev_mem[j];
./psymbfact_util.c:484:  if (new_mem != prev_mem)
./psymbfact_util.c:485:    SUPERLU_FREE (prev_mem);
./psymbfact_util.c:493: * <pre>
./psymbfact_util.c:494: * Expand the data structures for L and U pruned during the factorization.
./psymbfact_util.c:497: * </pre>
./psymbfact_util.c:500:int_t psymbfact_prLUXpand
./psymbfact_util.c:510: Llu_symbfact_t *Llu_symbfact, /* modified L/U pruned structures */
./psymbfact_util.c:514:  int_t *prev_mem, *new_mem;
./psymbfact_util.c:515:  int_t prev_len, new_len, len_tcopy_fbeg;
./psymbfact_util.c:517:  if ( mem_type == LSUB_PR ) {
./psymbfact_util.c:518:    prev_len = Llu_symbfact->szLsubPr;
./psymbfact_util.c:519:    prev_mem = Llu_symbfact->lsubPr;
./psymbfact_util.c:520:    len_tcopy_fbeg = Llu_symbfact->indLsubPr;
./psymbfact_util.c:521:  } else if ( mem_type == USUB_PR ) {
./psymbfact_util.c:522:    prev_len = Llu_symbfact->szUsubPr;
./psymbfact_util.c:523:    prev_mem = Llu_symbfact->usubPr;
./psymbfact_util.c:524:    len_tcopy_fbeg = Llu_symbfact->indUsubPr;
./psymbfact_util.c:528:  printf ("Pe[%d] Expand prmem prev_len %d min_new_l %d len_tfbeg %d\n", 
./psymbfact_util.c:529:	  iam, prev_len, min_new_len, len_tcopy_fbeg);
./psymbfact_util.c:532:  new_mem = expand (prev_len, min_new_len, prev_mem, 
./psymbfact_util.c:536:    fprintf(stderr, "Can't expand MemType %d: \n", mem_type);
./psymbfact_util.c:540:  Llu_symbfact->no_expand_pr ++;
./psymbfact_util.c:541:  if ( mem_type == LSUB_PR ) {
./psymbfact_util.c:542:    Llu_symbfact->lsubPr  = new_mem;
./psymbfact_util.c:543:    Llu_symbfact->szLsubPr = new_len;
./psymbfact_util.c:544:  } else if ( mem_type == USUB_PR ) {
./psymbfact_util.c:545:    Llu_symbfact->usubPr  = new_mem;
./psymbfact_util.c:546:    Llu_symbfact->szUsubPr = new_len;
./psymbfact_util.c:549:  SUPERLU_FREE (prev_mem);
./pxerr_dist.c:4:approvals from U.S. Dept. of Energy) 
./pxerr_dist.c:14: * <pre>
./pxerr_dist.c:21: * </pre>
./pxerr_dist.c:29:    printf("{" IFMT "," IFMT "}: On entry to %6s, parameter number " IFMT " had an illegal value\n",
./pzGetDiagU.c:4:approvals from U.S. Dept. of Energy) 
./pzGetDiagU.c:11:/*! @file p@(pre)GetDiagU.c
./pzGetDiagU.c:14: * <pre>
./pzGetDiagU.c:18: * Created:  April 16, 2002
./pzGetDiagU.c:20: * </pre>
./pzGetDiagU.c:28: * <pre>
./pzGetDiagU.c:45: *          The 2D process mesh. It contains the MPI communicator, the number
./pzGetDiagU.c:46: *          of process rows (NPROW), the number of process columns (NPCOL),
./pzGetDiagU.c:47: *          and my process rank. It is an input argument to all the
./pzGetDiagU.c:52: *          On exit, it is available on all processes.
./pzGetDiagU.c:59: * data structures, and are on the diagonal processes of the
./pzGetDiagU.c:60: * 2D process grid.
./pzGetDiagU.c:63: * </pre>
./pzGetDiagU.c:71:    int nsupr; /* number of rows in the block L(:,k) (LDA) */
./pzGetDiagU.c:73:    int_t num_diag_procs, *diag_procs, *diag_len;
./pzGetDiagU.c:82:    get_diag_procs(n, Glu_persist, grid, &num_diag_procs,
./pzGetDiagU.c:83:		   &diag_procs, &diag_len);
./pzGetDiagU.c:85:    for (j = 1; j < num_diag_procs; ++j) jj = SUPERLU_MAX( jj, diag_len[j] );
./pzGetDiagU.c:88:    for (p = 0; p < num_diag_procs; ++p) {
./pzGetDiagU.c:89:	pkk = diag_procs[p];
./pzGetDiagU.c:93:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pzGetDiagU.c:96:		nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pzGetDiagU.c:99:		    zwork[lwork+i] = lusup[i*(nsupr+1)];
./pzGetDiagU.c:109:	for (k = p; k < nsupers; k += num_diag_procs) {
./pzGetDiagU.c:117:    SUPERLU_FREE(diag_procs);
./pzdistribute.c:4:approvals from U.S. Dept. of Energy) 
./pzdistribute.c:13: * \brief Re-distribute A on the 2D process mesh.
./pzdistribute.c:14: * <pre>
./pzdistribute.c:18: * </pre>
./pzdistribute.c:29: * <pre>
./pzdistribute.c:32: *   Re-distribute A on the 2D process mesh.
./pzdistribute.c:50: *        The 2D process mesh.
./pzdistribute.c:60: * </pre>
./pzdistribute.c:81:    int    iam, it, p, procs, iam_g;
./pzdistribute.c:95:    procs = grid->nprow * grid->npcol;
./pzdistribute.c:100:    nnzToRecv = intCalloc_dist(2*procs);
./pzdistribute.c:101:    nnzToSend = nnzToRecv + procs;	
./pzdistribute.c:104:       COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
./pzdistribute.c:110:  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pzdistribute.c:114:	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pzdistribute.c:126:    for (p = 0; p < procs; ++p) {
./pzdistribute.c:136:    k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
./pzdistribute.c:148:    if ( procs > 1 ) {
./pzdistribute.c:150:	     SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) )
./pzdistribute.c:152:      if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) )
./pzdistribute.c:154:      if ( !(aij_send = (doublecomplex **)SUPERLU_MALLOC(procs*sizeof(doublecomplex*))) )
./pzdistribute.c:162:      if ( !(ptr_to_send = intCalloc_dist(procs)) )
./pzdistribute.c:171:      for (i = 0, j = 0, p = 0; p < procs; ++p) {
./pzdistribute.c:179:    } /* if procs > 1 */
./pzdistribute.c:192:  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pzdistribute.c:196:	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pzdistribute.c:218:    for (p = 0; p < procs; ++p) {
./pzdistribute.c:225:	               p, iam+procs, grid->comm, &send_req[procs+p] ); 
./pzdistribute.c:229:    for (p = 0; p < procs; ++p) {
./pzdistribute.c:234:            MPI_Recv( dtemp, it, SuperLU_MPI_DOUBLE_COMPLEX, p, p+procs,
./pzdistribute.c:248:    for (p = 0; p < procs; ++p) {
./pzdistribute.c:251:	    MPI_Wait( &send_req[procs+p], &status);
./pzdistribute.c:261:    if ( procs > 1 ) {
./pzdistribute.c:334: *   Distribute the matrix onto the 2D process mesh.
./pzdistribute.c:364: *        The 2D process mesh.
./pzdistribute.c:382:    int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
./pzdistribute.c:384:	int iam, jbrow, kcol, krow, mycol, myrow, pc, pr;
./pzdistribute.c:404:	doublecomplex **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pzdistribute.c:405:    int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pzdistribute.c:408:	RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
./pzdistribute.c:410:	RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
./pzdistribute.c:421:    int_t  **fsendx_plist; /* Column process list to send down Xk.   */
./pzdistribute.c:428:    int_t  **bsendx_plist; /* Column process list to send down Xk.   */
./pzdistribute.c:435:    int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./pzdistribute.c:436:    int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
./pzdistribute.c:437:    int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
./pzdistribute.c:438:    int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
./pzdistribute.c:440:    int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
./pzdistribute.c:441:    int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
./pzdistribute.c:442:    int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
./pzdistribute.c:443:    int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
./pzdistribute.c:470:#if ( PRNTlevel>=1 )
./pzdistribute.c:473:#if ( PROFlevel>=1 ) 
./pzdistribute.c:486://#if ( PRNTlevel>=1 )
./pzdistribute.c:495:#if ( PROFlevel>=1 )
./pzdistribute.c:502:#if ( PROFlevel>=1 )
./pzdistribute.c:504:    if ( !iam ) printf("--------\n"
./pzdistribute.c:510:#if ( PROFlevel>=1 )
./pzdistribute.c:513:	/* We can propagate the new values of A into the existing
./pzdistribute.c:519:	nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
./pzdistribute.c:530:#if ( PRNTlevel>=1 )
./pzdistribute.c:533:#if ( PROFlevel>=1 )
./pzdistribute.c:550:	    if ( mycol == pc ) { /* Block column jb in my process column */
./pzdistribute.c:559:			if ( myrow == PROW( gb, grid ) ) {
./pzdistribute.c:591:#if ( PROFlevel>=1 )
./pzdistribute.c:622:#if ( PROFlevel>=1 )
./pzdistribute.c:631:#if ( PROFlevel>=1 )
./pzdistribute.c:632:	if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
./pzdistribute.c:641:#if ( PROFlevel>=1 )
./pzdistribute.c:645:	 * propagate the values of A into them.
./pzdistribute.c:647:	lsub = Glu_freeable->lsub;    /* compressed L subscripts */
./pzdistribute.c:649:	usub = Glu_freeable->usub;    /* compressed U subscripts */
./pzdistribute.c:662:#if ( PRNTlevel>=1 )
./pzdistribute.c:667:	k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pzdistribute.c:694:#if ( PRNTlevel>=1 )	
./pzdistribute.c:701:	    if ( myrow == PROW( gb, grid ) ) {
./pzdistribute.c:709:#if ( PROFlevel>=1 )
./pzdistribute.c:714:	   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
./pzdistribute.c:731:		    pr = PROW( gb, grid );
./pzdistribute.c:734:			if  ( myrow == pr ) {
./pzdistribute.c:743:#if ( PRNTlevel>=1 )
./pzdistribute.c:755:	nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzdistribute.c:784:#if ( PROFlevel>=1 )
./pzdistribute.c:786:	if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
./pzdistribute.c:788:#if ( PRNTlevel>=1 )
./pzdistribute.c:812:#if ( PRNTlevel>=1 )	
./pzdistribute.c:832:		fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
./pzdistribute.c:836:		fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
./pzdistribute.c:846:	/* These lists of processes will be used for triangular solves. */
./pzdistribute.c:849:	len = k * grid->nprow;
./pzdistribute.c:853:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./pzdistribute.c:860:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./pzdistribute.c:863:#if ( PRNTlevel>=1 )
./pzdistribute.c:868:	  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
./pzdistribute.c:869:	  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
./pzdistribute.c:874:	    if ( mycol == pc ) { /* Block column jb in my process column */
./pzdistribute.c:884:			if ( myrow == PROW( gb, grid ) ) {
./pzdistribute.c:893:		jbrow = PROW( jb, grid );
./pzdistribute.c:898:#if ( PROFlevel>=1 )
./pzdistribute.c:911:			pr = PROW( gb, grid );
./pzdistribute.c:912:			if ( pr != jbrow &&
./pzdistribute.c:913:			     myrow == jbrow &&  /* diag. proc. owning jb */
./pzdistribute.c:914:			     bsendx_plist[ljb][pr] == EMPTY ) {
./pzdistribute.c:915:			    bsendx_plist[ljb][pr] = YES;
./pzdistribute.c:918:			if ( myrow == pr ) {
./pzdistribute.c:956:			} /* if myrow == pr ... */
./pzdistribute.c:961:#if ( PROFlevel>=1 )
./pzdistribute.c:977:		    pr = PROW( gb, grid ); /* Process row owning this block */
./pzdistribute.c:978:		    if ( pr != jbrow &&
./pzdistribute.c:979:			 myrow == jbrow &&  /* diag. proc. owning jb */
./pzdistribute.c:980:			 fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
./pzdistribute.c:981:			fsendx_plist[ljb][pr] = YES;
./pzdistribute.c:984:		    if ( myrow == pr ) {
./pzdistribute.c:996:#if ( PRNTlevel>=1 )
./pzdistribute.c:1043:		    /* Propagate the compressed row subscripts to Lindex[],
./pzdistribute.c:1049:			if ( myrow == PROW( gb, grid ) ) {
./pzdistribute.c:1070:				krow = PROW( jb, grid );
./pzdistribute.c:1121:			// printf("iam %5d Lindval %5d\n",iam, Lindval_loc_bc_ptr[ljb][jj]);
./pzdistribute.c:1125:			// printf("iam %5d Lindval %5d\n",iam, index[Lindval_loc_bc_ptr[ljb][jj+nrbl]]);
./pzdistribute.c:1136:#if ( PROFlevel>=1 )
./pzdistribute.c:1156:	nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
./pzdistribute.c:1215:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzdistribute.c:1228:#if ( PROFlevel>=1 )
./pzdistribute.c:1236:	if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pzdistribute.c:1238:	if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pzdistribute.c:1255:	if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pzdistribute.c:1257:	for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
./pzdistribute.c:1269:			pr = PROW( gb, grid );
./pzdistribute.c:1270:			ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
./pzdistribute.c:1281:		for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pzdistribute.c:1282:		for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pzdistribute.c:1283:		for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pzdistribute.c:1287:		for (j=0;j<grid->nprow;++j){
./pzdistribute.c:1290:			pr = PROW( gb, grid );
./pzdistribute.c:1291:			if(gb==jb)Root=pr;
./pzdistribute.c:1292:			if(myrow==pr)Iactive=1;		
./pzdistribute.c:1297:		quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
./pzdistribute.c:1300:			// printf("jb %5d damn\n",jb);
./pzdistribute.c:1305:			for (j = 0; j < grid->nprow; ++j){
./pzdistribute.c:1306:				if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pzdistribute.c:1307:					ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pzdistribute.c:1323:				// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pzdistribute.c:1327:				// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb],'z'));
./pzdistribute.c:1331:				// #if ( PRNTlevel>=1 )		
./pzdistribute.c:1334:					for (j = 0; j < grid->nprow; ++j) {
./pzdistribute.c:1341:					// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
./pzdistribute.c:1343:					// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
./pzdistribute.c:1344:					// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pzdistribute.c:1345:					// // printf("\n");
./pzdistribute.c:1360:#if ( PROFlevel>=1 )
./pzdistribute.c:1362:if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
./pzdistribute.c:1366:#if ( PROFlevel>=1 )
./pzdistribute.c:1371:	nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzdistribute.c:1379:		pr = PROW( k, grid );
./pzdistribute.c:1380:		if ( myrow == pr ) {
./pzdistribute.c:1387:	/* Every process receives the count, but it is only useful on the
./pzdistribute.c:1388:	   diagonal processes.  */
./pzdistribute.c:1393:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzdistribute.c:1451:			pr = PROW( ib, grid );
./pzdistribute.c:1452:			if ( myrow == pr ) { /* Block row ib in my process row */
./pzdistribute.c:1461:		ib = myrow+lib*grid->nprow;  /* not sure */
./pzdistribute.c:1463:			pr = PROW( ib, grid );
./pzdistribute.c:1495:						ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pzdistribute.c:1507:					// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
./pzdistribute.c:1513:					// printf("iam %5d rtree lk %5d tag %5d root %5d\n",iam,lib,ib,RdTree_IsRoot(LRtree_ptr[lib],'z'));
./pzdistribute.c:1518:					// #if ( PRNTlevel>=1 )
./pzdistribute.c:1521:					// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pzdistribute.c:1522:					// // printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pzdistribute.c:1523:					// // // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pzdistribute.c:1524:					// // printf("\n");
./pzdistribute.c:1548:#if ( PROFlevel>=1 )
./pzdistribute.c:1550:if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
./pzdistribute.c:1553:#if ( PROFlevel>=1 )
./pzdistribute.c:1562:	if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pzdistribute.c:1564:	if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pzdistribute.c:1580:	if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pzdistribute.c:1582:	for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
./pzdistribute.c:1597:				pr = PROW( gb, grid );
./pzdistribute.c:1598:				ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],gb);
./pzdistribute.c:1599:			// printf("gb:%5d jb: %5d nsupers: %5d\n",gb,jb,nsupers);
./pzdistribute.c:1601:				//if(gb==jb)Root=pr;
./pzdistribute.c:1606:		pr = PROW( jb, grid ); // take care of diagonal node stored as L
./pzdistribute.c:1607:		// printf("jb %5d current: %5d",jb,ActiveFlagAll[pr+ljb*grid->nprow]);
./pzdistribute.c:1609:		ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],jb);	
./pzdistribute.c:1619:		// if ( mycol == pc ) { /* Block column jb in my process column */
./pzdistribute.c:1621:		for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pzdistribute.c:1622:		for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pzdistribute.c:1623:		for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pzdistribute.c:1627:		for (j=0;j<grid->nprow;++j){
./pzdistribute.c:1630:			pr = PROW( gb, grid );
./pzdistribute.c:1631:			if(gb==jb)Root=pr;
./pzdistribute.c:1632:			if(myrow==pr)Iactive=1;		
./pzdistribute.c:1636:		quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
./pzdistribute.c:1637:	// printf("jb: %5d Iactive %5d\n",jb,Iactive);
./pzdistribute.c:1640:			// printf("root:%5d jb: %5d\n",Root,jb);
./pzdistribute.c:1645:			for (j = 0; j < grid->nprow; ++j){
./pzdistribute.c:1646:				if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pzdistribute.c:1647:					ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pzdistribute.c:1651:	// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
./pzdistribute.c:1663:				// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pzdistribute.c:1668:				for (j = 0; j < grid->nprow; ++j) {
./pzdistribute.c:1669:					// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
./pzdistribute.c:1675:				// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
./pzdistribute.c:1688:#if ( PROFlevel>=1 )
./pzdistribute.c:1690:if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
./pzdistribute.c:1693:#if ( PROFlevel>=1 )
./pzdistribute.c:1698:	nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzdistribute.c:1706:		pr = PROW( k, grid );
./pzdistribute.c:1707:		if ( myrow == pr ) {
./pzdistribute.c:1714:	/* Every process receives the count, but it is only useful on the
./pzdistribute.c:1715:	   diagonal processes.  */
./pzdistribute.c:1720:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzdistribute.c:1803:				pr = PROW( ib, grid );
./pzdistribute.c:1804:				if ( myrow == pr ) { /* Block row ib in my process row */
./pzdistribute.c:1811:		pr = PROW( jb, grid );
./pzdistribute.c:1812:		if ( myrow == pr ) { /* Block row ib in my process row */
./pzdistribute.c:1820:		ib = myrow+lib*grid->nprow;  /* not sure */
./pzdistribute.c:1822:			pr = PROW( ib, grid );
./pzdistribute.c:1853:						ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pzdistribute.c:1865:					// #if ( PRNTlevel>=1 )
./pzdistribute.c:1867:					// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
./pzdistribute.c:1870:					// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pzdistribute.c:1871:					// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pzdistribute.c:1872:					// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pzdistribute.c:1873:					// printf("\n");
./pzdistribute.c:1894:#if ( PROFlevel>=1 )
./pzdistribute.c:1896:if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
./pzdistribute.c:1933:#if ( PRNTlevel>=1 )
./pzdistribute.c:1934:	if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
./pzdistribute.c:1952:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzdistribute.c:1956:#if ( PROFlevel>=1 )
./pzdistribute.c:1957:	if ( !iam ) printf(".. 1st distribute time:\n "
./pzdistribute.c:1965:    if ( xa[A->ncol] > 0 ) { /* may not have any entries on this process. */
./pzgsequ.c:4:approvals from U.S. Dept. of Energy) 
./pzgsequ.c:23: <pre>    
./pzgsequ.c:36:    works well in practice.   
./pzgsequ.c:79:            The 2D process mesh.
./pzgsequ.c:81:</pre>
./pzgsequ.c:99:    int_t  procs;
./pzgsequ.c:216:    /* gather R from each process to get the global R.  */
./pzgsequ.c:218:    procs = grid->nprow * grid->npcol;
./pzgsequ.c:219:    if ( !(r_sizes = SUPERLU_MALLOC(2 * procs * sizeof(int))))
./pzgsequ.c:221:    displs = r_sizes + procs;
./pzgsequ.c:232:    for (i = 1; i < procs; ++i) displs[i] = displs[i-1] + r_sizes[i-1];
./pzgsmv.c:4:approvals from U.S. Dept. of Energy) 
./pzgsmv.c:15: * <pre>
./pzgsmv.c:19: * </pre>
./pzgsmv.c:30: int_t *row_to_proc,   /* Input. Mapping between rows and processes. */
./pzgsmv.c:36:    int iam, p, procs;
./pzgsmv.c:56:    procs = grid->nprow * grid->npcol;
./pzgsmv.c:65:    if ( !(SendCounts = SUPERLU_MALLOC(2*procs * sizeof(int))) )
./pzgsmv.c:67:    /*for (i = 0; i < 2*procs; ++i) SendCounts[i] = 0;*/
./pzgsmv.c:68:    RecvCounts = SendCounts + procs;
./pzgsmv.c:69:    if ( !(ptr_ind_tosend = intMalloc_dist(2*(procs+1))) )
./pzgsmv.c:71:    ptr_ind_torecv = ptr_ind_tosend + procs + 1;
./pzgsmv.c:77:       COUNT THE NUMBER OF X ENTRIES TO BE SENT TO EACH PROCESS.
./pzgsmv.c:85:    for (p = 0; p < procs; ++p) SendCounts[p] = 0;
./pzgsmv.c:90:            p = row_to_proc[jcol];
./pzgsmv.c:111:       LOAD THE X-INDICES TO BE SENT TO THE OTHER PROCESSES.
./pzgsmv.c:116:    for (p = 0, TotalIndSend = 0; p < procs; ++p) {
./pzgsmv.c:134:	        p = row_to_proc[jcol];
./pzgsmv.c:166:    for (p = 0, TotalValSend = 0; p < procs; ++p) {
./pzgsmv.c:176:	   SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))))
./pzgsmv.c:178:    recv_req = send_req + procs;
./pzgsmv.c:179:    for (p = 0; p < procs; ++p) {
./pzgsmv.c:190:    for (p = 0; p < procs; ++p) {
./pzgsmv.c:219:    PrintInt10("pzgsmv_init::rowptr", m_loc+1, rowptr);
./pzgsmv.c:220:    PrintInt10("pzgsmv_init::extern_start", m_loc, extern_start);
./pzgsmv.c:248:    int iam, procs;
./pzgsmv.c:268:    procs = grid->nprow * grid->npcol;
./pzgsmv.c:299:	   SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))))
./pzgsmv.c:301:    recv_req = send_req + procs;
./pzgsmv.c:302:    for (p = 0; p < procs; ++p) {
./pzgsmv.c:328:        for (p = 0; p < procs; ++p) {
./pzgsmv.c:351:        for (p = 0; p < procs; ++p) {
./pzgsmv_AXglobal.c:4:approvals from U.S. Dept. of Energy) 
./pzgsmv_AXglobal.c:15: * <pre>
./pzgsmv_AXglobal.c:19: * </pre>
./pzgsmv_AXglobal.c:28:static void zPrintMSRmatrix(int, doublecomplex [], int_t [], gridinfo_t *);
./pzgsmv_AXglobal.c:42: int_t *mv_sup_to_proc /* output */
./pzgsmv_AXglobal.c:47:    int N_update;    /* Number of variables updated on this process (output) */
./pzgsmv_AXglobal.c:49:    int nprocs = grid->nprow * grid->npcol;
./pzgsmv_AXglobal.c:65:	PrintInt10("xsup", supno[n-1]+1, xsup);
./pzgsmv_AXglobal.c:66:	PrintInt10("supno", n, supno);
./pzgsmv_AXglobal.c:72:	/* Figure out mv_sup_to_proc[] on all processes. */
./pzgsmv_AXglobal.c:73:	for (p = 0; p < nprocs; ++p) {
./pzgsmv_AXglobal.c:74:	    t1 = n / nprocs;       /* Number of rows */
./pzgsmv_AXglobal.c:75:	    t2 = n - t1 * nprocs;  /* left-over, which will be assigned
./pzgsmv_AXglobal.c:76:				      to the first t2 processes.  */
./pzgsmv_AXglobal.c:78:	    else { /* First t2 processes will get one more row. */
./pzgsmv_AXglobal.c:96:		    mv_sup_to_proc[i] = p;
./pzgsmv_AXglobal.c:98:		    if ( mv_sup_to_proc[i] == p-1 ) {
./pzgsmv_AXglobal.c:99:			fprintf(stderr, 
./pzgsmv_AXglobal.c:100:				"mv_sup_to_proc conflicts at supno %d\n", i);
./pzgsmv_AXglobal.c:115:		printf("(%2d) N_update = %4d\t"
./pzgsmv_AXglobal.c:125:	t1 = nsupers / nprocs;
./pzgsmv_AXglobal.c:126:	t2 = nsupers - t1 * nprocs; /* left-over */
./pzgsmv_AXglobal.c:143:    PrintInt10("mv_sup_to_proc", nsupers, mv_sup_to_proc);
./pzgsmv_AXglobal.c:144:    zPrintMSRmatrix(N_update, *val, *bindx, grid);
./pzgsmv_AXglobal.c:154: * <pre>
./pzgsmv_AXglobal.c:165: * </pre> 
./pzgsmv_AXglobal.c:250: * <pre>
./pzgsmv_AXglobal.c:254: *   - ax product is distributed the same way as A
./pzgsmv_AXglobal.c:255: * </pre>
./pzgsmv_AXglobal.c:285: *   - ax product is distributed the same way as A
./pzgsmv_AXglobal.c:308: * Print the local MSR matrix
./pzgsmv_AXglobal.c:310:static void zPrintMSRmatrix
./pzgsmv_AXglobal.c:324:    printf("(%2d) MSR submatrix has %d rows -->\n", iam, m);
./pzgsmv_AXglobal.c:325:    PrintDoublecomplex("val", nnzp1, val);
./pzgsmv_AXglobal.c:326:    PrintInt10("bindx", nnzp1, bindx);
./pzgsrfs.c:4:approvals from U.S. Dept. of Energy) 
./pzgsrfs.c:13: * \brief Improves the computed solution to a system of linear equations and provides error bounds and backward error estimates
./pzgsrfs.c:15: * <pre>
./pzgsrfs.c:22: * </pre>
./pzgsrfs.c:30: * <pre>
./pzgsrfs.c:34: * PZGSRFS improves the computed solution to a system of linear   
./pzgsrfs.c:35: * equations and provides error bounds and backward error estimates
./pzgsrfs.c:64: *        The 2D process mesh. It contains the MPI communicator, the number
./pzgsrfs.c:65: *        of process rows (NPROW), the number of process columns (NPCOL),
./pzgsrfs.c:66: *        and my process rank. It is an input argument to all the
./pzgsrfs.c:80: *            transformed system A1*Y = Pc*Pr*B. where
./pzgsrfs.c:81: *            A1 = Pc*Pr*diag(R)*A*diag(C)*Pc' and Y = Pc*diag(C)^(-1)*X.
./pzgsrfs.c:82: *        On exit, the improved solution matrix Y.
./pzgsrfs.c:85: *        Y should be permutated by Pc^T, and premultiplied by diag(C)
./pzgsrfs.c:116: * </pre>
./pzgsrfs.c:191:    if ( !iam ) printf(".. eps = %e\tanorm = %e\tsafe1 = %e\tsafe2 = %e\n",
./pzgsrfs.c:230:#if ( PRNTlevel>= 1 )
./pzgsrfs.c:232:		printf("(%2d) .. Step " IFMT ": berr[j] = %e\n", iam, count, berr[j]);
./pzgsrfs_ABXglobal.c:4:approvals from U.S. Dept. of Energy) 
./pzgsrfs_ABXglobal.c:13: * \brief Improves the computed solution and provies error bounds
./pzgsrfs_ABXglobal.c:15: * <pre>
./pzgsrfs_ABXglobal.c:22: * </pre>
./pzgsrfs_ABXglobal.c:28:/*-- Function prototypes --*/
./pzgsrfs_ABXglobal.c:37: * <pre>
./pzgsrfs_ABXglobal.c:41: * pzgsrfs_ABXglobal improves the computed solution to a system of linear   
./pzgsrfs_ABXglobal.c:42: * equations and provides error bounds and backward error estimates
./pzgsrfs_ABXglobal.c:53: *        A is also permuted into the form Pc*Pr*A*Pc', where Pr and Pc
./pzgsrfs_ABXglobal.c:57: *        NOTE: Currently, A must reside in all processes when calling
./pzgsrfs_ABXglobal.c:71: *        The 2D process mesh. It contains the MPI communicator, the number
./pzgsrfs_ABXglobal.c:72: *        of process rows (NPROW), the number of process columns (NPCOL),
./pzgsrfs_ABXglobal.c:73: *        and my process rank. It is an input argument to all the
./pzgsrfs_ABXglobal.c:82: *        NOTE: Currently, B must reside on all processes when calling
./pzgsrfs_ABXglobal.c:90: *        On exit, the improved solution matrix X.
./pzgsrfs_ABXglobal.c:91: *        If DiagScale = COL or BOTH, X should be premultiplied by diag(C)
./pzgsrfs_ABXglobal.c:94: *        NOTE: Currently, X must reside on all processes when calling
./pzgsrfs_ABXglobal.c:120: * </pre>
./pzgsrfs_ABXglobal.c:137:    int_t  N_update; /* Number of variables updated on this process */
./pzgsrfs_ABXglobal.c:139:			on this processor.                     */
./pzgsrfs_ABXglobal.c:142:    int_t *mv_sup_to_proc;  /* Supernode to process mapping in
./pzgsrfs_ABXglobal.c:150:          nprow, nsupers, nz, p;
./pzgsrfs_ABXglobal.c:157:    int_t num_diag_procs, *diag_procs; /* Record diagonal process numbers. */
./pzgsrfs_ABXglobal.c:158:    int_t *diag_len; /* Length of the X vector on diagonal processes. */
./pzgsrfs_ABXglobal.c:160:    /*-- Function prototypes --*/
./pzgsrfs_ABXglobal.c:186:    nprow = grid->nprow;
./pzgsrfs_ABXglobal.c:196:    get_diag_procs(n, Glu_persist, grid, &num_diag_procs,
./pzgsrfs_ABXglobal.c:197:		   &diag_procs, &diag_len);
./pzgsrfs_ABXglobal.c:198:#if ( PRNTlevel>=1 )
./pzgsrfs_ABXglobal.c:200:	printf(".. number of diag processes = " IFMT "\n", num_diag_procs);
./pzgsrfs_ABXglobal.c:201:	PrintInt10("diag_procs", num_diag_procs, diag_procs);
./pzgsrfs_ABXglobal.c:202:	PrintInt10("diag_len", num_diag_procs, diag_len);
./pzgsrfs_ABXglobal.c:206:    if ( !(mv_sup_to_proc = intCalloc_dist(nsupers)) )
./pzgsrfs_ABXglobal.c:207:	ABORT("Calloc fails for mv_sup_to_proc[]");
./pzgsrfs_ABXglobal.c:210:		          &val, &bindx, mv_sup_to_proc);
./pzgsrfs_ABXglobal.c:212:    i = CEILING( nsupers, nprow ); /* Number of local block rows */
./pzgsrfs_ABXglobal.c:216:    for (j = 1; j < num_diag_procs; ++j) jj = SUPERLU_MAX( jj, diag_len[j] );
./pzgsrfs_ABXglobal.c:243:	PrintDoublecomplex("Mult A*x", N_update, ax);
./pzgsrfs_ABXglobal.c:260:    if ( !iam ) printf(".. eps = %e\tanorm = %e\tsafe1 = %e\tsafe2 = %e\n",
./pzgsrfs_ABXglobal.c:269:	/* Copy X into x on the diagonal processes. */
./pzgsrfs_ABXglobal.c:272:	for (p = 0; p < num_diag_procs; ++p) {
./pzgsrfs_ABXglobal.c:273:	    pkk = diag_procs[p];
./pzgsrfs_ABXglobal.c:275:		for (k = p; k < nsupers; k += num_diag_procs) {
./pzgsrfs_ABXglobal.c:281:		    dx_trs[ii-XK_H].r = k;/* Block number prepended in header. */
./pzgsrfs_ABXglobal.c:285:	/* Copy B into b distributed the same way as matrix-vector product. */
./pzgsrfs_ABXglobal.c:316:#if ( PRNTlevel>= 1 )
./pzgsrfs_ABXglobal.c:318:		printf("(%2d) .. Step " IFMT ": berr[j] = %e\n", iam, count, berr[j]);
./pzgsrfs_ABXglobal.c:323:				   mv_sup_to_proc, dx_trs);
./pzgsrfs_ABXglobal.c:327:		for (p = 0; p < num_diag_procs; ++p) 
./pzgsrfs_ABXglobal.c:328:		    if ( iam == diag_procs[p] )
./pzgsrfs_ABXglobal.c:329:			for (k = p; k < nsupers; k += num_diag_procs) {
./pzgsrfs_ABXglobal.c:339:		/* Transfer x_trs (on diagonal processes) into X
./pzgsrfs_ABXglobal.c:340:		   (on all processes). */
./pzgsrfs_ABXglobal.c:342:					num_diag_procs, diag_procs, diag_len,
./pzgsrfs_ABXglobal.c:355:    SUPERLU_FREE(diag_procs);
./pzgsrfs_ABXglobal.c:362:    SUPERLU_FREE(mv_sup_to_proc);
./pzgsrfs_ABXglobal.c:375: * <pre>
./pzgsrfs_ABXglobal.c:377: * matrix-vector product.
./pzgsrfs_ABXglobal.c:378: * </pre>
./pzgsrfs_ABXglobal.c:382:		   LocalLU_t *Llu, gridinfo_t *grid, int_t mv_sup_to_proc[],
./pzgsrfs_ABXglobal.c:397:	pkk = PNUM( PROW( k, grid ), PCOL( k, grid ), grid );
./pzgsrfs_ABXglobal.c:398:	psrc = mv_sup_to_proc[k];
./pzgsrfs_ABXglobal.c:423: * <pre>
./pzgsrfs_ABXglobal.c:424: * Gather the components of x vector on the diagonal processes
./pzgsrfs_ABXglobal.c:425: * onto all processes, and combine them into the global vector y.
./pzgsrfs_ABXglobal.c:426: * </pre>
./pzgsrfs_ABXglobal.c:431:			gridinfo_t *grid, int_t num_diag_procs,
./pzgsrfs_ABXglobal.c:432:			int_t diag_procs[], int_t diag_len[],
./pzgsrfs_ABXglobal.c:444:    for (p = 0; p < num_diag_procs; ++p) {
./pzgsrfs_ABXglobal.c:445:	pkk = diag_procs[p];
./pzgsrfs_ABXglobal.c:449:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pzgsrfs_ABXglobal.c:462:	for (k = p; k < nsupers; k += num_diag_procs) {
./pzgssvx.c:4:approvals from U.S. Dept. of Energy) 
./pzgssvx.c:15: * <pre>
./pzgssvx.c:21: * April 5, 2015
./pzgssvx.c:24: * April 10, 2018  version 5.3
./pzgssvx.c:26: * </pre>
./pzgssvx.c:34: * <pre>
./pzgssvx.c:44: * to run accurately and efficiently on large numbers of processors.
./pzgssvx.c:69: *        m_loc is the number of rows local to this processor
./pzgssvx.c:80: *      -  grid, a structure describing the 2D processor mesh
./pzgssvx.c:82: *            improve the accuracy of the computed solution using 
./pzgssvx.c:90: *      are used when A is sufficiently similar to a previously 
./pzgssvx.c:91: *      solved problem to save time by reusing part or all of 
./pzgssvx.c:92: *      the previous factorization.)
./pzgssvx.c:105: *                             condition number and so improve the
./pzgssvx.c:144: *                A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pzgssvx.c:146: *               (Note that A1 = Pc*Pr*Aout, where Aout is the matrix stored
./pzgssvx.c:153: *            the same nonzero pattern as a previously factored matrix. In
./pzgssvx.c:154: *            this case the algorithm saves time by reusing the previously
./pzgssvx.c:167: *      previous column permutation from ScalePermstruct->perm_c is used as
./pzgssvx.c:187: *      all of the previously computed structure of L and U.
./pzgssvx.c:190: *            assuming not only the same nonzero pattern as the previously
./pzgssvx.c:206: *        o  ScalePermstruct->DiagScale, how the previous matrix was row
./pzgssvx.c:208: *        o  ScalePermstruct->R, the row scalings of the previous matrix,
./pzgssvx.c:210: *        o  ScalePermstruct->C, the columns scalings of the previous matrix, 
./pzgssvx.c:212: *        o  ScalePermstruct->perm_r, the row permutation of the previous
./pzgssvx.c:214: *        o  ScalePermstruct->perm_c, the column permutation of the previous 
./pzgssvx.c:216: *        o  all of LUstruct, the previously computed information about
./pzgssvx.c:230: *      identical to a matrix that has already been factored on a previous 
./pzgssvx.c:233: *      -  options->Fact = Factored: A is identical to a previously
./pzgssvx.c:234: *            factorized matrix, so the entire previous factorization
./pzgssvx.c:245: *              A from the previous call, so that it has been scaled and permuted)
./pzgssvx.c:263: *           be factorized based on the previous history.
./pzgssvx.c:276: *             pattern was performed prior to this one. Therefore, this
./pzgssvx.c:293: *             prior to this one. Therefore, this factorization will reuse
./pzgssvx.c:296: *             distributed data structure set up from the previous symbolic
./pzgssvx.c:326: *           = LargeDiag_APWM: use the parallel approximate-weight perfect
./pzgssvx.c:348: *           = SLU_DOUBLE: accumulate residual in double precision.
./pzgssvx.c:349: *           = SLU_EXTRA:  accumulate residual in extra precision.
./pzgssvx.c:351: *         NOTE: all options must be identical on all processes when
./pzgssvx.c:358: *           That is, A is stored in distributed compressed row format.
./pzgssvx.c:369: *           performed on the matrix Pc*Pr*diag(R)*A*diag(C)*Pc^T.
./pzgssvx.c:379: *           = ROW:     row equilibration, i.e., A was premultiplied by
./pzgssvx.c:390: *           Row permutation vector, which defines the permutation matrix Pr;
./pzgssvx.c:391: *           perm_r[i] = j means row i of A is in position j in Pr*A.
./pzgssvx.c:403: *           On exit, perm_c may be overwritten by the product of the input
./pzgssvx.c:427: *           process and is defined in the data structure of matrix A.
./pzgssvx.c:439: *         The 2D process mesh. It contains the MPI communicator, the number
./pzgssvx.c:440: *         of process rows (NPROW), the number of process columns (NPCOL),
./pzgssvx.c:441: *         and my process rank. It is an input argument to all the
./pzgssvx.c:463: *           Global data structure (xsup, supno) replicated on all processes,
./pzgssvx.c:500: * </pre>
./pzgssvx.c:519:	       replicated on all processrs.
./pzgssvx.c:520:	           (lsub, xlsub) contains the compressed subscript of
./pzgssvx.c:522:          	   (usub, xusub) contains the compressed subscript of
./pzgssvx.c:553:    int_t lk,k,knsupc,nsupr;
./pzgssvx.c:556:#if ( PRNTlevel>= 2 )
./pzgssvx.c:557:    double   dmin, dsum, dprod;
./pzgssvx.c:562:    int   noDomains, nprocs_num;
./pzgssvx.c:595:	printf("ERROR: Extra precise iterative refinement yet to support.\n");
./pzgssvx.c:605:	printf("ERROR: Relaxation (NREL) cannot be larger than max. supernode size (NSUP).\n"
./pzgssvx.c:628:    /* The following arrays are replicated on all processes. */
./pzgssvx.c:715:#if ( PRNTlevel>=1 )
./pzgssvx.c:716:		    fprintf(stderr, "The " IFMT "-th row of A is exactly zero\n", iinfo);
./pzgssvx.c:719:#if ( PRNTlevel>=1 )
./pzgssvx.c:720:                    fprintf(stderr, "The " IFMT "-th column of A is exactly zero\n", iinfo-n);
./pzgssvx.c:743:#if ( PRNTlevel>=1 )
./pzgssvx.c:745:		printf(".. equilibrated? *equed = %c\n", *equed);
./pzgssvx.c:760:	 * compressed row format to global A in compressed column format.
./pzgssvx.c:770:            pzCompRow_loc_to_CompCol_global(need_value, A, grid, &GA);
./pzgssvx.c:785:           Find the row permutation Pr for A, and apply Pr*[GA].
./pzgssvx.c:786:	   GA is overwritten by Pr*[GA].
./pzgssvx.c:807:	            if ( !iam ) { /* Process 0 finds a row permutation */
./pzgssvx.c:834:#if ( PRNTlevel>=2 )
./pzgssvx.c:837:	            dprod = 1.0;
./pzgssvx.c:855:#if ( PRNTlevel>=2 )
./pzgssvx.c:862:				        dprod *= slud_z_abs1(&a[i]);
./pzgssvx.c:881:                        /* Now permute global GA to prepare for symbfact() */
./pzgssvx.c:902:#if ( PRNTlevel>=2 )
./pzgssvx.c:904:		        if ( !iam ) printf("\tsmallest diagonal %e\n", dmin);
./pzgssvx.c:906:		        if ( !iam ) printf("\tsum of diagonal %e\n", dsum);
./pzgssvx.c:908:		        if ( !iam ) printf("\t product of diagonal %e\n", dprod);
./pzgssvx.c:916:		        printf("CombBLAS is not available\n"); fflush(stdout);
./pzgssvx.c:923:#if ( PRNTlevel>=1 )
./pzgssvx.c:925:		    printf(".. LDPERM job " IFMT "\t time: %.2f\n", job, t);
./pzgssvx.c:936:        if ( !iam ) PrintInt10("perm_r",  m, perm_r);
./pzgssvx.c:945:#if ( PRNTlevel>=1 )
./pzgssvx.c:946:	if ( !iam ) { printf(".. anorm %e\n", anorm); 	fflush(stdout); }
./pzgssvx.c:968:	    nprocs_num = grid->nprow * grid->npcol;
./pzgssvx.c:969:  	    noDomains = (int) ( pow(2, ((int) LOG2( nprocs_num ))));
./pzgssvx.c:972:               processes in grid->comm */
./pzgssvx.c:993:		printf("{" IFMT "," IFMT "}: pzgssvx: invalid ColPerm option when ParSymbfact is used\n",
./pzgssvx.c:1001:	// #pragma omp parallel  
./pzgssvx.c:1003:	// #pragma omp master
./pzgssvx.c:1010:	      flinfo = get_perm_c_parmetis(A, perm_r, perm_c, nprocs_num,
./pzgssvx.c:1016:#if ( PRNTlevel>=1 )
./pzgssvx.c:1017:	          fprintf(stderr, "Insufficient memory for get_perm_c parmetis\n");
./pzgssvx.c:1035:		/* GA = Pr*A, perm_r[] is already applied. */
./pzgssvx.c:1041:	        /* Form Pc*A*Pc^T to preserve the diagonal of the matrix GAC. */
./pzgssvx.c:1053:	        /* Perform a symbolic factorization on Pc*Pr*A*Pc^T and set up
./pzgssvx.c:1055:#if ( PRNTlevel>=1 ) 
./pzgssvx.c:1057:		    printf(".. symbfact(): relax " IFMT ", maxsuper " IFMT ", fill " IFMT "\n",
./pzgssvx.c:1067:	    	/* Every process does this. */
./pzgssvx.c:1074:#if ( PRNTlevel>=1 )
./pzgssvx.c:1076:		    	printf("\tNo of supers " IFMT "\n", Glu_persist->supno[n-1]+1);
./pzgssvx.c:1077:		    	printf("\tSize of G(L) " IFMT "\n", Glu_freeable->xlsub[n]);
./pzgssvx.c:1078:		    	printf("\tSize of G(U) " IFMT "\n", Glu_freeable->xusub[n]);
./pzgssvx.c:1079:		    	printf("\tint %d, short %d, float %d, double %d\n", 
./pzgssvx.c:1082:		    	printf("\tSYMBfact (MB):\tL\\U %.2f\ttotal %.2f\texpansions " IFMT "\n",
./pzgssvx.c:1090:#if ( PRNTlevel>=1 )
./pzgssvx.c:1092:		        fprintf(stderr,"symbfact() error returns " IFMT "\n",iinfo);
./pzgssvx.c:1100:	    	flinfo = symbfact_dist(nprocs_num, noDomains, A, perm_c, perm_r,
./pzgssvx.c:1107:#if ( PRNTlevel>=1 )
./pzgssvx.c:1108:	      	    fprintf(stderr, "Insufficient memory for parallel symbolic factorization.");
./pzgssvx.c:1133:	    /* Distribute Pc*Pr*diag(R)*A*diag(C)*Pc^T into L and U storage. 
./pzgssvx.c:1134:	       NOTE: the row permutation Pc*Pr is applied internally in the
./pzgssvx.c:1147:	    /* Distribute Pc*Pr*diag(R)*A*diag(C)*Pc' into L and U storage. 
./pzgssvx.c:1148:	       NOTE: the row permutation Pc*Pr is applied internally in the
./pzgssvx.c:1162:	/*if (!iam) printf ("\tDISTRIBUTE time  %8.2f\n", stat->utime[DIST]);*/
./pzgssvx.c:1166:    // #pragma omp parallel  
./pzgssvx.c:1168:	// #pragma omp master
./pzgssvx.c:1176:#if ( PRNTlevel>=1 )
./pzgssvx.c:1178:       SUM OVER ALL ENTRIES OF A AND PRINT NNZ AND SIZE OF A.
./pzgssvx.c:1206:			nsupr = lsub[1];
./pzgssvx.c:1208:				for (i = 0; i < nsupr; ++i) 
./pzgssvx.c:1209:					z_add(&lsum,&lsum,&lusup[j*nsupr+i]);
./pzgssvx.c:1226:	print_options_dist(options);
./pzgssvx.c:1230:    printf(".. Ainfo mygid %5d   mysid %5d   nnz_loc " IFMT "  sum_loc  %e lsum_loc   %e nnz "IFMT " nnzLU %ld sum %e  lsum %e  N "IFMT "\n", iam_g,iam,Astore->rowptr[Astore->m_loc],asum.r+asum.i, lsum.r+lsum.i, nnz_tot,nnzLU,asum_tot.r+asum_tot.i,lsum_tot.r+lsum_tot.i,A->ncol);
./pzgssvx.c:1236:// #ifdef GPU_PROF
./pzgssvx.c:1245://          printf("File being opend is %s\n",ttemp );
./pzgssvx.c:1250://              fprintf(stderr," Couldn't open output file %s\n",ttemp);
./pzgssvx.c:1257://                  fprintf(fp,"%d,%d,%d,%d,%d,%d\n",gs1.mnk_min_stats[ii],gs1.mnk_min_stats[ii+nsup],
./pzgssvx.c:1264://          fprintf(fp,"Min %lf Max %lf totaltime %lf \n",gs1.osDgemmMin,gs1.osDgemmMax,stat->utime[FACT]);
./pzgssvx.c:1273:	if ( options->PrintStat ) {
./pzgssvx.c:1311:		printf("\n** Memory Usage **********************************\n");
./pzgssvx.c:1312:                printf("** NUMfact space (MB): (sum-of-all-processes)\n"
./pzgssvx.c:1315:                printf("** Total highmark (MB):\n"
./pzgssvx.c:1318:		       avg / grid->nprow / grid->npcol * 1e-6,
./pzgssvx.c:1320:		printf("**************************************************\n");
./pzgssvx.c:1323:	} /* end printing stats */
./pzgssvx.c:1408:    // #pragma omp parallel  
./pzgssvx.c:1410:	// #pragma omp master
./pzgssvx.c:1418:	   Use iterative refinement to improve the computed solution and
./pzgssvx.c:1422:	    /* Improve the solution by iterative refinement. */
./pzgssvx.c:1433:	        pzgsmv_init(A, SOLVEstruct->row_to_proc, grid,
./pzgssvx.c:1454:		        p = SOLVEstruct->row_to_proc[jcol];
./pzgssvx.c:1463:		   previous call to pzgsmv_init() */
./pzgssvx.c:1478:	        SOLVEstruct1->row_to_proc = SOLVEstruct->row_to_proc;
./pzgssvx.c:1480:	        SOLVEstruct1->num_diag_procs = SOLVEstruct->num_diag_procs;
./pzgssvx.c:1481:	        SOLVEstruct1->diag_procs = SOLVEstruct->diag_procs;
./pzgssvx.c:1507:	pzPermute_Dense_Matrix(fst_row, m_loc, SOLVEstruct->row_to_proc,
./pzgssvx.c:1511:	printf("\n (%d) .. After pzPermute_Dense_Matrix(): b =\n", iam);
./pzgssvx.c:1513:	  printf("\t(%d)\t%4d\t%.10f\n", iam, i+fst_row, B[i]);
./pzgssvx.c:1547:#if ( PRNTlevel>=1 )
./pzgssvx.c:1548:    if ( !iam ) printf(".. DiagScale = %d\n", ScalePermstruct->DiagScale);
./pzgssvx_ABglobal.c:4:approvals from U.S. Dept. of Energy) 
./pzgssvx_ABglobal.c:15: * <pre>
./pzgssvx_ABglobal.c:22: * </pre>
./pzgssvx_ABglobal.c:29: * <pre>
./pzgssvx_ABglobal.c:39: * to run accurately and efficiently on large numbers of processors.
./pzgssvx_ABglobal.c:50: *      -  grid, a structure describing the 2D processor mesh
./pzgssvx_ABglobal.c:52: *            improve the accuracy of the computed solution using 
./pzgssvx_ABglobal.c:60: *      are used when A is sufficiently similar to a previously 
./pzgssvx_ABglobal.c:61: *      solved problem to save time by reusing part or all of 
./pzgssvx_ABglobal.c:62: *      the previous factorization.)
./pzgssvx_ABglobal.c:75: *                           condition number and so improve the
./pzgssvx_ABglobal.c:105: *                Pc*Pr*diag(R)*A*diag(C)
./pzgssvx_ABglobal.c:107: *                Pr and Pc are row and columns permutation matrices determined
./pzgssvx_ABglobal.c:116: *                A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pzgssvx_ABglobal.c:125: *            the same nonzero pattern as a previously factored matrix. In this
./pzgssvx_ABglobal.c:126: *            case the algorithm saves time by reusing the previously computed
./pzgssvx_ABglobal.c:138: *      previous column permutation from ScalePermstruct->perm_c is used as
./pzgssvx_ABglobal.c:158: *      all of the previously computed structure of L and U.
./pzgssvx_ABglobal.c:161: *            assuming not only the same nonzero pattern as the previously
./pzgssvx_ABglobal.c:177: *      -  ScalePermstruct->DiagScale, how the previous matrix was row and/or
./pzgssvx_ABglobal.c:179: *      -  ScalePermstruct->R, the row scalings of the previous matrix, if any
./pzgssvx_ABglobal.c:180: *      -  ScalePermstruct->C, the columns scalings of the previous matrix, 
./pzgssvx_ABglobal.c:182: *      -  ScalePermstruct->perm_r, the row permutation of the previous matrix
./pzgssvx_ABglobal.c:183: *      -  ScalePermstruct->perm_c, the column permutation of the previous 
./pzgssvx_ABglobal.c:185: *      -  all of LUstruct, the previously computed information about L and U
./pzgssvx_ABglobal.c:199: *      identical to a matrix that has already been factored on a previous 
./pzgssvx_ABglobal.c:202: *      -  options->Fact = Factored: A is identical to a previously
./pzgssvx_ABglobal.c:203: *            factorized matrix, so the entire previous factorization
./pzgssvx_ABglobal.c:214: *            the previous call, so that it has been scaled and permuted)
./pzgssvx_ABglobal.c:231: *           be factorized based on the previous history.
./pzgssvx_ABglobal.c:244: *             pattern was performed prior to this one. Therefore, this
./pzgssvx_ABglobal.c:261: *             prior to this one. Therefore, this factorization will reuse
./pzgssvx_ABglobal.c:264: *             distributed data structure set up from the previous symbolic
./pzgssvx_ABglobal.c:294: *           = LargeDiag_APWM: use the parallel approximate-weight perfect
./pzgssvx_ABglobal.c:316: *           = SLU_DOUBLE: accumulate residual in double precision.
./pzgssvx_ABglobal.c:317: *           = SLU_EXTRA:  accumulate residual in extra precision.
./pzgssvx_ABglobal.c:319: *         NOTE: all options must be identical on all processes when
./pzgssvx_ABglobal.c:326: *         compressed column format (also known as Harwell-Boeing format).
./pzgssvx_ABglobal.c:330: *         On exit, A may be overwritten by Pc*Pr*diag(R)*A*diag(C),
./pzgssvx_ABglobal.c:336: *                Pr*diag(R)*A*diag(C).
./pzgssvx_ABglobal.c:338: *                Pc*Pr*diag(R)*A*diag(C).
./pzgssvx_ABglobal.c:340: *         performed on the matrix Pc*Pr*diag(R)*A*diag(C)*Pc^T.
./pzgssvx_ABglobal.c:342: *         NOTE: Currently, A must reside in all processes when calling
./pzgssvx_ABglobal.c:353: *           = ROW:     row equilibration, i.e., A was premultiplied by
./pzgssvx_ABglobal.c:364: *           Row permutation vector, which defines the permutation matrix Pr;
./pzgssvx_ABglobal.c:365: *           perm_r[i] = j means row i of A is in position j in Pr*A.
./pzgssvx_ABglobal.c:377: *           On exit, perm_c may be overwritten by the product of the input
./pzgssvx_ABglobal.c:402: *         NOTE: Currently, B must reside in all processes when calling
./pzgssvx_ABglobal.c:414: *         The 2D process mesh. It contains the MPI communicator, the number
./pzgssvx_ABglobal.c:415: *         of process rows (NPROW), the number of process columns (NPCOL),
./pzgssvx_ABglobal.c:416: *         and my process rank. It is an input argument to all the
./pzgssvx_ABglobal.c:438: *           Global data structure (xsup, supno) replicated on all processes,
./pzgssvx_ABglobal.c:468: * </pre>
./pzgssvx_ABglobal.c:483:	       replicated on all processrs.
./pzgssvx_ABglobal.c:484:	           (lsub, xlsub) contains the compressed subscript of
./pzgssvx_ABglobal.c:486:          	   (usub, xusub) contains the compressed subscript of
./pzgssvx_ABglobal.c:508:#if ( PRNTlevel>= 2 )
./pzgssvx_ABglobal.c:509:    double   dmin, dsum, dprod;
./pzgssvx_ABglobal.c:525:	fprintf(stderr, "Extra precise iterative refinement yet to support.");
./pzgssvx_ABglobal.c:644:#if ( PRNTlevel>=1 )
./pzgssvx_ABglobal.c:645:			    fprintf(stderr, "The " IFMT "-th row of A is exactly zero\n", 
./pzgssvx_ABglobal.c:649:#if ( PRNTlevel>=1 )
./pzgssvx_ABglobal.c:650:                            fprintf(stderr, "The " IFMT "-th column of A is exactly zero\n", 
./pzgssvx_ABglobal.c:683:#if ( PRNTlevel>=1 )
./pzgssvx_ABglobal.c:685:		printf(".. equilibrated? *equed = %c\n", *equed);
./pzgssvx_ABglobal.c:721:		/* Process 0 finds a row permutation for large diagonal. */
./pzgssvx_ABglobal.c:749:#if ( PRNTlevel>=2 )
./pzgssvx_ABglobal.c:752:	    dprod = 1.0;
./pzgssvx_ABglobal.c:767:#if ( PRNTlevel>=2 )
./pzgssvx_ABglobal.c:769:				dprod *= slud_z_abs1(&a[i]);
./pzgssvx_ABglobal.c:797:#if ( PRNTlevel>=2 )
./pzgssvx_ABglobal.c:804:				dprod *= slud_z_abs1(&a[i]);
./pzgssvx_ABglobal.c:814:#if ( PRNTlevel>=2 )
./pzgssvx_ABglobal.c:816:		if ( !iam ) printf("\tsmallest diagonal %e\n", dmin);
./pzgssvx_ABglobal.c:818:		if ( !iam ) printf("\tsum of diagonal %e\n", dsum);
./pzgssvx_ABglobal.c:820:		if ( !iam ) printf("\t product of diagonal %e\n", dprod);
./pzgssvx_ABglobal.c:854:	    /* Use an ordering provided by SuperLU */
./pzgssvx_ABglobal.c:863:	/* Form Pc*A*Pc' to preserve the diagonal of the matrix Pr*A. */
./pzgssvx_ABglobal.c:875:#if ( PRNTlevel>=1 ) 
./pzgssvx_ABglobal.c:877:		printf(".. symbfact(): relax " IFMT ", maxsuper " IFMT ", fill " IFMT "\n",
./pzgssvx_ABglobal.c:892:#if ( PRNTlevel>=1 ) 
./pzgssvx_ABglobal.c:894:		    printf("\tNo of supers " IFMT "\n", Glu_persist->supno[n-1]+1);
./pzgssvx_ABglobal.c:895:		    printf("\tSize of G(L) " IFMT "\n", Glu_freeable->xlsub[n]);
./pzgssvx_ABglobal.c:896:		    printf("\tSize of G(U) " IFMT "\n", Glu_freeable->xusub[n]);
./pzgssvx_ABglobal.c:897:		    printf("\tint %d, short %d, float %d, double %d\n", 
./pzgssvx_ABglobal.c:900:		    printf("\tSYMBfact (MB):\tL\\U %.2f\ttotal %.2f\texpansions " IFMT "\n",
./pzgssvx_ABglobal.c:907:#if ( PRNTlevel>=1 )
./pzgssvx_ABglobal.c:909:		    fprintf(stderr, "symbfact() error returns " IFMT "\n", iinfo);
./pzgssvx_ABglobal.c:916:	/* Distribute the L and U factors onto the process grid. */
./pzgssvx_ABglobal.c:932:#if ( PRNTlevel>=1 )
./pzgssvx_ABglobal.c:953:		printf("\tNUMfact (MB) all PEs:\tL\\U\t%.2f\tall\t%.2f\n",
./pzgssvx_ABglobal.c:955:		printf("\tAll space (MB):"
./pzgssvx_ABglobal.c:957:		       avg*1e-6, avg/grid->nprow/grid->npcol*1e-6, max*1e-6);
./pzgssvx_ABglobal.c:958:		printf("\tNumber of tiny pivots: %10d\n", stat->TinyPivots);
./pzgssvx_ABglobal.c:959:		printf(".. pzgstrf INFO = %d\n", *info);
./pzgssvx_ABglobal.c:966:	 * NOTE: rows of A were previously permuted to Pc*A.
./pzgssvx_ABglobal.c:999:	   Permute the right-hand side to form Pr*B.
./pzgssvx_ABglobal.c:1041:	   Use iterative refinement to improve the computed solution and
./pzgssvx_ABglobal.c:1045:	    /* Improve the solution by iterative refinement. */
./pzgssvx_ABglobal.c:1082:#if ( PRNTlevel>=1 )
./pzgssvx_ABglobal.c:1083:    if ( !iam ) printf(".. DiagScale = %d\n", ScalePermstruct->DiagScale);
./pzgstrf.c:4:approvals from U.S. Dept. of Energy) 
./pzgstrf.c:15: * <pre>
./pzgstrf.c:68: *       krow = PROW( k, grid );
./pzgstrf.c:79: *       if ( iam == Pkk ) multicast L_k,k to this process row;
./pzgstrf.c:81: *          Recv L_k,k from process Pkk;
./pzgstrf.c:88: *       if ( myrow == krow ) multicast U_k,k+1:N to this process column;
./pzgstrf.c:89: *       if ( mycol == kcol ) multicast L_k+1:N,k to this process row;
./pzgstrf.c:92: *          Recv U_k,k+1:N from process Pkj;
./pzgstrf.c:96: *          Recv L_k+1:N,k from process Pik;
./pzgstrf.c:100: *              if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pzgstrf.c:106: * </pre>
./pzgstrf.c:122:    Name    : SUPERNODE_PROFILE  
./pzgstrf.c:123:    Purpose : For SuperNode Level profiling of various measurements such as gigaflop/sec
./pzgstrf.c:127:// #define SUPERNODE_PROFILE   
./pzgstrf.c:190: * <pre>
./pzgstrf.c:222: *           Global data structure (xsup, supno) replicated on all processes,
./pzgstrf.c:233: *        The 2D process mesh. It contains the MPI communicator, the number
./pzgstrf.c:234: *        of process rows (NPROW), the number of process columns (NPCOL),
./pzgstrf.c:235: *        and my process rank. It is an input argument to all the
./pzgstrf.c:251: * </pre>
./pzgstrf.c:273:    int_t Pc, Pr;
./pzgstrf.c:279:    int nsupr, nbrow, segsize;
./pzgstrf.c:285:    int_t *iuip, *ruip; /* Pointers to U index/nzval; size ceil(NSUPERS/Pr). */
./pzgstrf.c:289:    doublecomplex *tempu, *tempv, *tempr;
./pzgstrf.c:299:    int ldt, ldu, lead_zero, ncols, ncb, nrb, p, pr, pc, nblocks;
./pzgstrf.c:376:#if ( PRNTlevel>= 1)
./pzgstrf.c:384:#if ( PRNTlevel==3 )
./pzgstrf.c:387:#if ( PROFlevel>=1 )
./pzgstrf.c:397:    } gemm_profile;
./pzgstrf.c:398:    gemm_profile *gemm_stats;
./pzgstrf.c:422:    Pr = grid->nprow;
./pzgstrf.c:432:        fprintf (stderr, "Could not get TAG_UB\n");
./pzgstrf.c:437:#if ( PRNTlevel>=1 )
./pzgstrf.c:439:        printf ("MPI tag upper bound = %d\n", tag_ub); fflush(stdout);
./pzgstrf.c:445:        printf (" ***** warning s_eps = %e *****\n", s_eps);
./pzgstrf.c:448:#if (PROFlevel >= 1 )
./pzgstrf.c:449:    gemm_stats = (gemm_profile *) SUPERLU_MALLOC(nsupers * sizeof(gemm_profile));
./pzgstrf.c:451:    int *prof_sendR = intCalloc_dist(nsupers);
./pzgstrf.c:462:    if (Pr * Pc > 1) {
./pzgstrf.c:464:              (MPI_Request *) SUPERLU_MALLOC (Pr * sizeof (MPI_Request))))
./pzgstrf.c:483:	    tempr = Llu->Lval_buf_2[0];
./pzgstrf.c:485:		Llu->Lval_buf_2[jj+1] = tempr + i*(jj+1); /* vectorize */
./pzgstrf.c:501:	    tempr = Llu->Uval_buf_2[0];
./pzgstrf.c:503:                Llu->Uval_buf_2[jj+1] = tempr + i*(jj+1); /* vectorize */
./pzgstrf.c:551:        if (!(send_reqs_u[i] = (MPI_Request *) SUPERLU_MALLOC (2 * Pr * sizeof (MPI_Request))))
./pzgstrf.c:571:#pragma omp parallel default(shared)
./pzgstrf.c:572:    #pragma omp master
./pzgstrf.c:585:#if ( PRNTlevel>=1 )
./pzgstrf.c:587:       printf(".. Starting with %d OpenMP threads \n", num_threads );
./pzgstrf.c:594:    nrb = nsupers / Pr; /* number of row blocks, vertical  */
./pzgstrf.c:629:    PrintInt10("schedule:perm_c_supno", nsupers, perm_c_supno);
./pzgstrf.c:632:    printf("[%d] .. Turn off static schedule for debugging ..\n", iam);
./pzgstrf.c:649:        ib = lb * Pr + myrow;
./pzgstrf.c:662:    if (myrow < nsupers % grid->nprow) { /* leftover block rows */
./pzgstrf.c:663:        ib = nrb * Pr + myrow;
./pzgstrf.c:731:#if ( PRNTlevel>=1 )
./pzgstrf.c:733:        printf (".. thresh = s_eps %e * anorm %e = %e\n", s_eps, anorm,
./pzgstrf.c:735:        printf
./pzgstrf.c:753:    k = CEILING (nsupers, Pr);  /* Number of local block rows */
./pzgstrf.c:761:#pragma omp parallel for reduction(max :local_max_row_size) private(lk,lsub) 
./pzgstrf.c:798:    int Threads_per_process = get_thread_per_process();
./pzgstrf.c:799:    int buffer_size  = SUPERLU_MAX(max_row_size*Threads_per_process*ldt,get_max_buffer_size());
./pzgstrf.c:804:    /* Note that in following expression 8 can be anything
./pzgstrf.c:818:    doublecomplex* bigU; /* for storing entire U(k,:) panel, prepare for GEMM.
./pzgstrf.c:823:#if ( PRNTlevel>=1 )
./pzgstrf.c:825:	printf("max_nrows in L panel %d\n", max_row_size);
./pzgstrf.c:826:	printf("\t.. GEMM buffer size: max_nrows X max_ncols = %d x " IFMT "\n",
./pzgstrf.c:828:	printf(".. BIG U size " IFMT "\t BIG V size " IFMT "\n", bigu_size, bigv_size);
./pzgstrf.c:839:#if ( PRNTlevel>=1 )
./pzgstrf.c:840:    if (!iam) printf("[%d] .. BIG V bigv_size %d, using buffer_size %d (on GPU)\n", iam, bigv_size, buffer_size);
./pzgstrf.c:847:#if ( PRNTlevel>=1 )
./pzgstrf.c:848:    printf(" Starting with %d Cuda Streams \n",nstreams );
./pzgstrf.c:873:        fprintf(stderr, "!!!! Error in allocating A in the device %ld \n",m*k*sizeof(doublecomplex) );
./pzgstrf.c:881:        fprintf(stderr, "!!!! Error in allocating B in the device %ld \n",n*k*sizeof(doublecomplex));
./pzgstrf.c:887:        fprintf(stderr, "!!!! Error in allocating C in the device \n" );
./pzgstrf.c:919:#if ( PRNTlevel>=1 )
./pzgstrf.c:921:	printf ("  Max row size is %d \n", max_row_size);
./pzgstrf.c:922:        printf ("  Threads per process %d \n", num_threads);
./pzgstrf.c:954:    int_t mrb=    (nsupers+Pr-1) / Pr;
./pzgstrf.c:1015:    krow = PROW (k, grid);
./pzgstrf.c:1025:        scp = &grid->rscp;      /* The scope of process row. */
./pzgstrf.c:1027:        /* Multicasts numeric values of L(:,0) to process rows. */
./pzgstrf.c:1042:#if ( PROFlevel>=1 )
./pzgstrf.c:1053:                printf ("[%d] first block cloumn Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pzgstrf.c:1057:#if ( PROFlevel>=1 )
./pzgstrf.c:1061:		++prof_sendR[lk];
./pzgstrf.c:1069:            scp = &grid->rscp;  /* The scope of process row. */
./pzgstrf.c:1070:#if ( PROFlevel>=1 )
./pzgstrf.c:1079:#if ( PROFlevel>=1 )
./pzgstrf.c:1092:            scp = &grid->cscp;  /* The scope of process column. */
./pzgstrf.c:1095:#if ( PROFlevel>=1 )
./pzgstrf.c:1104:#if ( PROFlevel>=1 )
./pzgstrf.c:1150:                    /* Multicasts numeric values of L(:,kk) to process rows. */
./pzgstrf.c:1152:                    msgcnt = msgcnts[look_id];  /* point to the proper count array */
./pzgstrf.c:1164:                    scp = &grid->rscp;  /* The scope of process row. */
./pzgstrf.c:1168:#if ( PROFlevel>=1 )
./pzgstrf.c:1177:#if ( PROFlevel>=1 )
./pzgstrf.c:1181:			    ++prof_sendR[lk];
./pzgstrf.c:1184:			    printf ("[%d] -1- Send L(:,%4d): #lsub1 %4d, #lusup1 %4d right to Pj %2d\n",
./pzgstrf.c:1193:                        scp = &grid->rscp;  /* The scope of process row. */
./pzgstrf.c:1195:#if ( PROFlevel>=1 )
./pzgstrf.c:1205:#if ( PROFlevel>=1 )
./pzgstrf.c:1215:            /* Pre-post irecv for U-row look-ahead */
./pzgstrf.c:1216:            krow = PROW (kk, grid);
./pzgstrf.c:1219:                    scp = &grid->cscp;  /* The scope of process column. */
./pzgstrf.c:1222:#if ( PROFlevel>=1 )
./pzgstrf.c:1231:#if ( PROFlevel>=1 )
./pzgstrf.c:1253:                krow = PROW (kk, grid);
./pzgstrf.c:1272:#if ( PROFlevel>=1 )
./pzgstrf.c:1290:#if ( PROFlevel>=1 )
./pzgstrf.c:1302:                    scp = &grid->cscp;  /* The scope of process column. */
./pzgstrf.c:1305:                        /* Parallel triangular solve across process row *krow* --
./pzgstrf.c:1309:/* #pragma omp parallel */ /* Sherry -- parallel done inside pzgstrs2 */
./pzgstrf.c:1319:                        /* Multicasts U(kk,:) to process columns. */
./pzgstrf.c:1331:                            for (pi = 0; pi < Pr; ++pi) {
./pzgstrf.c:1333:#if ( PROFlevel>=1 )
./pzgstrf.c:1342:                                               scp->comm, &send_reqs_u[look_id][pi + Pr]);
./pzgstrf.c:1344:#if ( PROFlevel>=1 )
./pzgstrf.c:1351:                                    printf ("[%d] Send U(%4d,:) to Pr %2d\n",
./pzgstrf.c:1366:         * == start processing the current row of U(k,:) *
./pzgstrf.c:1369:        krow = PROW (k, grid);
./pzgstrf.c:1383:#if ( PROFlevel>=1 )
./pzgstrf.c:1393:#if ( PROFlevel>=1 )
./pzgstrf.c:1403:                scp = &grid->rscp;  /* The scope of process row. */
./pzgstrf.c:1406:                 * Waiting for L(:,kk) for outer-product uptate  *
./pzgstrf.c:1411:#if ( PROFlevel>=1 )
./pzgstrf.c:1421:		    printf("\t[%d] k=%d, look_id=%d, recv_req[0] == MPI_REQUEST_NULL, msgcnt[0] = %d\n", 
./pzgstrf.c:1433:		    printf("\t[%d] k=%d, look_id=%d, recv_req[1] == MPI_REQUEST_NULL, msgcnt[1] = %d\n", 
./pzgstrf.c:1438:#if ( PROFlevel>=1 )
./pzgstrf.c:1444:                printf("[%d] Recv L(:,%4d): #lsub %4d, #lusup %4d from Pc %2d\n",
./pzgstrf.c:1449:#if ( PRNTlevel==3 )
./pzgstrf.c:1462:        scp = &grid->cscp;      /* The scope of process column. */
./pzgstrf.c:1471:                /* Parallel triangular solve across process row *krow* --
./pzgstrf.c:1475:/* #pragma omp parallel */ /* Sherry -- parallel done inside pzgstrs2 */
./pzgstrf.c:1484:                /* Multicasts U(k,:) along process columns. */
./pzgstrf.c:1493:                    for (pi = 0; pi < Pr; ++pi) {
./pzgstrf.c:1494:                        if (pi != myrow) { /* Matching recv was pre-posted before */
./pzgstrf.c:1495:#if ( PROFlevel>=1 )
./pzgstrf.c:1504:#if ( PROFlevel>=1 )
./pzgstrf.c:1512:                            printf ("[%d] Send U(%4d,:) down to Pr %2d\n", iam, k, pi);
./pzgstrf.c:1518:            } else { /* Panel U(k,:) already factorized from previous look-ahead */
./pzgstrf.c:1522:		* for outer-product update.                        *
./pzgstrf.c:1526:#if ( PROFlevel>=1 )
./pzgstrf.c:1529:                    for (pi = 0; pi < Pr; ++pi) {
./pzgstrf.c:1532:                            MPI_Wait (&send_reqs_u[look_id][pi + Pr], &status);
./pzgstrf.c:1535:#if ( PROFlevel>=1 )
./pzgstrf.c:1549:             * Wait for U(k,:) for outer-product updates. *
./pzgstrf.c:1553:#if ( PROFlevel>=1 )
./pzgstrf.c:1561:#if ( PROFlevel>=1 )
./pzgstrf.c:1569:                printf ("[%d] Recv U(%4d,:) from Pr %2d\n", iam, k, krow);
./pzgstrf.c:1571:#if ( PRNTlevel==3 )
./pzgstrf.c:1579:        } /* end if myrow == Pr(k) */
./pzgstrf.c:1585:         *         if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pzgstrf.c:1593:            nsupr = lsub[1];    /* LDA of lusup. */
./pzgstrf.c:1664:                        scp = &grid->rscp;  /* The scope of process row. */
./pzgstrf.c:1668:#if ( PROFlevel>=1 )
./pzgstrf.c:1678:#if ( PROFlevel>=1 )
./pzgstrf.c:1698:                        /* Process column *kcol+1* multicasts numeric
./pzgstrf.c:1699:			   values of L(:,k+1) to process rows. */
./pzgstrf.c:1712:                        scp = &grid->rscp;  /* The scope of process row. */
./pzgstrf.c:1715:#if ( PROFlevel>=1 )
./pzgstrf.c:1724:#if ( PROFlevel>=1 )
./pzgstrf.c:1728:				++prof_sendR[lk];
./pzgstrf.c:1767:#if ( PRNTlevel>=1 )
./pzgstrf.c:1768:    /* Print detailed statistics */
./pzgstrf.c:1774:	printf("\nInitialization time\t%8.2lf seconds\n"
./pzgstrf.c:1776:        printf("\n==== Time breakdown in factorization (rank 0) ====\n");
./pzgstrf.c:1777:	printf("Panel factorization \t %8.2lf seconds\n",
./pzgstrf.c:1779:	printf(".. L-panel pxgstrf2 \t %8.2lf seconds\n", pdgstrf2_timer);
./pzgstrf.c:1780:	printf(".. U-panel pxgstrs2 \t %8.2lf seconds\n", pdgstrs2_timer);
./pzgstrf.c:1781:	printf("Time in Look-ahead update \t %8.2lf seconds\n", lookaheadupdatetimer);
./pzgstrf.c:1782:        printf("Time in Schur update \t\t %8.2lf seconds\n", NetSchurUpTimer);
./pzgstrf.c:1783:        printf(".. Time to Gather L buffer\t %8.2lf  (Separate L panel by Lookahead/Remain)\n", GatherLTimer);
./pzgstrf.c:1784:        printf(".. Time to Gather U buffer\t %8.2lf \n", GatherUTimer);
./pzgstrf.c:1786:        printf(".. Time in GEMM %8.2lf \n",
./pzgstrf.c:1788:        printf("\t* Look-ahead\t %8.2lf \n", LookAheadGEMMTimer);
./pzgstrf.c:1789:        printf("\t* Remain\t %8.2lf\tFlops %8.2le\tGflops %8.2lf\n", 
./pzgstrf.c:1791:        printf(".. Time to Scatter %8.2lf \n", 
./pzgstrf.c:1793:        printf("\t* Look-ahead\t %8.2lf \n", LookAheadScatterTimer);
./pzgstrf.c:1794:        printf("\t* Remain\t %8.2lf \n", RemainScatterTimer);
./pzgstrf.c:1796:        printf("Total factorization time            \t: %8.2lf seconds, \n", pxgstrfTimer);
./pzgstrf.c:1797:        printf("--------\n");
./pzgstrf.c:1798:	printf("GEMM maximum block: %d-%d-%d\n", gemm_max_m, gemm_max_k, gemm_max_n);
./pzgstrf.c:1803:    for (i = 0; i < Pr * Pc; ++i) {
./pzgstrf.c:1805:            zPrintLblocks(iam, nsupers, grid, Glu_persist, Llu);
./pzgstrf.c:1806:            zPrintUblocks(iam, nsupers, grid, Glu_persist, Llu);
./pzgstrf.c:1807:            printf ("(%d)\n", iam);
./pzgstrf.c:1808:            PrintInt10 ("Recv", nsupers, Llu->ToRecv);
./pzgstrf.c:1818:    if (Pr * Pc > 1) {
./pzgstrf.c:1827:            for (krow = 0; krow < Pr; ++krow) {
./pzgstrf.c:1908:#if ( PRNTlevel>=1 )
./pzgstrf.c:1934:#if ( PROFlevel>=1 )
./pzgstrf.c:1938:    /* Prepare error message - find the smallesr index i that U(i,i)==0 */
./pzgstrf.c:1944:#if ( PROFlevel>=1 )
./pzgstrf.c:1959:            printf ("\tPZGSTRF comm stat:"
./pzgstrf.c:1962:                    msg_cnt_sum / Pr / Pc, msg_cnt_max,
./pzgstrf.c:1963:                    msg_vol_sum / Pr / Pc * 1e-6, msg_vol_max * 1e-6);
./pzgstrf.c:1964:	    printf("\t\tcomm time on task 0: %8.2lf\n"
./pzgstrf.c:1972:	    printf("gemm_count %d\n", gemm_count);
./pzgstrf.c:1974:		fprintf(fgemm, "%8d%8d%8d\t %20.16e\t%8d\n", gemm_stats[i].m, gemm_stats[i].n,
./pzgstrf.c:1975:			gemm_stats[i].k, gemm_stats[i].microseconds, prof_sendR[i]);
./pzgstrf.c:1980:	SUPERLU_FREE(prof_sendR);
./pzgstrf.c:1984:#if ( PRNTlevel==3 )
./pzgstrf.c:1987:        printf (".. # msg of zero size\t%d\n", iinfo);
./pzgstrf.c:1990:        printf (".. # total msg\t%d\n", iinfo);
./pzgstrf.c:1994:    for (i = 0; i < Pr * Pc; ++i) {
./pzgstrf.c:1996:            zPrintLblocks (iam, nsupers, grid, Glu_persist, Llu);
./pzgstrf.c:1997:            zPrintUblocks (iam, nsupers, grid, Glu_persist, Llu);
./pzgstrf.c:1998:            printf ("(%d)\n", iam);
./pzgstrf.c:1999:            PrintInt10 ("Recv", nsupers, Llu->ToRecv);
./pzgstrf.c:2006:    printf ("(%d) num_copy=%d, num_update=%d\n", iam, num_copy, num_update);
./pzgstrf2.c:4:approvals from U.S. Dept. of Energy) 
./pzgstrf2.c:15: * <pre>
./pzgstrf2.c:23: * <pre>
./pzgstrf2.c:29: *   Only the column processes that own block column *k* participate
./pzgstrf2.c:48: *        Global data structures (xsup, supno) replicated on all processes.
./pzgstrf2.c:51: *        The 2D process mesh.
./pzgstrf2.c:73: * </pre>
./pzgstrf2.c:87:    /* printf("entering pzgstrf2 %d \n", grid->iam); */
./pzgstrf2.c:88:    int cols_left, iam, l, pkk, pr;
./pzgstrf2.c:91:    int nsupr;                  /* number of rows in the block (LDA) */
./pzgstrf2.c:99:    int_t Pr;
./pzgstrf2.c:106:    Pr = grid->nprow;
./pzgstrf2.c:108:    krow = PROW (k, grid);
./pzgstrf2.c:109:    pkk = PNUM (PROW (k, grid), PCOL (k, grid), grid);
./pzgstrf2.c:116:        nsupr = Llu->Lrowind_bc_ptr[j][1];
./pzgstrf2.c:118:        nsupr = 0;
./pzgstrf2.c:120:    printf ("rank %d  Iter %d  k=%d \t ztrsm nsuper %d \n",
./pzgstrf2.c:121:            iam, k0, k, nsupr);
./pzgstrf2.c:134:#if ( PROFlevel>=1 )
./pzgstrf2.c:137:        for (pr = 0; pr < Pr; ++pr) {
./pzgstrf2.c:138:            if (pr != myrow) {
./pzgstrf2.c:139:                MPI_Wait (U_diag_blk_send_req + pr, &status);
./pzgstrf2.c:142:#if ( PROFlevel>=1 )
./pzgstrf2.c:151:    if (iam == pkk) {            /* diagonal process */
./pzgstrf2.c:160:#if ( PRNTlevel>=2 )
./pzgstrf2.c:161:                    printf ("(%d) .. col %d, tiny pivot %e  ",
./pzgstrf2.c:168:#if ( PRNTlevel>=2 )
./pzgstrf2.c:169:                    printf ("replaced by %e\n", lusup[i]);
./pzgstrf2.c:176:            for (l = 0; l < cols_left; ++l, i += nsupr, ++u_diag_cnt)
./pzgstrf2.c:182:            for (l = 0; l < cols_left; ++l, i += nsupr, ++u_diag_cnt) {
./pzgstrf2.c:199:                /* l = nsupr - j - 1;  */
./pzgstrf2.c:202:                       &ujrow[ld_ujrow], &incy, &lusup[luptr + nsupr + 1],
./pzgstrf2.c:203:                       &nsupr);
./pzgstrf2.c:209:            luptr += nsupr + 1; /* move to next column */
./pzgstrf2.c:219:#if ( PROFlevel>=1 )
./pzgstrf2.c:222:            for (pr = 0; pr < Pr; ++pr) {
./pzgstrf2.c:223:                if (pr != krow) {
./pzgstrf2.c:226:                    MPI_Isend (ublk_ptr, nsupc * nsupc, SuperLU_MPI_DOUBLE_COMPLEX, pr,
./pzgstrf2.c:228:                               comm, U_diag_blk_send_req + pr);
./pzgstrf2.c:232:#if ( PROFlevel>=1 )
./pzgstrf2.c:242:        /* pragma below would be changed by an MKL call */
./pzgstrf2.c:244:        l = nsupr - nsupc;
./pzgstrf2.c:248:        printf ("calling ztrsm\n");
./pzgstrf2.c:249:        printf ("ztrsm diagonal param 11:  %d \n", nsupr);
./pzgstrf2.c:254:                &alpha, ublk_ptr, &ld_ujrow, &lusup[nsupc], &nsupr,
./pzgstrf2.c:258:                &alpha, ublk_ptr, &ld_ujrow, &lusup[nsupc], &nsupr);
./pzgstrf2.c:261:    } else {  /* non-diagonal process */
./pzgstrf2.c:270:        // printf("hello message receiving%d %d\n",(nsupc*(nsupc+1))>>1,SLU_MPI_TAG(4,k0));
./pzgstrf2.c:271:#if ( PROFlevel>=1 )
./pzgstrf2.c:277:#if ( PROFlevel>=1 )
./pzgstrf2.c:282:        if (nsupr > 0) {
./pzgstrf2.c:286:            printf ("ztrsm non diagonal param 11:  %d \n", nsupr);
./pzgstrf2.c:288:                printf (" Rank :%d \t Empty block column occurred :\n", iam);
./pzgstrf2.c:291:            ztrsm_ ("R", "U", "N", "N", &nsupr, &nsupc,
./pzgstrf2.c:292:                    &alpha, ublk_ptr, &ld_ujrow, lusup, &nsupr, 1, 1, 1, 1);
./pzgstrf2.c:294:            ztrsm_ ("R", "U", "N", "N", &nsupr, &nsupc,
./pzgstrf2.c:295:                    &alpha, ublk_ptr, &ld_ujrow, lusup, &nsupr);
./pzgstrf2.c:297:	    stat->ops[FACT] += 4.0 * ((flops_t) nsupc * (nsupc+1) * nsupr);
./pzgstrf2.c:302:    /* printf("exiting pzgstrf2 %d \n", grid->iam);  */
./pzgstrf2.c:314:    printf("====Entering pzgstrs2==== \n");
./pzgstrf2.c:318:    int nsupr;                /* number of rows in the block L(:,k) (LDA) */
./pzgstrf2.c:339:    pkk = PNUM (PROW (k, grid), PCOL (k, grid), grid);
./pzgstrf2.c:340:    //int k_row_cycle = k / grid->nprow;  /* for which cycle k exist (to assign rowwise thread blocking) */
./pzgstrf2.c:348:        nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pzgstrf2.c:351:        nsupr = Llu->Lsub_buf_2[k0 % (1 + stat->num_look_aheads)][1];   /* LDA of lusup[] */
./pzgstrf2.c:377:    // https://stackoverflow.com/questions/13065943/task-based-programming-pragma-omp-task-versus-pragma-omp-parallel-for
./pzgstrf2.c:378:#pragma omp parallel for schedule(static) default(shared) \
./pzgstrf2.c:379:    private(b,j,iukp,rukp,segsize)
./pzgstrf2.c:389:#pragma omp task default(shared) firstprivate(segsize,rukp) if (segsize > 30)
./pzgstrf2.c:391:		    int_t luptr = (knsupc - segsize) * (nsupr + 1);
./pzgstrf2.c:392:		    //printf("[2] segsize %d, nsupr %d\n", segsize, nsupr);
./pzgstrf2.c:395:                    ztrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
./pzgstrf2.c:398:                    ztrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
./pzgstrf2.c:406:/* #pragma omp taskwait */
./pzgstrf_irecv.c:4:approvals from U.S. Dept. of Energy) 
./pzgstrf_irecv.c:15: * <pre>
./pzgstrf_irecv.c:60: *       krow = PROW( k, grid );
./pzgstrf_irecv.c:71: *       if ( iam == Pkk ) multicast L_k,k to this process row;
./pzgstrf_irecv.c:73: *          Recv L_k,k from process Pkk;
./pzgstrf_irecv.c:80: *       if ( myrow == krow ) multicast U_k,k+1:N to this process column;
./pzgstrf_irecv.c:81: *       if ( mycol == kcol ) multicast L_k+1:N,k to this process row;
./pzgstrf_irecv.c:84: *          Recv U_k,k+1:N from process Pkj;
./pzgstrf_irecv.c:88: *          Recv L_k+1:N,k from process Pik;
./pzgstrf_irecv.c:92: *              if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pzgstrf_irecv.c:101: * </pre>
./pzgstrf_irecv.c:108: * Internal prototypes
./pzgstrf_irecv.c:124: * <pre>
./pzgstrf_irecv.c:156: *           Global data structure (xsup, supno) replicated on all processes,
./pzgstrf_irecv.c:167: *        The 2D process mesh. It contains the MPI communicator, the number
./pzgstrf_irecv.c:168: *        of process rows (NPROW), the number of process columns (NPCOL),
./pzgstrf_irecv.c:169: *        and my process rank. It is an input argument to all the
./pzgstrf_irecv.c:185: * </pre>
./pzgstrf_irecv.c:210:    int_t Pc, Pr;
./pzgstrf_irecv.c:213:    int   nsupr, nbrow, segsize;
./pzgstrf_irecv.c:225:    int_t  *iuip, *ruip;/* Pointers to U index/nzval; size ceil(NSUPERS/Pr). */
./pzgstrf_irecv.c:243:#if ( PRNTlevel==3 )
./pzgstrf_irecv.c:246:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:269:    Pr = grid->nprow;
./pzgstrf_irecv.c:283:    if ( Pr*Pc > 1 ) {
./pzgstrf_irecv.c:305:#if ( PRNTlevel>=1 )
./pzgstrf_irecv.c:307:	printf(".. thresh = s_eps %e * anorm %e = %e\n", s_eps, anorm, thresh);
./pzgstrf_irecv.c:308:	printf(".. Buffer size: Lsub %d\tLval %d\tUsub %d\tUval %d\tLDA %d\n",
./pzgstrf_irecv.c:334:    k = CEILING( nsupers, Pr ); /* Number of local block rows */
./pzgstrf_irecv.c:347:	scp = &grid->rscp; /* The scope of process row. */
./pzgstrf_irecv.c:349:	/* Process column *kcol* multicasts numeric values of L(:,k) 
./pzgstrf_irecv.c:350:	   to process rows. */
./pzgstrf_irecv.c:362:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:370:		printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pzgstrf_irecv.c:373:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:383:	    scp = &grid->rscp; /* The scope of process row. */
./pzgstrf_irecv.c:389:	    printf("(%d) Post Irecv L(:,%4d)\n", iam, 0);
./pzgstrf_irecv.c:400:	krow = PROW( k, grid );
./pzgstrf_irecv.c:417:		scp = &grid->rscp; /* The scope of process row. */
./pzgstrf_irecv.c:418:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:421:		/*probe_recv(iam, kcol, (4*k)%NTAGS, mpi_int_t, scp->comm, 
./pzgstrf_irecv.c:427:		/*probe_recv(iam, kcol, (4*k+1)%NTAGS, SuperLU_MPI_DOUBLE_COMPLEX, scp->comm, 
./pzgstrf_irecv.c:433:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:438:		printf("(%d) Recv L(:,%4d): lsub %4d, lusup %4d from Pc %2d\n",
./pzgstrf_irecv.c:444:#if ( PRNTlevel==3 )
./pzgstrf_irecv.c:451:	scp = &grid->cscp; /* The scope of process column. */
./pzgstrf_irecv.c:454:	    /* Parallel triangular solve across process row *krow* --
./pzgstrf_irecv.c:462:	    /* Multicasts U(k,:) to process columns. */
./pzgstrf_irecv.c:474:		for (pi = 0; pi < Pr; ++pi) {
./pzgstrf_irecv.c:476:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:483:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:490:			printf("(%d) Send U(%4d,:) to Pr %2d\n", iam, k, pi);
./pzgstrf_irecv.c:497:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:500:		/*probe_recv(iam, krow, (4*k+2)%NTAGS, mpi_int_t, scp->comm, 
./pzgstrf_irecv.c:505:		/*probe_recv(iam, krow, (4*k+3)%NTAGS, SuperLU_MPI_DOUBLE_COMPLEX, scp->comm, 
./pzgstrf_irecv.c:510:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:517:		printf("(%d) Recv U(%4d,:) from Pr %2d\n", iam, k, krow);
./pzgstrf_irecv.c:519:#if ( PRNTlevel==3 )
./pzgstrf_irecv.c:524:	} /* if myrow == Pr(k) */
./pzgstrf_irecv.c:530:	 *         if ( myrow == PROW( i, grid ) && mycol == PCOL( j, grid )
./pzgstrf_irecv.c:537:	    nsupr = lsub[1]; /* LDA of lusup. */
./pzgstrf_irecv.c:572:		/* Prepare to call DGEMM. */
./pzgstrf_irecv.c:593:		  printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pzgstrf_irecv.c:621:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pzgstrf_irecv.c:625:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pzgstrf_irecv.c:629:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pzgstrf_irecv.c:689:/*#pragma _CRI cache_bypass nzval,tempv*/
./pzgstrf_irecv.c:718:	    /* Process column *kcol+1* multicasts numeric values of L(:,k+1) 
./pzgstrf_irecv.c:719:	       to process rows. */
./pzgstrf_irecv.c:729:	    scp = &grid->rscp; /* The scope of process row. */
./pzgstrf_irecv.c:733:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:740:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:747:		    printf("(%d) Send L(:,%4d): lsub %4d, lusup %4d to Pc %2d\n",
./pzgstrf_irecv.c:754:		scp = &grid->rscp; /* The scope of process row. */
./pzgstrf_irecv.c:760:		printf("(%d) Post Irecv L(:,%4d)\n", iam, k+1);
./pzgstrf_irecv.c:778:		/* Prepare to call DGEMM. */
./pzgstrf_irecv.c:793:		printf("(%d) full=%d,k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./pzgstrf_irecv.c:827:			  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pzgstrf_irecv.c:831:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pzgstrf_irecv.c:835:			   &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr, 
./pzgstrf_irecv.c:896:/*#pragma _CRI cache_bypass nzval,tempv*/
./pzgstrf_irecv.c:922:    if ( Pr*Pc > 1 ) {
./pzgstrf_irecv.c:936:    /* Prepare error message. */
./pzgstrf_irecv.c:938:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:942:#if ( PROFlevel>=1 )
./pzgstrf_irecv.c:957:	    printf("\tPZGSTRF comm stat:"
./pzgstrf_irecv.c:960:		   msg_cnt_sum/Pr/Pc, msg_cnt_max,
./pzgstrf_irecv.c:961:		   msg_vol_sum/Pr/Pc*1e-6, msg_vol_max*1e-6);
./pzgstrf_irecv.c:969:#if ( PRNTlevel==3 )
./pzgstrf_irecv.c:971:    if ( !iam ) printf(".. # msg of zero size\t%d\n", iinfo);
./pzgstrf_irecv.c:973:    if ( !iam ) printf(".. # total msg\t%d\n", iinfo);
./pzgstrf_irecv.c:977:    for (i = 0; i < Pr * Pc; ++i) {
./pzgstrf_irecv.c:979:	    zPrintLblocks(iam, nsupers, grid, Glu_persist, Llu);
./pzgstrf_irecv.c:980:	    zPrintUblocks(iam, nsupers, grid, Glu_persist, Llu);
./pzgstrf_irecv.c:981:	    printf("(%d)\n", iam);
./pzgstrf_irecv.c:982:	    PrintInt10("Recv", nsupers, Llu->ToRecv);
./pzgstrf_irecv.c:989:    printf("(%d) num_copy=%d, num_update=%d\n", iam, num_copy, num_update);
./pzgstrf_irecv.c:1000: * <pre>
./pzgstrf_irecv.c:1004: *   Only the process column that owns block column *k* participates
./pzgstrf_irecv.c:1017: *        Global data structures (xsup, supno) replicated on all processes.
./pzgstrf_irecv.c:1020: *        The 2D process mesh.
./pzgstrf_irecv.c:1036: * </pre>
./pzgstrf_irecv.c:1048:    int    nsupr; /* number of rows in the block (LDA) */
./pzgstrf_irecv.c:1062:    krow  = PROW( k, grid );
./pzgstrf_irecv.c:1063:    pkk   = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pzgstrf_irecv.c:1069:    if ( Llu->Lrowind_bc_ptr[j] ) nsupr = Llu->Lrowind_bc_ptr[j][1];
./pzgstrf_irecv.c:1076:	   the process column. */
./pzgstrf_irecv.c:1077:	if ( iam == pkk ) { /* Diagonal process. */
./pzgstrf_irecv.c:1081:#if ( PRNTlevel>=2 )
./pzgstrf_irecv.c:1082:		    printf("(%d) .. col %d, tiny pivot %e  ",
./pzgstrf_irecv.c:1089:#if ( PRNTlevel>=2 )
./pzgstrf_irecv.c:1090:		    printf("replaced by %e\n", lusup[i]);
./pzgstrf_irecv.c:1095:	    for (l = 0; l < c; ++l, i += nsupr)	ujrow[l] = lusup[i];
./pzgstrf_irecv.c:1108:	    printf("..(%d) k %d, j %d: Send ujrow[0] %e\n",iam,k,j,ujrow[0]);
./pzgstrf_irecv.c:1110:	    printf("..(%d) k %d, j %d: Recv ujrow[0] %e\n",iam,k,j,ujrow[0]);
./pzgstrf_irecv.c:1128:		for (i = luptr+1; i < luptr-j+nsupr; ++i)
./pzgstrf_irecv.c:1130:		stat->ops[FACT] += 6*(nsupr-j-1) + 10;
./pzgstrf_irecv.c:1132:		for (i = luptr; i < luptr+nsupr; ++i)
./pzgstrf_irecv.c:1134:		stat->ops[FACT] += 6*nsupr + 10;
./pzgstrf_irecv.c:1141:		l = nsupr - j - 1;
./pzgstrf_irecv.c:1144:		      &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pzgstrf_irecv.c:1147:		      &ujrow[1], &incy, &lusup[luptr+nsupr+1], &nsupr);
./pzgstrf_irecv.c:1152:		CGERU(&nsupr, &c, &alpha, &lusup[luptr], &incx, 
./pzgstrf_irecv.c:1153:		      &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pzgstrf_irecv.c:1155:		zgeru_(&nsupr, &c, &alpha, &lusup[luptr], &incx, 
./pzgstrf_irecv.c:1156:		      &ujrow[1], &incy, &lusup[luptr+nsupr], &nsupr);
./pzgstrf_irecv.c:1158:		stat->ops[FACT] += 8 * nsupr * c;
./pzgstrf_irecv.c:1163:	if ( iam == pkk ) luptr += nsupr + 1;
./pzgstrf_irecv.c:1164:	else luptr += nsupr;
./pzgstrf_irecv.c:1190: *   Only the process row that owns block row *k* participates
./pzgstrf_irecv.c:1203: *        Global data structures (xsup, supno) replicated on all processes.
./pzgstrf_irecv.c:1206: *        The 2D process mesh.
./pzgstrf_irecv.c:1219:    int    nsupr; /* number of rows in the block L(:,k) (LDA) */
./pzgstrf_irecv.c:1234:    pkk  = PNUM( PROW(k, grid), PCOL(k, grid), grid );
./pzgstrf_irecv.c:1244:	nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
./pzgstrf_irecv.c:1247:	nsupr = Llu->Lsub_buf_2[k%2][1]; /* LDA of lusup[] */
./pzgstrf_irecv.c:1261:		luptr = (knsupc - segsize) * (nsupr + 1);
./pzgstrf_irecv.c:1263:		CTRSV(ftcs1, ftcs2, ftcs3, &segsize, &lusup[luptr], &nsupr, 
./pzgstrf_irecv.c:1266:		ztrsv_("L", "N", "U", &segsize, &lusup[luptr], &nsupr, 
./pzgstrf_irecv.c:1269:		ztrsv_("L", "N", "U", &segsize, &lusup[luptr], &nsupr, 
./pzgstrf_irecv.c:1282:probe_recv(int iam, int source, int tag, MPI_Datatype datatype, MPI_Comm comm,
./pzgstrf_irecv.c:1288:    MPI_Probe( source, tag, comm, &status );
./pzgstrf_irecv.c:1291:        printf("(%d) Recv'ed count %d > buffer size $d\n",
./pzgstrs.c:4:approvals from U.S. Dept. of Energy) 
./pzgstrs.c:14: * general N-by-N matrix A using the LU factors computed previously.
./pzgstrs.c:16: * <pre>
./pzgstrs.c:21: * </pre>
./pzgstrs.c:42: *          if all local updates done, Isend lsum[] to diagonal process;
./pzgstrs.c:44: *      } else if ( message is LSUM ) { .. this must be a diagonal process 
./pzgstrs.c:48: *              Isend Xi down to the current process column;
./pzgstrs.c:61: *   + prepend a header recording the global block number.
./pzgstrs.c:89: * Function prototypes
./pzgstrs.c:101: * <pre>
./pzgstrs.c:104: *   Re-distribute B on the diagonal processes of the 2D process mesh.
./pzgstrs.c:134: *        The solution vector. It is valid only on the diagonal processes.
./pzgstrs.c:141: *        The 2D process mesh.
./pzgstrs.c:149: * </pre>
./pzgstrs.c:167:    int    p, procs;
./pzgstrs.c:182:    procs = grid->nprow * grid->npcol;
./pzgstrs.c:186:    SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
./pzgstrs.c:187:    RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
./pzgstrs.c:188:    RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
./pzgstrs.c:189:    sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
./pzgstrs.c:190:    sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
./pzgstrs.c:191:    rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
./pzgstrs.c:192:    rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
./pzgstrs.c:200:	if(procs==1){ // faster memory copy when procs=1 
./pzgstrs.c:203:#pragma omp parallel default (shared)
./pzgstrs.c:207:#pragma omp master
./pzgstrs.c:212:#pragma	omp	taskloop private (i,l,irow,k,j,knsupc) untied 
./pzgstrs.c:215:			irow = perm_c[perm_r[i+fst_row]]; /* Row number in Pc*Pr*B */
./pzgstrs.c:221:			x[l - XK_H].r = k; /* Block number prepended in the header. */
./pzgstrs.c:232:		k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pzgstrs.c:233:		l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pzgstrs.c:240:		if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pzgstrs.c:242:		if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pzgstrs.c:244:		if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pzgstrs.c:246:		if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pzgstrs.c:249:		for (p = 0; p < procs; ++p) {
./pzgstrs.c:257:			irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
./pzgstrs.c:259:		p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
./pzgstrs.c:272:		// printf(".. copy to send buffer time\t%8.4f\n", t);	
./pzgstrs.c:294:		   Copy buffer into X on the diagonal processes.
./pzgstrs.c:299:		for (p = 0; p < procs; ++p) {
./pzgstrs.c:302:			/* Only the diagonal processes do this; the off-diagonal processes
./pzgstrs.c:309:			x[l - XK_H].r = k; /* Block number prepended in the header. */
./pzgstrs.c:321:		// printf(".. copy to x time\t%8.4f\n", t);	
./pzgstrs.c:340: * <pre>
./pzgstrs.c:343: *   Re-distribute X on the diagonal processes to B distributed on all
./pzgstrs.c:344: *   the processes.
./pzgstrs.c:350: * </pre>
./pzgstrs.c:367:    int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
./pzgstrs.c:369:    int  iam, p, q, pkk, procs;
./pzgstrs.c:370:    int_t  num_diag_procs, *diag_procs;
./pzgstrs.c:386:    procs = grid->nprow * grid->npcol;
./pzgstrs.c:389:    SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
./pzgstrs.c:390:    RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
./pzgstrs.c:391:    RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
./pzgstrs.c:392:    sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
./pzgstrs.c:393:    sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
./pzgstrs.c:394:    rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
./pzgstrs.c:395:    rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
./pzgstrs.c:400:	if(procs==1){ //faster memory copy when procs=1
./pzgstrs.c:403:#pragma omp parallel default (shared)
./pzgstrs.c:407:#pragma omp master
./pzgstrs.c:412:#pragma	omp	taskloop private (k,knsupc,lk,irow,l,i,j) untied 
./pzgstrs.c:428:		k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
./pzgstrs.c:429:		l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
./pzgstrs.c:435:		if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pzgstrs.c:437:		if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
./pzgstrs.c:439:		if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pzgstrs.c:441:		if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
./pzgstrs.c:444:		for (p = 0; p < procs; ++p) {
./pzgstrs.c:448:		num_diag_procs = SOLVEstruct->num_diag_procs;
./pzgstrs.c:449:		diag_procs = SOLVEstruct->diag_procs;
./pzgstrs.c:450: 		for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
./pzgstrs.c:451:		pkk = diag_procs[p];
./pzgstrs.c:453:			for (k = p; k < nsupers; k += num_diag_procs) {
./pzgstrs.c:464:				q = row_to_proc[ii];
./pzgstrs.c:527: * <pre>
./pzgstrs.c:532: * </pre>
./pzgstrs.c:552:    int    Pc, Pr, iam;
./pzgstrs.c:553:    int    knsupc, nsupr;
./pzgstrs.c:566:#if ( PROFlevel>=1 )
./pzgstrs.c:570:#if ( PRNTlevel>=1 )
./pzgstrs.c:572:	printf("computing inverse of diagonal blocks...\n");
./pzgstrs.c:582:    Pr = grid->nprow;
./pzgstrs.c:592:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pzgstrs.c:601:         krow = PROW( k, grid );
./pzgstrs.c:605:	     if ( mycol == kcol ) { /* diagonal process */
./pzgstrs.c:612:		  nsupr = lsub[1];	
./pzgstrs.c:625:		  	  z_copy(&Linv[j*knsupc+i],&lusup[j*nsupr+i]);
./pzgstrs.c:629:		          z_copy(&Uinv[j*knsupc+i],&lusup[j*nsupr+i]);
./pzgstrs.c:642:#if ( PROFlevel>=1 )
./pzgstrs.c:645:	printf(".. L-diag_inv time\t%10.5f\n", t);
./pzgstrs.c:657: * <pre>
./pzgstrs.c:666: *     A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pzgstrs.c:668: *     A1 * Y = Pc*Pr*B1, where B was overwritten by B1 = diag(R)*B, and
./pzgstrs.c:669: * the permutation to B1 by Pc*Pr is applied internally in this routine.
./pzgstrs.c:683: *        A1 = Pc*Pr*diag(R)*A*diag(C)*Pc^T = L*U
./pzgstrs.c:686: *        The 2D process mesh. It contains the MPI communicator, the number
./pzgstrs.c:687: *        of process rows (NPROW), the number of process columns (NPCOL),
./pzgstrs.c:688: *        and my process rank. It is an input argument to all the
./pzgstrs.c:723: * </pre>       
./pzgstrs.c:768:    int    Pc, Pr, iam;
./pzgstrs.c:769:    int    knsupc, nsupr, nprobe;
./pzgstrs.c:786:    			 Count the number of local block products to
./pzgstrs.c:793:    			 from processes in this row. 
./pzgstrs.c:794:    			 It is only valid on the diagonal processes. */
./pzgstrs.c:809:    			 processes in this row. */
./pzgstrs.c:823:    int_t tmpresult;
./pzgstrs.c:825:    // #if ( PROFlevel>=1 )
./pzgstrs.c:839:	int_t procs = grid->nprow * grid->npcol;
./pzgstrs.c:856:	#pragma omp threadprivate(thread_id)
./pzgstrs.c:860:#pragma omp parallel default(shared)
./pzgstrs.c:869:#if ( PRNTlevel>=1 )
./pzgstrs.c:871:	printf("num_thread: %5d\n", num_thread);
./pzgstrs.c:894:    Pr = grid->nprow;
./pzgstrs.c:904:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pzgstrs.c:927:    if ( !(leaf_send = intMalloc_dist((CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))*aln_i)) )
./pzgstrs.c:930:    if ( !(root_send = intMalloc_dist((CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))*aln_i)) )
./pzgstrs.c:941:    /* Obtain ilsum[] and ldalsum for process column 0. */
./pzgstrs.c:954:#pragma omp parallel default(shared) private(ii)
./pzgstrs.c:974:#pragma omp parallel default(shared) private(ii)
./pzgstrs.c:1002:    /* Redistribute B into X on the diagonal processes. */
./pzgstrs.c:1006:#if ( PRNTlevel>=1 )
./pzgstrs.c:1008:    if ( !iam) printf(".. B to X redistribute time\t%8.4f\n", t);
./pzgstrs.c:1015:	#pragma omp simd lastprivate(krow,lk,il)
./pzgstrs.c:1018:	krow = PROW( k, grid );
./pzgstrs.c:1022:	    lsum[il - LSUM_H].r = k;/* Block number prepended in the header.*/
./pzgstrs.c:1028:	   Initialize the async Bcast trees on all processes.
./pzgstrs.c:1035:			// printf("LBtree_ptr lk %5d\n",lk); 
./pzgstrs.c:1044:	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pzgstrs.c:1054:if(procs==1){
./pzgstrs.c:1056:		gb = myrow+lk*grid->nprow;  /* not sure */
./pzgstrs.c:1072:			gb = myrow+lk*grid->nprow;  /* not sure */
./pzgstrs.c:1075:				if(mycol==kcol) { /* Diagonal process */
./pzgstrs.c:1088:#pragma omp simd
./pzgstrs.c:1097:	printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n,  nbtree %4d\n,  nrtree %4d\n",
./pzgstrs.c:1102:#if ( PRNTlevel>=1 )
./pzgstrs.c:1104:	if ( !iam) printf(".. Setup L-solve time\t%8.4f\n", t);
./pzgstrs.c:1121:	   Solve the leaf nodes first by all the diagonal processes.
./pzgstrs.c:1124:	printf("(%2d) nleaf %4d\n", iam, nleaf);
./pzgstrs.c:1131:#pragma omp parallel default (shared) 
./pzgstrs.c:1139:#pragma	omp	for firstprivate(nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nowait	
./pzgstrs.c:1145:				// #pragma	omp	task firstprivate (k,nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Linv,i,lib,rtemp_loc)	 	
./pzgstrs.c:1149:#if ( PROFlevel>=1 )
./pzgstrs.c:1163:					nsupr = lsub[1];
./pzgstrs.c:1181:					#pragma omp simd
./pzgstrs.c:1188:					// printf("x_l: %f %f\n",x[ii+i].r,x[ii+i].i);
./pzgstrs.c:1193:#if ( PROFlevel>=1 )
./pzgstrs.c:1205:					printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs.c:1209:					 * Send Xk to process column Pc[k].
./pzgstrs.c:1217:#pragma omp atomic capture
./pzgstrs.c:1227:#pragma	omp	for firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nowait	
./pzgstrs.c:1233:#if ( PROFlevel>=1 )
./pzgstrs.c:1247:					nsupr = lsub[1];
./pzgstrs.c:1252:							lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:1255:							lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
./pzgstrs.c:1258:							lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:1263:					// printf("x_l: %f %f\n",x[ii+i].r,x[ii+i].i);
./pzgstrs.c:1268:#if ( PROFlevel>=1 )
./pzgstrs.c:1280:					printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs.c:1284:					 * Send Xk to process column Pc[k].
./pzgstrs.c:1292:#pragma omp atomic capture
./pzgstrs.c:1306:#pragma omp parallel default (shared)
./pzgstrs.c:1312:#pragma omp master
./pzgstrs.c:1317:#pragma	omp	taskloop private (k,ii,lk) num_tasks(num_thread*8) nogroup
./pzgstrs.c:1324:							/* Diagonal process */
./pzgstrs.c:1335:						// } /* if diagonal process ... */
./pzgstrs.c:1363:			   Compute the internal nodes asynchronously by all processes.
./pzgstrs.c:1367:#pragma omp parallel default (shared) 
./pzgstrs.c:1371:#pragma omp master 
./pzgstrs.c:1376:#if ( PROFlevel>=1 )
./pzgstrs.c:1390:						// #pragma omp taskyield
./pzgstrs.c:1393:#if ( PROFlevel>=1 )		 
./pzgstrs.c:1406:							printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pzgstrs.c:1429:										krow = PROW( k, grid );
./pzgstrs.c:1464:								// #pragma omp atomic capture
./pzgstrs.c:1476:												#pragma omp simd
./pzgstrs.c:1486:													#pragma omp simd
./pzgstrs.c:1497:											nsupr = lsub[1];
./pzgstrs.c:1499:#if ( PROFlevel>=1 )
./pzgstrs.c:1519:													#pragma omp simd
./pzgstrs.c:1528:														lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:1531:														lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
./pzgstrs.c:1534:														lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:1538:#if ( PROFlevel>=1 )
./pzgstrs.c:1546:											printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs.c:1550:											 * Send Xk to process column Pc[k].
./pzgstrs.c:1564:												krow = PROW( k, grid );
./pzgstrs.c:1582:												#pragma omp simd
./pzgstrs.c:1602:#if ( PRNTlevel>=1 )
./pzgstrs.c:1606:			printf(".. L-solve time\t%8.4f\n", t);
./pzgstrs.c:1614:			printf(".. L-solve time (MAX) \t%8.4f\n", tmax);	
./pzgstrs.c:1625:			printf("(%d) .. After L-solve: y =\n", iam);
./pzgstrs.c:1627:				krow = PROW( k, grid );
./pzgstrs.c:1629:				if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pzgstrs.c:1634:						printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pzgstrs.c:1675:		 * on the diagonal processes.
./pzgstrs.c:1694:#pragma omp parallel default(shared) private(ii)
./pzgstrs.c:1701:	#pragma omp simd lastprivate(krow,lk,il)
./pzgstrs.c:1704:	krow = PROW( k, grid );
./pzgstrs.c:1708:	    lsum[il - LSUM_H].r = k;/* Block number prepended in the header.*/
./pzgstrs.c:1715:		krow = PROW( k, grid );
./pzgstrs.c:1732:		for (p = 0; p < Pr*Pc; ++p) {
./pzgstrs.c:1734:				printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pzgstrs.c:1736:					printf("(%2d) Local col %2d: # row blocks %2d\n",
./pzgstrs.c:1740:							printf("(%2d) .. row blk %2d:\
./pzgstrs.c:1751:		for (p = 0; p < Pr*Pc; ++p) {
./pzgstrs.c:1753:				printf("\n(%d) bsendx_plist[][]", iam);
./pzgstrs.c:1755:					printf("\n(%d) .. local col %2d: ", iam, lb);
./pzgstrs.c:1756:					for (i = 0; i < Pr; ++i)
./pzgstrs.c:1757:						printf("%4d", bsendx_plist[lb][i]);
./pzgstrs.c:1759:				printf("\n");
./pzgstrs.c:1769:	   Initialize the async Bcast trees on all processes.
./pzgstrs.c:1776:			// printf("UBtree_ptr lk %5d\n",lk); 
./pzgstrs.c:1785:	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pzgstrs.c:1793:			// printf("here lk %5d myid %5d\n",lk,iam);
./pzgstrs.c:1800:			gb = myrow+lk*grid->nprow;  /* not sure */
./pzgstrs.c:1803:				if(mycol==kcol) { /* Diagonal process */
./pzgstrs.c:1814:	#pragma omp simd
./pzgstrs.c:1817:	// for (i = 0; i < nlb; ++i)printf("bmod[i]: %5d\n",bmod[i]);
./pzgstrs.c:1825:	printf("(%2d) nbrecvx %4d,  nbrecvmod %4d,  nroot %4d\n,  nbtree %4d\n,  nrtree %4d\n",
./pzgstrs.c:1831:#if ( PRNTlevel>=1 )
./pzgstrs.c:1833:	if ( !iam) printf(".. Setup U-solve time\t%8.4f\n", t);
./pzgstrs.c:1840:		 * Solve the roots first by all the diagonal processes.
./pzgstrs.c:1843:		printf("(%2d) nroot %4d\n", iam, nroot);
./pzgstrs.c:1850:#pragma omp parallel default (shared) 
./pzgstrs.c:1854:#pragma omp master
./pzgstrs.c:1858:#pragma	omp	taskloop firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,jj,k,knsupc,lk,luptr,lsub,nsupr,lusup,t1,t2,Uinv,i,lib,rtemp_loc,nroot_send_tmp) nogroup		
./pzgstrs.c:1863:#if ( PROFlevel>=1 )
./pzgstrs.c:1879:			nsupr = lsub[1];
./pzgstrs.c:1899:					#pragma omp simd
./pzgstrs.c:1907:						lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:1910:						lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
./pzgstrs.c:1913:						lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:1917:			// printf("x_u: %f %f\n",x[ii+i].r,x[ii+i].i);
./pzgstrs.c:1922:				// printf("x: %f\n",x[ii+i]);
./pzgstrs.c:1926:#if ( PROFlevel>=1 )
./pzgstrs.c:1934:			printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs.c:1938:			 * Send Xk to process column Pc[k].
./pzgstrs.c:1943:#pragma omp atomic capture
./pzgstrs.c:1955:#pragma omp parallel default (shared) 
./pzgstrs.c:1959:#pragma omp master
./pzgstrs.c:1963:#pragma	omp	taskloop private (ii,jj,k,lk) nogroup		
./pzgstrs.c:2000:		 * Compute the internal nodes asychronously by all processes.
./pzgstrs.c:2004:#pragma omp parallel default (shared) 
./pzgstrs.c:2008:#pragma omp master 
./pzgstrs.c:2012:			// printf("iam %4d nbrecv %4d nbrecvx %4d nbrecvmod %4d\n", iam, nbrecv, nbrecvxnbrecvmod);
./pzgstrs.c:2018:#if ( PROFlevel>=1 )
./pzgstrs.c:2028:#if ( PROFlevel>=1 )		 
./pzgstrs.c:2038:			printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pzgstrs.c:2071:						#pragma omp simd
./pzgstrs.c:2080:			// #pragma omp atomic capture
./pzgstrs.c:2091:								#pragma omp simd
./pzgstrs.c:2101:								#pragma omp simd
./pzgstrs.c:2111:						nsupr = lsub[1];
./pzgstrs.c:2132:								#pragma omp simd
./pzgstrs.c:2140:									lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:2143:									lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
./pzgstrs.c:2146:									lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs.c:2150:#if ( PROFlevel>=1 )
./pzgstrs.c:2158:						printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs.c:2162:						 * Send Xk to process column Pc[k].
./pzgstrs.c:2184:								#pragma omp simd
./pzgstrs.c:2198:#if ( PRNTlevel>=1 )
./pzgstrs.c:2201:		if ( !iam ) printf(".. U-solve time\t%8.4f\n", t);
./pzgstrs.c:2205:			printf(".. U-solve time (MAX) \t%8.4f\n", tmax);	
./pzgstrs.c:2218:			printf("\n(%d) .. After U-solve: x (ON DIAG PROCS) = \n", iam);
./pzgstrs.c:2222:				krow = PROW( k, grid );
./pzgstrs.c:2225:				if ( iam == diag ) { /* Diagonal process. */
./pzgstrs.c:2231:							printf("\t(%d)\t%4d\t%.10f\n",
./pzgstrs.c:2246:#if ( PRNTlevel>=1 )
./pzgstrs.c:2248:		if ( !iam) printf(".. X to B redistribute time\t%8.4f\n", t);
./pzgstrs.c:2262:#if ( PRNTlevel>=2 )
./pzgstrs.c:2263:			if(iam==0)printf("thread %5d gemm %9.5f\n",i,stat_loc[i]->utime[SOL_GEMM]);
./pzgstrs.c:2312:#if ( PROFlevel>=2 )
./pzgstrs.c:2325:				printf ("\tPDGSTRS comm stat:"
./pzgstrs.c:2328:						msg_cnt_sum / Pr / Pc, msg_cnt_max,
./pzgstrs.c:2329:						msg_vol_sum / Pr / Pc * 1e-6, msg_vol_max * 1e-6);
./pzgstrs1.c:4:approvals from U.S. Dept. of Energy) 
./pzgstrs1.c:15: * <pre>
./pzgstrs1.c:24: * </pre>
./pzgstrs1.c:32: * Function prototypes
./pzgstrs1.c:47: * <pre>
./pzgstrs1.c:60: * distributed in the diagonal processes.
./pzgstrs1.c:74: *        The 2D process mesh.
./pzgstrs1.c:81: *              the diagonal processes.
./pzgstrs1.c:93: * </pre>      
./pzgstrs1.c:116:    int_t  Pc, Pr;
./pzgstrs1.c:117:    int    knsupc, nsupr;
./pzgstrs1.c:132:			     processes in this row. */
./pzgstrs1.c:141:			     processes in this row. */
./pzgstrs1.c:166:    Pr = grid->nprow;
./pzgstrs1.c:173:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pzgstrs1.c:202:    /* Compute ilsum[] and ldalsum for process column 0. */
./pzgstrs1.c:223:     * Prepended the block number in the header for lsum[].
./pzgstrs1.c:227:	krow = PROW( k, grid );
./pzgstrs1.c:237:     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
./pzgstrs1.c:245:	    krow = PROW( k, grid );
./pzgstrs1.c:253:	/*PrintInt10("mod_bit", nlb, mod_bit);*/
./pzgstrs1.c:255:	/* Every process receives the count, but it is only useful on the
./pzgstrs1.c:256:	   diagonal processes.  */
./pzgstrs1.c:260:	    krow = PROW( k, grid );
./pzgstrs1.c:264:		if ( mycol == kcol ) { /* diagonal process */
./pzgstrs1.c:274:	    krow = PROW( k, grid );
./pzgstrs1.c:277:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs1.c:279:		    i = 1;  /* Contribution from non-diagonal process. */
./pzgstrs1.c:283:		if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs1.c:287:		    printf("(%2d) frecv[%4d]  %2d\n", iam, k, frecv[lk]);
./pzgstrs1.c:297:       Solve the leaf nodes first by all the diagonal processes.
./pzgstrs1.c:300:    printf("(%2d) nleaf %4d\n", iam, nleaf);
./pzgstrs1.c:303:	krow = PROW( k, grid );
./pzgstrs1.c:305:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pzgstrs1.c:314:		nsupr = lsub[1];
./pzgstrs1.c:317:		      lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:320:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs1.c:323:		       lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:328:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs1.c:332:		 * Send Xk to process column Pc[k].
./pzgstrs1.c:334:		for (p = 0; p < Pr; ++p)
./pzgstrs1.c:347:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs1.c:363:	} /* if diagonal process ... */
./pzgstrs1.c:367:     * Compute the internal nodes asynchronously by all processes.
./pzgstrs1.c:370:    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
./pzgstrs1.c:378:	/* -MPI- FATAL: Remote protocol queue full */
./pzgstrs1.c:390:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pzgstrs1.c:431:		  nsupr = lsub[1];
./pzgstrs1.c:434:			lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:437:			 lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs1.c:440:			 lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:444:		  printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs1.c:448:		   * Send Xk to process column Pc[k].
./pzgstrs1.c:451:		  for (p = 0; p < Pr; ++p)
./pzgstrs1.c:463:			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs1.c:484:	      printf("(%2d) Recv'd wrong message tag %4d\n", iam,  status.MPI_TAG);
./pzgstrs1.c:492:#if ( PRNTlevel>=2 )
./pzgstrs1.c:494:    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
./pzgstrs1.c:499:    if ( !iam ) printf("\n.. After L-solve: y =\n");
./pzgstrs1.c:501:	krow = PROW( k, grid );
./pzgstrs1.c:503:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pzgstrs1.c:508:		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pzgstrs1.c:528:     * on the diagonal processes.
./pzgstrs1.c:541:     * Compute brecv[] and nbrecvmod counts on the diagonal processes.
./pzgstrs1.c:549:	    krow = PROW( k, grid );
./pzgstrs1.c:552:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs1.c:558:	/* Every process receives the count, but it is only useful on the
./pzgstrs1.c:559:	   diagonal processes.  */
./pzgstrs1.c:563:	    krow = PROW( k, grid );
./pzgstrs1.c:566:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs1.c:567:		if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs1.c:571:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pzgstrs1.c:581:	    krow = PROW( k, grid );
./pzgstrs1.c:584:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs1.c:586:		    i = 1;  /* Contribution from non-diagonal process. */
./pzgstrs1.c:590:		if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs1.c:594:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pzgstrs1.c:605:	krow = PROW( k, grid );
./pzgstrs1.c:675:    for (p = 0; p < Pr*Pc; ++p) {
./pzgstrs1.c:677:	    printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pzgstrs1.c:679:		printf("(%2d) Local col %2d: # row blocks %2d\n",
./pzgstrs1.c:683:			printf("(%2d) .. row blk %2d:\
./pzgstrs1.c:694:    for (p = 0; p < Pr*Pc; ++p) {
./pzgstrs1.c:696:	    printf("\n(%d) bsendx_plist[][]", iam);
./pzgstrs1.c:698:		printf("\n(%d) .. local col %2d: ", iam, lb);
./pzgstrs1.c:699:		for (i = 0; i < Pr; ++i)
./pzgstrs1.c:700:		    printf("%4d", bsendx_plist[lb][i]);
./pzgstrs1.c:702:	    printf("\n");
./pzgstrs1.c:709:#if ( PRNTlevel>=2 )
./pzgstrs1.c:711:    if ( !iam) printf(".. Setup U-solve time\t%8.2f\n", t);
./pzgstrs1.c:716:     * Solve the roots first by all the diagonal processes.
./pzgstrs1.c:719:    printf("(%2d) nroot %4d\n", iam, nroot);
./pzgstrs1.c:722:	krow = PROW( k, grid );
./pzgstrs1.c:724:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process. */
./pzgstrs1.c:733:		nsupr = lsub[1];
./pzgstrs1.c:736:		      lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:739:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs1.c:742:		       lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:747:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs1.c:750:		 * Send Xk to process column Pc[k].
./pzgstrs1.c:752:		for (p = 0; p < Pr; ++p)
./pzgstrs1.c:764:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs1.c:777:	} /* if diagonal process ... */
./pzgstrs1.c:782:     * Compute the internal nodes asynchronously by all processes.
./pzgstrs1.c:792:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pzgstrs1.c:825:		    nsupr = lsub[1];
./pzgstrs1.c:828:			  lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:831:			   lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs1.c:834:			   lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs1.c:838:		    printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs1.c:841:		     * Send Xk to process column Pc[k].
./pzgstrs1.c:844:		    for (p = 0; p < Pr; ++p)
./pzgstrs1.c:856:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs1.c:875:		printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
./pzgstrs1.c:883:#if ( PRNTlevel>=2 )
./pzgstrs1.c:885:    if ( !iam ) printf(".. U-solve time\t%8.2f\n", t);
./pzgstrs_Bglobal.c:4:approvals from U.S. Dept. of Energy) 
./pzgstrs_Bglobal.c:15: * <pre>
./pzgstrs_Bglobal.c:24: * </pre>
./pzgstrs_Bglobal.c:32: * Function prototypes
./pzgstrs_Bglobal.c:49: * <pre>
./pzgstrs_Bglobal.c:70: *        The 2D process mesh. It contains the MPI communicator, the number
./pzgstrs_Bglobal.c:71: *        of process rows (NPROW), the number of process columns (NPCOL),
./pzgstrs_Bglobal.c:72: *        and my process rank. It is an input argument to all the
./pzgstrs_Bglobal.c:84: *              processes when calling this routine.
./pzgstrs_Bglobal.c:99: * </pre>    
./pzgstrs_Bglobal.c:125:    int    Pc, Pr, iam;
./pzgstrs_Bglobal.c:126:    int    knsupc, nsupr;
./pzgstrs_Bglobal.c:141:			     processes in this row. */
./pzgstrs_Bglobal.c:150:			     processes in this row. */
./pzgstrs_Bglobal.c:175:    Pr = grid->nprow;
./pzgstrs_Bglobal.c:182:    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
./pzgstrs_Bglobal.c:212:    /* Obtain ilsum[] and ldalsum for process column 0. */
./pzgstrs_Bglobal.c:236:     * Copy B into X on the diagonal processes.
./pzgstrs_Bglobal.c:241:	krow = PROW( k, grid );
./pzgstrs_Bglobal.c:245:	    lsum[il - LSUM_H].r = k;/* Block number prepended in the header. */
./pzgstrs_Bglobal.c:248:	    if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs_Bglobal.c:250:		x[jj - XK_H].r = k; /* Block number prepended in the header. */
./pzgstrs_Bglobal.c:261:     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
./pzgstrs_Bglobal.c:269:	    krow = PROW( k, grid );
./pzgstrs_Bglobal.c:278:	/* Every process receives the count, but it is only useful on the
./pzgstrs_Bglobal.c:279:	   diagonal processes.  */
./pzgstrs_Bglobal.c:283:	    krow = PROW( k, grid );
./pzgstrs_Bglobal.c:287:		if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs_Bglobal.c:297:	    krow = PROW( k, grid );
./pzgstrs_Bglobal.c:300:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs_Bglobal.c:302:		    i = 1;  /* Contribution from non-diagonal process. */
./pzgstrs_Bglobal.c:306:		if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs_Bglobal.c:310:		    printf("(%2d) frecv[%4d]  %2d\n", iam, k, frecv[lk]);
./pzgstrs_Bglobal.c:320:       Solve the leaf nodes first by all the diagonal processes.
./pzgstrs_Bglobal.c:323:    printf("(%2d) nleaf %4d\n", iam, nleaf);
./pzgstrs_Bglobal.c:326:	krow = PROW( k, grid );
./pzgstrs_Bglobal.c:328:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pzgstrs_Bglobal.c:337:		nsupr = lsub[1];
./pzgstrs_Bglobal.c:340:		      lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:343:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs_Bglobal.c:346:		       lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:352:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs_Bglobal.c:356:		 * Send Xk to process column Pc[k].
./pzgstrs_Bglobal.c:358:		for (p = 0; p < Pr; ++p) {
./pzgstrs_Bglobal.c:377:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs_Bglobal.c:393:	} /* if diagonal process ... */
./pzgstrs_Bglobal.c:397:       Compute the internal nodes asynchronously by all processes.
./pzgstrs_Bglobal.c:400:    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
./pzgstrs_Bglobal.c:411:	/* -MPI- FATAL: Remote protocol queue full */
./pzgstrs_Bglobal.c:422:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pzgstrs_Bglobal.c:447:	  case LSUM: /* Receiver must be a diagonal process */
./pzgstrs_Bglobal.c:464:		  nsupr = lsub[1];
./pzgstrs_Bglobal.c:467:			lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:470:			 lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs_Bglobal.c:473:			 lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:479:		  printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs_Bglobal.c:483:		   * Send Xk to process column Pc[k].
./pzgstrs_Bglobal.c:486:		  for (p = 0; p < Pr; ++p) {
./pzgstrs_Bglobal.c:503:			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs_Bglobal.c:524:	      printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
./pzgstrs_Bglobal.c:532:#if ( PRNTlevel>=2 )
./pzgstrs_Bglobal.c:534:    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
./pzgstrs_Bglobal.c:539:    printf("\n(%d) .. After L-solve: y =\n", iam);
./pzgstrs_Bglobal.c:541:	krow = PROW( k, grid );
./pzgstrs_Bglobal.c:543:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
./pzgstrs_Bglobal.c:548:		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
./pzgstrs_Bglobal.c:568:     * on the diagonal processes.
./pzgstrs_Bglobal.c:581:     * Compute brecv[] and nbrecvmod counts on the diagonal processes.
./pzgstrs_Bglobal.c:589:	    krow = PROW( k, grid );
./pzgstrs_Bglobal.c:592:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs_Bglobal.c:598:	/* Every process receives the count, but it is only useful on the
./pzgstrs_Bglobal.c:599:	   diagonal processes.  */
./pzgstrs_Bglobal.c:603:	    krow = PROW( k, grid );
./pzgstrs_Bglobal.c:606:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs_Bglobal.c:607:		if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs_Bglobal.c:611:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pzgstrs_Bglobal.c:621:	    krow = PROW( k, grid );
./pzgstrs_Bglobal.c:624:		kcol = PCOL( k, grid ); /* Root process in this row scope. */
./pzgstrs_Bglobal.c:626:		    i = 1;  /* Contribution from non-diagonal process. */
./pzgstrs_Bglobal.c:630:		if ( mycol == kcol ) { /* Diagonal process. */
./pzgstrs_Bglobal.c:634:		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
./pzgstrs_Bglobal.c:645:	krow = PROW( k, grid );
./pzgstrs_Bglobal.c:716:    for (p = 0; p < Pr*Pc; ++p) {
./pzgstrs_Bglobal.c:718:	    printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
./pzgstrs_Bglobal.c:720:		printf("(%2d) Local col %2d: # row blocks %2d\n",
./pzgstrs_Bglobal.c:724:			printf("(%2d) .. row blk %2d:\
./pzgstrs_Bglobal.c:735:    for (p = 0; p < Pr*Pc; ++p) {
./pzgstrs_Bglobal.c:737:	    printf("\n(%d) bsendx_plist[][]", iam);
./pzgstrs_Bglobal.c:739:		printf("\n(%d) .. local col %2d: ", iam, lb);
./pzgstrs_Bglobal.c:740:		for (i = 0; i < Pr; ++i)
./pzgstrs_Bglobal.c:741:		    printf("%4d", bsendx_plist[lb][i]);
./pzgstrs_Bglobal.c:743:	    printf("\n");
./pzgstrs_Bglobal.c:750:#if ( PRNTlevel>=2 )
./pzgstrs_Bglobal.c:752:    if ( !iam) printf(".. Setup U-solve time\t%8.2f\n", t);
./pzgstrs_Bglobal.c:757:     * Solve the roots first by all the diagonal processes.
./pzgstrs_Bglobal.c:760:    printf("(%2d) nroot %4d\n", iam, nroot);
./pzgstrs_Bglobal.c:763:	krow = PROW( k, grid );
./pzgstrs_Bglobal.c:765:	if ( myrow == krow && mycol == kcol ) { /* Diagonal process. */
./pzgstrs_Bglobal.c:774:		nsupr = lsub[1];
./pzgstrs_Bglobal.c:777:		      lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:780:		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs_Bglobal.c:783:		       lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:789:		printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs_Bglobal.c:792:		 * Send Xk to process column Pc[k].
./pzgstrs_Bglobal.c:794:		for (p = 0; p < Pr; ++p) {
./pzgstrs_Bglobal.c:811:			printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs_Bglobal.c:824:	} /* if diagonal process ... */
./pzgstrs_Bglobal.c:829:     * Compute the internal nodes asynchronously by all processes.
./pzgstrs_Bglobal.c:840:	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
./pzgstrs_Bglobal.c:857:	    case LSUM: /* Receiver must be a diagonal process */
./pzgstrs_Bglobal.c:874:		    nsupr = lsub[1];
./pzgstrs_Bglobal.c:877:			  lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:880:			   lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
./pzgstrs_Bglobal.c:883:			   lusup, &nsupr, &x[ii], &knsupc);
./pzgstrs_Bglobal.c:888:		    printf("(%2d) Solve X[%2d]\n", iam, k);
./pzgstrs_Bglobal.c:891:		     * Send Xk to process column Pc[k].
./pzgstrs_Bglobal.c:894:		    for (p = 0; p < Pr; ++p) {
./pzgstrs_Bglobal.c:911:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs_Bglobal.c:930:		printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
./pzgstrs_Bglobal.c:938:#if ( PRNTlevel>=2 )
./pzgstrs_Bglobal.c:940:    if ( !iam ) printf(".. U-solve time\t%8.2f\n", t);
./pzgstrs_Bglobal.c:944:    /* Copy the solution X into B (on all processes). */
./pzgstrs_Bglobal.c:946:	int_t num_diag_procs, *diag_procs, *diag_len;
./pzgstrs_Bglobal.c:949:	get_diag_procs(n, Glu_persist, grid, &num_diag_procs,
./pzgstrs_Bglobal.c:950:		       &diag_procs, &diag_len);
./pzgstrs_Bglobal.c:952:	for (j = 1; j < num_diag_procs; ++j) jj = SUPERLU_MAX(jj, diag_len[j]);
./pzgstrs_Bglobal.c:956:			   grid, num_diag_procs, diag_procs, diag_len,
./pzgstrs_Bglobal.c:958:	SUPERLU_FREE(diag_procs);
./pzgstrs_Bglobal.c:996: * Gather the components of x vector on the diagonal processes
./pzgstrs_Bglobal.c:997: * onto all processes, and combine them into the global vector y.
./pzgstrs_Bglobal.c:1002:		   gridinfo_t *grid, int_t num_diag_procs,
./pzgstrs_Bglobal.c:1003:		   int_t diag_procs[], int_t diag_len[],
./pzgstrs_Bglobal.c:1016:    for (p = 0; p < num_diag_procs; ++p) {
./pzgstrs_Bglobal.c:1017:	pkk = diag_procs[p];
./pzgstrs_Bglobal.c:1021:	    for (k = p; k < nsupers; k += num_diag_procs) {
./pzgstrs_Bglobal.c:1038:	for (k = p; k < nsupers; k += num_diag_procs) {
./pzgstrs_lsum.c:4:approvals from U.S. Dept. of Energy) 
./pzgstrs_lsum.c:15: * <pre>
./pzgstrs_lsum.c:23: * </pre>
./pzgstrs_lsum.c:36: * Function prototypes
./pzgstrs_lsum.c:51: * <pre>
./pzgstrs_lsum.c:55: * </pre>
./pzgstrs_lsum.c:81:    int    iam, iknsupc, myrow, nbrow, nsupr, nsupr1, p, pi;
./pzgstrs_lsum.c:90:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:94:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:103:    nsupr = lsub[1];
./pzgstrs_lsum.c:110:	      &alpha, &lusup[luptr], &nsupr, xk,
./pzgstrs_lsum.c:114:	       &alpha, &lusup[luptr], &nsupr, xk,
./pzgstrs_lsum.c:118:	       &alpha, &lusup[luptr], &nsupr, xk,
./pzgstrs_lsum.c:138:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:161:		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pzgstrs_lsum.c:164:	    } else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:176:		    nsupr1 = lsub1[1];
./pzgstrs_lsum.c:177:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:182:			  lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:185:			   lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);
./pzgstrs_lsum.c:188:			   lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:190:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:198:		    printf("(%2d) Solve X[%2d]\n", iam, ik);
./pzgstrs_lsum.c:202:		     * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:204:		    for (p = 0; p < grid->nprow; ++p) {
./pzgstrs_lsum.c:221:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs_lsum.c:271:    int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
./pzgstrs_lsum.c:298:	gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzgstrs_lsum.c:340:		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pzgstrs_lsum.c:343:	    } else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:355:		    nsupr = lsub[1];
./pzgstrs_lsum.c:358:			  lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:361:			   lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);
./pzgstrs_lsum.c:364:			   lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:369:		    printf("(%2d) Solve X[%2d]\n", iam, gik);
./pzgstrs_lsum.c:373:		     * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:375:		    for (p = 0; p < grid->nprow; ++p) {
./pzgstrs_lsum.c:392:			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
./pzgstrs_lsum.c:417: * <pre>
./pzgstrs_lsum.c:421: * </pre>
./pzgstrs_lsum.c:451:	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r,m;
./pzgstrs_lsum.c:491:	// #if ( PROFlevel>=1 )
./pzgstrs_lsum.c:504:		nsupr = lsub[1];
./pzgstrs_lsum.c:506:		// printf("nlb: %5d lk: %5d\n",nlb,lk);
./pzgstrs_lsum.c:509:		krow = PROW( k, grid );
./pzgstrs_lsum.c:515:			m = nsupr-knsupc;
./pzgstrs_lsum.c:521:			m = nsupr;
./pzgstrs_lsum.c:536:#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j,nleaf_send_tmp) untied nogroup	
./pzgstrs_lsum.c:557:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:569:						  &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pzgstrs_lsum.c:573:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pzgstrs_lsum.c:577:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pzgstrs_lsum.c:596:							#pragma omp simd							
./pzgstrs_lsum.c:607:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:615:#pragma omp atomic capture
./pzgstrs_lsum.c:634:									#pragma omp simd							
./pzgstrs_lsum.c:642:#pragma omp atomic capture
./pzgstrs_lsum.c:648:							} else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:650:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:655:									#pragma omp simd							
./pzgstrs_lsum.c:665:									#pragma omp simd							
./pzgstrs_lsum.c:677:								nsupr1 = lsub1[1];
./pzgstrs_lsum.c:697:									#pragma omp simd							
./pzgstrs_lsum.c:706:											lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:709:											lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
./pzgstrs_lsum.c:712:											lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:717:								// printf("x_lsum: %f %f\n",x[ii+i].r,x[ii+i].i);
./pzgstrs_lsum.c:721:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:731:								printf("(%2d) Solve X[%2d]\n", iam, ik);
./pzgstrs_lsum.c:736:								 * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:741:#pragma omp atomic capture
./pzgstrs_lsum.c:752:								// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
./pzgstrs_lsum.c:771:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:777:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pzgstrs_lsum.c:781:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pzgstrs_lsum.c:785:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pzgstrs_lsum.c:809:					#pragma omp simd							
./pzgstrs_lsum.c:823:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:832:#pragma omp atomic capture
./pzgstrs_lsum.c:851:							#pragma omp simd							
./pzgstrs_lsum.c:859:#pragma omp atomic capture
./pzgstrs_lsum.c:864:					} else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:866:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:871:							#pragma omp simd							
./pzgstrs_lsum.c:881:							#pragma omp simd							
./pzgstrs_lsum.c:892:						nsupr1 = lsub1[1];
./pzgstrs_lsum.c:910:							#pragma omp simd							
./pzgstrs_lsum.c:918:									lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:921:									lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
./pzgstrs_lsum.c:924:									lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:929:							// printf("x_lsum: %f %f\n",x[ii+i].r,x[ii+i].i);
./pzgstrs_lsum.c:934:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:943:						printf("(%2d) Solve X[%2d]\n", iam, ik);
./pzgstrs_lsum.c:947:						 * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:953:#pragma omp atomic capture
./pzgstrs_lsum.c:956:							// printf("nleaf_send_tmp %5d lk %5d\n",nleaf_send_tmp);
./pzgstrs_lsum.c:966:						// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1) untied priority(1) 	
./pzgstrs_lsum.c:992: * <pre>
./pzgstrs_lsum.c:996: * </pre>
./pzgstrs_lsum.c:1026:	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r;
./pzgstrs_lsum.c:1057:	// #if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1068:		// printf("ya1 %5d k %5d lk %5d\n",thread_id,k,lk);
./pzgstrs_lsum.c:1073:		// printf("ya2 %5d k %5d lk %5d\n",thread_id,k,lk);
./pzgstrs_lsum.c:1080:		nsupr = lsub[1];
./pzgstrs_lsum.c:1082:		// printf("nlb: %5d lk: %5d\n",nlb,lk);
./pzgstrs_lsum.c:1085:		krow = PROW( k, grid );
./pzgstrs_lsum.c:1091:			m = nsupr-knsupc;
./pzgstrs_lsum.c:1097:			m = nsupr;
./pzgstrs_lsum.c:1110:#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied
./pzgstrs_lsum.c:1131:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1143:						  &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pzgstrs_lsum.c:1147:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pzgstrs_lsum.c:1151:						   &alpha, &lusup[luptr_tmp1], &nsupr, xk,
./pzgstrs_lsum.c:1170:								#pragma omp simd lastprivate(irow)
./pzgstrs_lsum.c:1181:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1190:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1196:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pzgstrs_lsum.c:1200:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pzgstrs_lsum.c:1204:					&alpha, &lusup[luptr_tmp], &nsupr, xk,
./pzgstrs_lsum.c:1228:						#pragma omp simd lastprivate(irow)
./pzgstrs_lsum.c:1239:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1251:			// #pragma omp atomic capture
./pzgstrs_lsum.c:1280:							#pragma omp simd
./pzgstrs_lsum.c:1291:				} else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:1297:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1303:							#pragma omp simd
./pzgstrs_lsum.c:1314:							#pragma omp simd 
./pzgstrs_lsum.c:1325:					nsupr1 = lsub1[1];
./pzgstrs_lsum.c:1343:							#pragma omp simd 
./pzgstrs_lsum.c:1351:								lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:1354:								lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
./pzgstrs_lsum.c:1357:								lusup1, &nsupr1, &x[ii], &iknsupc);
./pzgstrs_lsum.c:1361:					// printf("x_usum: %f %f\n",x[ii+i].r,x[ii+i].i);
./pzgstrs_lsum.c:1365:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1375:					printf("(%2d) Solve X[%2d]\n", iam, ik);
./pzgstrs_lsum.c:1379:					 * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:1390:					// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
./pzgstrs_lsum.c:1446:	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
./pzgstrs_lsum.c:1490:		// printf("Unnz: %5d nub: %5d knsupc: %5d\n",Llu->Unnz[lk],nub,knsupc);
./pzgstrs_lsum.c:1492:#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr) untied nogroup	
./pzgstrs_lsum.c:1517:				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzgstrs_lsum.c:1522:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1535:							#pragma omp simd							
./pzgstrs_lsum.c:1550:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1557:		#pragma omp atomic capture
./pzgstrs_lsum.c:1568:							#pragma omp simd							
./pzgstrs_lsum.c:1576:#pragma omp atomic capture
./pzgstrs_lsum.c:1583:						printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pzgstrs_lsum.c:1586:					} else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:1588:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1595:							#pragma omp simd							
./pzgstrs_lsum.c:1607:							#pragma omp simd							
./pzgstrs_lsum.c:1619:							nsupr = lsub[1];
./pzgstrs_lsum.c:1637:								#pragma omp simd							
./pzgstrs_lsum.c:1645:										lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:1648:										lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
./pzgstrs_lsum.c:1651:										lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:1655:								// printf("x_usum: %f %f\n",x[ii+i].r,x[ii+i].i);
./pzgstrs_lsum.c:1659:		#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1667:							printf("(%2d) Solve X[%2d]\n", iam, gik);
./pzgstrs_lsum.c:1671:							 * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:1675:								// printf("xre: %f\n",x[ii+i]);
./pzgstrs_lsum.c:1680:#pragma omp atomic capture
./pzgstrs_lsum.c:1692:								// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
./pzgstrs_lsum.c:1717:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzgstrs_lsum.c:1722:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1734:						#pragma omp simd							
./pzgstrs_lsum.c:1749:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1755:	#pragma omp atomic capture
./pzgstrs_lsum.c:1766:						#pragma omp simd							
./pzgstrs_lsum.c:1773:#pragma omp atomic capture
./pzgstrs_lsum.c:1780:					printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pzgstrs_lsum.c:1783:				} else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:1785:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1792:						#pragma omp simd							
./pzgstrs_lsum.c:1804:						#pragma omp simd							
./pzgstrs_lsum.c:1816:						nsupr = lsub[1];
./pzgstrs_lsum.c:1834:							#pragma omp simd							
./pzgstrs_lsum.c:1842:									lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:1845:									lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
./pzgstrs_lsum.c:1848:									lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:1852:	#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:1859:						printf("(%2d) Solve X[%2d]\n", iam, gik);
./pzgstrs_lsum.c:1863:						 * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:1867:							// printf("xre: %f\n",x[ii+i]);
./pzgstrs_lsum.c:1872:#pragma omp atomic capture
./pzgstrs_lsum.c:1886:							// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
./pzgstrs_lsum.c:1940:	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
./pzgstrs_lsum.c:1981:	// printf("Urbs2[lk] %5d lk %5d nub %5d\n",Urbs2[lk],lk,nub);
./pzgstrs_lsum.c:1991:#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,gik,usub,uval,iknsupc,il,i,irow,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz) untied	
./pzgstrs_lsum.c:2002:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:2020:				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzgstrs_lsum.c:2034:							#pragma omp simd							
./pzgstrs_lsum.c:2049:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:2057:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:2067:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzgstrs_lsum.c:2081:						#pragma omp simd							
./pzgstrs_lsum.c:2096:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:2107:		gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzgstrs_lsum.c:2111:	// #pragma omp atomic capture
./pzgstrs_lsum.c:2122:					#pragma omp simd							
./pzgstrs_lsum.c:2131:				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
./pzgstrs_lsum.c:2134:			} else { /* Diagonal process: X[i] += lsum[i]. */
./pzgstrs_lsum.c:2136:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:2142:					#pragma omp simd							
./pzgstrs_lsum.c:2154:					#pragma omp simd							
./pzgstrs_lsum.c:2166:					nsupr = lsub[1];
./pzgstrs_lsum.c:2184:						#pragma omp simd							
./pzgstrs_lsum.c:2192:								lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:2195:								lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
./pzgstrs_lsum.c:2198:								lusup, &nsupr, &x[ii], &iknsupc);
./pzgstrs_lsum.c:2202:#if ( PROFlevel>=1 )
./pzgstrs_lsum.c:2209:					printf("(%2d) Solve X[%2d]\n", iam, gik);
./pzgstrs_lsum.c:2213:					 * Send Xk to process column Pc[k].
./pzgstrs_lsum.c:2217:						// printf("xre: %f\n",x[ii+i]);
./pzgstrs_lsum.c:2229:						// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
./pzlangs.c:4:approvals from U.S. Dept. of Energy) 
./pzlangs.c:15: * <pre>
./pzlangs.c:18: * </pre>
./pzlangs.c:25:<pre> 
./pzlangs.c:59:            The 2D process mesh.
./pzlangs.c:61:</pre>
./pzlangs.c:74:    double   *temprwork;
./pzlangs.c:113:	if ( !(temprwork = (double *) doubleCalloc_dist(A->ncol)) )
./pzlangs.c:114:	    ABORT("doubleCalloc_dist fails for temprwork.");
./pzlangs.c:115:	MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);
./pzlangs.c:118:	    value = SUPERLU_MAX(value, temprwork[j]);
./pzlangs.c:120:	SUPERLU_FREE (temprwork);
./pzlaqgs.c:4:approvals from U.S. Dept. of Energy) 
./pzlaqgs.c:15: * <pre>
./pzlaqgs.c:18: * </pre>
./pzlaqgs.c:25:<pre>
./pzlaqgs.c:60:            = 'R':  Row equilibration, i.e., A has been premultiplied by  
./pzlaqgs.c:80:</pre>
./pzlaqgs.c:108:    small = dmach_dist("Safe minimum") / dmach_dist("Precision");
./pzsymbfact_distdata.c:4:approvals from U.S. Dept. of Energy) 
./pzsymbfact_distdata.c:15: * <pre>
./pzsymbfact_distdata.c:25: * </pre>
./pzsymbfact_distdata.c:40: * <pre>
./pzsymbfact_distdata.c:53: * and the supernodes information.  This represents the arrays:
./pzsymbfact_distdata.c:78: *         of processors, stored by columns.
./pzsymbfact_distdata.c:81: *         Structure of L distributed on a 2D grid of processors, 
./pzsymbfact_distdata.c:86: *         of processors, stored by rows.
./pzsymbfact_distdata.c:89: *         Structure of U distributed on a 2D grid of processors, 
./pzsymbfact_distdata.c:93: *        The 2D process mesh.
./pzsymbfact_distdata.c:99: *        (an approximation).
./pzsymbfact_distdata.c:100: * </pre>
./pzsymbfact_distdata.c:110:  int   iam, nprocs, pc, pr, p, np, p_diag;
./pzsymbfact_distdata.c:118:  int_t maxszsn, maxNvtcsPProc;
./pzsymbfact_distdata.c:139:  nprocs = (int) grid->nprow * grid->npcol;
./pzsymbfact_distdata.c:142:  maxNvtcsPProc = Pslu_freeable->maxNvtcsPProc;
./pzsymbfact_distdata.c:153:  mem           = intCalloc_dist(12 * nprocs);
./pzsymbfact_distdata.c:156:  memAux     = (float) (12 * nprocs * sizeof(int_t));
./pzsymbfact_distdata.c:158:  nnzToSend     = nnzToRecv + 2*nprocs;
./pzsymbfact_distdata.c:159:  nnzToSend_l   = nnzToSend + 2 * nprocs;
./pzsymbfact_distdata.c:160:  nnzToSend_u   = nnzToSend_l + nprocs;
./pzsymbfact_distdata.c:161:  send_1        = nnzToSend_u + nprocs;
./pzsymbfact_distdata.c:162:  send_2        = send_1 + nprocs;
./pzsymbfact_distdata.c:163:  tmp_ptrToSend = send_2 + nprocs;
./pzsymbfact_distdata.c:164:  nnzToRecv_l   = tmp_ptrToSend + nprocs;
./pzsymbfact_distdata.c:165:  nnzToRecv_u   = nnzToRecv_l + nprocs;
./pzsymbfact_distdata.c:168:  ptrToRecv = nnzToSend + nprocs;
./pzsymbfact_distdata.c:170:  nvtcs = (int *) SUPERLU_MALLOC(5 * nprocs * sizeof(int));
./pzsymbfact_distdata.c:171:  intBuf1 = nvtcs + nprocs;
./pzsymbfact_distdata.c:172:  intBuf2 = nvtcs + 2 * nprocs;
./pzsymbfact_distdata.c:173:  intBuf3 = nvtcs + 3 * nprocs;
./pzsymbfact_distdata.c:174:  intBuf4 = nvtcs + 4 * nprocs;
./pzsymbfact_distdata.c:175:  memAux += 5 * nprocs * sizeof(int);
./pzsymbfact_distdata.c:181:    fprintf (stderr, "Malloc fails for supno_n[].");
./pzsymbfact_distdata.c:198:       each processor */
./pzsymbfact_distdata.c:199:    for (k = 0, p = 0; p < nprocs; p++) {
./pzsymbfact_distdata.c:205:  if (nprocs > 1) {
./pzsymbfact_distdata.c:209:	fprintf (stderr, "Malloc fails for temp[].");
./pzsymbfact_distdata.c:215:    for (p=0; p<nprocs; p++) {
./pzsymbfact_distdata.c:250:    if (nprocs > 1) {
./pzsymbfact_distdata.c:258:  for (p = 0; p < 2 *nprocs; p++)
./pzsymbfact_distdata.c:265:    fprintf (stderr, "Malloc fails for xsup_n[].");
./pzsymbfact_distdata.c:271:     COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
./pzsymbfact_distdata.c:285:  for (p = 0; p < nprocs; p++) {
./pzsymbfact_distdata.c:293:      pr = PROW( gb_n, grid );
./pzsymbfact_distdata.c:294:      p_diag = PNUM( pr, pc, grid);
./pzsymbfact_distdata.c:305:	  p = (int) PNUM( PROW(gb, grid), pc, grid );
./pzsymbfact_distdata.c:314:	  p = PNUM( pr, PCOL(gb, grid), grid);
./pzsymbfact_distdata.c:321:      for (p = pr * grid->npcol; p < (pr + 1) * grid->npcol; p++) {
./pzsymbfact_distdata.c:325:      for (p = pr * grid->npcol; p < (pr + 1) * grid->npcol; p++) 
./pzsymbfact_distdata.c:334:      for (p = pc; p < nprocs; p += grid->npcol) {
./pzsymbfact_distdata.c:338:      for (p = pc; p < nprocs; p += grid->npcol) 
./pzsymbfact_distdata.c:354:  for (p = 0; p < nprocs; p++) {
./pzsymbfact_distdata.c:370:  nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pzsymbfact_distdata.c:373:    fprintf (stderr, "Malloc fails for xlsub_n[].");
./pzsymbfact_distdata.c:379:    fprintf (stderr, "Malloc fails for xusub_n[].");
./pzsymbfact_distdata.c:388:      fprintf (stderr, "Malloc fails for rcv_luind[].");
./pzsymbfact_distdata.c:394:  if ( nprocs > 1 && (SendCnt_l || SendCnt_u) ) {
./pzsymbfact_distdata.c:396:      fprintf (stderr, "Malloc fails for index[].");
./pzsymbfact_distdata.c:417:    for (i = 0, j = 0, p = 0; p < nprocs; p++) {
./pzsymbfact_distdata.c:431:	pr = PROW( gb_n, grid );
./pzsymbfact_distdata.c:432:	p_diag = PNUM( pr, pc, grid );
./pzsymbfact_distdata.c:441:	  p = pc;                np = grid->nprow;	  
./pzsymbfact_distdata.c:443:	  p = pr * grid->npcol;  np = grid->npcol;
./pzsymbfact_distdata.c:466:	      p = PNUM( PROW(gb, grid), pc, grid );
./pzsymbfact_distdata.c:468:	      p = PNUM( pr, PCOL(gb, grid), grid);
./pzsymbfact_distdata.c:487:	  for (p = pc; p < nprocs; p += grid->npcol) {
./pzsymbfact_distdata.c:490:		if (PNUM(PROW(gb, grid), pc, grid) != p) {
./pzsymbfact_distdata.c:504:	  for (p = pr * grid->npcol; p < (pr + 1) * grid->npcol; p++) {
./pzsymbfact_distdata.c:508:		if(PNUM( pr, PCOL(gb, grid), grid) != p) {
./pzsymbfact_distdata.c:526:       each processor (structure needed in MPI_Alltoallv) */
./pzsymbfact_distdata.c:527:    for (i = 0, p = 0; p < nprocs; p++) {
./pzsymbfact_distdata.c:535:    if (nprocs > 1) {
./pzsymbfact_distdata.c:538:      for (p=0; p<nprocs; p++) {
./pzsymbfact_distdata.c:565:      if ( nprocs > 1 && (SendCnt_l || SendCnt_u) ) {
./pzsymbfact_distdata.c:575:    for (p = 0; p < nprocs; p ++) {
./pzsymbfact_distdata.c:607:	  fprintf (stderr, "Malloc fails for lsub_n[].");
./pzsymbfact_distdata.c:618:	  fprintf (stderr, "Malloc fails for usub_n[].");
./pzsymbfact_distdata.c:628:    for (p = 0; p < nprocs; p++) {
./pzsymbfact_distdata.c:633:	  printf ("Pe[%d] p %d gb " IFMT " nsupers " IFMT " i " IFMT " i-k " IFMT "\n",
./pzsymbfact_distdata.c:657:  memAux -= (float) (12 * nprocs * iword);
./pzsymbfact_distdata.c:659:  memAux -= (float) (5 * nprocs * sizeof(int));
./pzsymbfact_distdata.c:686: * <pre>
./pzsymbfact_distdata.c:689: *   Re-distribute A on the 2D process mesh.  The lower part is
./pzsymbfact_distdata.c:708: *        The 2D process mesh.
./pzsymbfact_distdata.c:712: *         of processors, stored by columns.
./pzsymbfact_distdata.c:716: *         2D grid of processors, stored by columns.
./pzsymbfact_distdata.c:720: *         2D grid of processors, stored by columns.
./pzsymbfact_distdata.c:724: *         of processors, stored by rows.
./pzsymbfact_distdata.c:728: *         2D grid of processors, stored by rows.
./pzsymbfact_distdata.c:732: *         2D grid of processors, stored by rows.
./pzsymbfact_distdata.c:746: *        (an approximation).
./pzsymbfact_distdata.c:747: * </pre>
./pzsymbfact_distdata.c:758:  int    iam, p, procs;
./pzsymbfact_distdata.c:794:  procs = grid->nprow * grid->npcol;
./pzsymbfact_distdata.c:799:  if (!(nnzToRecv = intCalloc_dist(2*procs))) {
./pzsymbfact_distdata.c:800:    fprintf (stderr, "Malloc fails for nnzToRecv[].");
./pzsymbfact_distdata.c:803:  memAux = (float) (2 * procs * iword);
./pzsymbfact_distdata.c:805:  nnzToSend = nnzToRecv + procs;
./pzsymbfact_distdata.c:809:     COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
./pzsymbfact_distdata.c:815:      irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pzsymbfact_distdata.c:819:      p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pzsymbfact_distdata.c:831:  for (p = 0; p < procs; ++p) {
./pzsymbfact_distdata.c:841:  k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
./pzsymbfact_distdata.c:846:    fprintf (stderr, "Malloc fails for ia[].");
./pzsymbfact_distdata.c:852:    fprintf (stderr, "Malloc fails for aij[].");
./pzsymbfact_distdata.c:858:  if ( procs > 1 ) {
./pzsymbfact_distdata.c:860:	   SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) ) {
./pzsymbfact_distdata.c:861:      fprintf (stderr, "Malloc fails for send_req[].");
./pzsymbfact_distdata.c:864:    memAux += (float) (2*procs *sizeof(MPI_Request));
./pzsymbfact_distdata.c:865:    if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) ) {
./pzsymbfact_distdata.c:866:      fprintf(stderr, "Malloc fails for ia_send[].");
./pzsymbfact_distdata.c:869:    memAux += (float) (procs*sizeof(int_t*));
./pzsymbfact_distdata.c:870:    if ( !(aij_send = (doublecomplex **)SUPERLU_MALLOC(procs*sizeof(doublecomplex*))) ) {
./pzsymbfact_distdata.c:871:      fprintf(stderr, "Malloc fails for aij_send[].");
./pzsymbfact_distdata.c:874:    memAux += (float) (procs*sizeof(doublecomplex*));    
./pzsymbfact_distdata.c:876:      fprintf(stderr, "Malloc fails for index[].");
./pzsymbfact_distdata.c:881:      fprintf(stderr, "Malloc fails for nzval[].");
./pzsymbfact_distdata.c:885:    if ( !(ptr_to_send = intCalloc_dist(procs)) ) {
./pzsymbfact_distdata.c:886:      fprintf(stderr, "Malloc fails for ptr_to_send[].");
./pzsymbfact_distdata.c:889:    memAux += (float) (procs * iword);
./pzsymbfact_distdata.c:891:      fprintf(stderr, "Malloc fails for itemp[].");
./pzsymbfact_distdata.c:896:      fprintf(stderr, "Malloc fails for dtemp[].");
./pzsymbfact_distdata.c:901:    for (i = 0, j = 0, p = 0; p < procs; ++p) {
./pzsymbfact_distdata.c:909:  } /* if procs > 1 */
./pzsymbfact_distdata.c:911:  nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./pzsymbfact_distdata.c:914:    fprintf (stderr, "Malloc fails for *ainf_colptr[].");
./pzsymbfact_distdata.c:919:    fprintf (stderr, "Malloc fails for *asup_rowptr[].");
./pzsymbfact_distdata.c:933:      irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
./pzsymbfact_distdata.c:937:      p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
./pzsymbfact_distdata.c:967:  for (p = 0; p < procs; ++p) {
./pzsymbfact_distdata.c:974:		 p, iam+procs, grid->comm, &send_req[procs+p] ); 
./pzsymbfact_distdata.c:978:  for (p = 0; p < procs; ++p) {
./pzsymbfact_distdata.c:983:      MPI_Recv( dtemp, it, SuperLU_MPI_DOUBLE_COMPLEX, p, p+procs,
./pzsymbfact_distdata.c:1009:  for (p = 0; p < procs; ++p) {
./pzsymbfact_distdata.c:1012:      MPI_Wait( &send_req[procs+p], &status);
./pzsymbfact_distdata.c:1021:  memAux -= 2 * procs * iword;
./pzsymbfact_distdata.c:1022:  if ( procs > 1 ) {
./pzsymbfact_distdata.c:1031:    memAux -= 2*procs *sizeof(MPI_Request) + procs*sizeof(int_t*) +
./pzsymbfact_distdata.c:1032:      procs*sizeof(doublecomplex*) + 2*SendCnt * iword +
./pzsymbfact_distdata.c:1033:      SendCnt* dword + procs*iword +
./pzsymbfact_distdata.c:1042:      fprintf (stderr, "Malloc fails for *ainf_rowind[].");
./pzsymbfact_distdata.c:1047:      fprintf (stderr, "Malloc fails for *ainf_val[].");
./pzsymbfact_distdata.c:1058:      fprintf (stderr, "Malloc fails for *asup_colind[].");
./pzsymbfact_distdata.c:1063:      fprintf (stderr, "Malloc fails for *asup_val[].");
./pzsymbfact_distdata.c:1135:  fprintf (stdout, "Size of allocated memory (MB) %.3f\n", memRet*1e-6);
./pzsymbfact_distdata.c:1143: * <pre>
./pzsymbfact_distdata.c:1146: *   Distribute the input matrix onto the 2D process mesh.
./pzsymbfact_distdata.c:1178: *        The 2D process mesh.
./pzsymbfact_distdata.c:1185: *        (an approximation).
./pzsymbfact_distdata.c:1186: * </pre>
./pzsymbfact_distdata.c:1199:    len, len1, nsupc, nsupc_gb, ii, nprocs;
./pzsymbfact_distdata.c:1206:  int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
./pzsymbfact_distdata.c:1208:  int iam, jbrow, jbcol, jcol, kcol, krow, mycol, myrow, pc, pr, ljb_i, ljb_j, p;
./pzsymbfact_distdata.c:1232:  doublecomplex **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pzsymbfact_distdata.c:1233:  int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
./pzsymbfact_distdata.c:1237:  RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
./pzsymbfact_distdata.c:1239:  RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
./pzsymbfact_distdata.c:1252:  int_t  **fsendx_plist; /* Column process list to send down Xk.   */
./pzsymbfact_distdata.c:1259:  int_t  **bsendx_plist; /* Column process list to send down Xk.   */
./pzsymbfact_distdata.c:1267:  int_t *Urb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./pzsymbfact_distdata.c:1272:  int_t *Lrb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./pzsymbfact_distdata.c:1303:#if ( PRNTlevel>=1 )
./pzsymbfact_distdata.c:1306:#if ( PROFlevel>=1 ) 
./pzsymbfact_distdata.c:1318:  nprocs = grid->npcol * grid->nprow;
./pzsymbfact_distdata.c:1339:  nsupers_i = CEILING( nsupers, grid->nprow );/* No of local row blocks */
./pzsymbfact_distdata.c:1343:    fprintf (stderr, "Malloc fails for ilsum[].");  
./pzsymbfact_distdata.c:1348:    fprintf (stderr, "Malloc fails for ilsum_j[].");
./pzsymbfact_distdata.c:1357:    if ( myrow == PROW( gb, grid ) ) {
./pzsymbfact_distdata.c:1387:   * propagate the values of A into them.
./pzsymbfact_distdata.c:1390:    fprintf(stderr, "Calloc fails for ToRecv[].");
./pzsymbfact_distdata.c:1398:    fprintf(stderr, "Malloc fails for ToSendR[].");
./pzsymbfact_distdata.c:1404:    fprintf(stderr, "Malloc fails for index[].");
./pzsymbfact_distdata.c:1415:    fprintf(stderr, "Calloc fails for LUb_length[].");
./pzsymbfact_distdata.c:1419:    fprintf(stderr, "Malloc fails for LUb_indptr[].");
./pzsymbfact_distdata.c:1423:    fprintf(stderr, "Calloc fails for LUb_number[].");
./pzsymbfact_distdata.c:1427:    fprintf(stderr, "Calloc fails for LUb_valptr[].");
./pzsymbfact_distdata.c:1432:  k = CEILING( nsupers, grid->nprow ); 
./pzsymbfact_distdata.c:1436:    fprintf(stderr, "Malloc fails for Unzval_br_ptr[].");
./pzsymbfact_distdata.c:1440:    fprintf(stderr, "Malloc fails for Ufstnz_br_ptr[].");
./pzsymbfact_distdata.c:1448:    fprintf(stderr, "Malloc fails for ToSendD[].");
./pzsymbfact_distdata.c:1455:    fprintf(stderr, "Calloc fails for rb_marker[].");
./pzsymbfact_distdata.c:1459:    fprintf(stderr, "Calloc fails for rb_marker[].");
./pzsymbfact_distdata.c:1469:    fprintf(stderr, "Calloc fails for SPA dense[].");
./pzsymbfact_distdata.c:1474:    fprintf(stderr, "Calloc fails for fmod[].");
./pzsymbfact_distdata.c:1478:    fprintf(stderr, "Calloc fails for bmod[].");
./pzsymbfact_distdata.c:1488:    fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[].");
./pzsymbfact_distdata.c:1492:    fprintf(stderr, "Malloc fails for Lrowind_bc_ptr[].");
./pzsymbfact_distdata.c:1498:	fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
./pzsymbfact_distdata.c:1503:	fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
./pzsymbfact_distdata.c:1507:    fprintf(stderr, "Malloc fails for Lindval_loc_bc_ptr[].");
./pzsymbfact_distdata.c:1512:    fprintf(stderr, "Malloc fails for Unnz[].");
./pzsymbfact_distdata.c:1524:  /* These lists of processes will be used for triangular solves. */
./pzsymbfact_distdata.c:1526:    fprintf(stderr, "Malloc fails for fsendx_plist[].");
./pzsymbfact_distdata.c:1529:  len = nsupers_j * grid->nprow;
./pzsymbfact_distdata.c:1531:    fprintf(stderr, "Malloc fails for fsendx_plist[0]");
./pzsymbfact_distdata.c:1535:  for (i = 0, j = 0; i < nsupers_j; ++i, j += grid->nprow)
./pzsymbfact_distdata.c:1538:    fprintf(stderr, "Malloc fails for bsendx_plist[].");
./pzsymbfact_distdata.c:1542:    fprintf(stderr, "Malloc fails for bsendx_plist[0]");
./pzsymbfact_distdata.c:1546:  for (i = 0, j = 0; i < nsupers_j; ++i, j += grid->nprow)
./pzsymbfact_distdata.c:1552:    PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
./pzsymbfact_distdata.c:1553:    THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
./pzsymbfact_distdata.c:1557:    jbrow = PROW( jb, grid );
./pzsymbfact_distdata.c:1563:    if ( myrow == jbrow ) { /* Block row jb in my process row */
./pzsymbfact_distdata.c:1568:	    printf ("ERR7\n");
./pzsymbfact_distdata.c:1571:	    printf ("Pe[%d] ERR distsn jb " IFMT " gb " IFMT " j " IFMT " jcol %d\n",
./pzsymbfact_distdata.c:1575:	  if (gb >= nsupers || lb >= nsupers_j) printf ("ERR8\n");
./pzsymbfact_distdata.c:1578:	    printf ("Pe[%d] ERR1 jb " IFMT " gb " IFMT " j " IFMT " jcol %d\n",
./pzsymbfact_distdata.c:1593:	if (i >= xusub[nsupers_i]) printf ("ERR10\n");
./pzsymbfact_distdata.c:1598:	  printf ("Pe[%d] [%d %d] elt [%d] jbcol %d pc %d\n",
./pzsymbfact_distdata.c:1602:	pc = PCOL( gb, grid ); /* Process col owning this block */
./pzsymbfact_distdata.c:1605:	pr = PROW( gb, grid );
./pzsymbfact_distdata.c:1606:	if ( pr != jbrow  && mycol == pc)
./pzsymbfact_distdata.c:1613:	    if (Urb_marker[lb] == FALSE && gb != jb && myrow != pr) nbrecvx ++;
./pzsymbfact_distdata.c:1617:	       printf ("Pe[%d] T1 [%d %d] nrbu %d \n",
./pzsymbfact_distdata.c:1623:#if ( PRNTlevel>=1 )
./pzsymbfact_distdata.c:1648:	  fprintf (stderr, "Malloc fails for Uindex[]");
./pzsymbfact_distdata.c:1654:	  fprintf (stderr, "Malloc fails for Unzval_br_ptr[*][]");
./pzsymbfact_distdata.c:1680:	/* Propagate the fstnz subscripts to Ufstnz_br_ptr[],
./pzsymbfact_distdata.c:1718:    if (mycol == jbcol) {  /* Block column jb in my process column */
./pzsymbfact_distdata.c:1723:	  if (irow >= n) printf ("Pe[%d] ERR1\n", iam);
./pzsymbfact_distdata.c:1725:	  if (gb >= nsupers) printf ("Pe[%d] ERR5\n", iam);
./pzsymbfact_distdata.c:1726:	  if ( myrow == PROW( gb, grid ) ) {
./pzsymbfact_distdata.c:1729:	    if (irow >= ldaspa) printf ("Pe[%d] ERR0\n", iam);
./pzsymbfact_distdata.c:1756:	pr = PROW( gb, grid ); /* Process row owning this block */
./pzsymbfact_distdata.c:1757:	if ( pr != jbrow && fsendx_plist[ljb_j][pr] == EMPTY &&
./pzsymbfact_distdata.c:1759:	  fsendx_plist[ljb_j][pr] = YES;
./pzsymbfact_distdata.c:1762:	if ( myrow == pr ) {
./pzsymbfact_distdata.c:1774:#if ( PRNTlevel>=1 )
./pzsymbfact_distdata.c:1801:	  fprintf (stderr, "Malloc fails for index[]");
./pzsymbfact_distdata.c:1807:	  fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[*][] col block " IFMT, jb);
./pzsymbfact_distdata.c:1847:	  /* Propagate the compressed row subscripts to Lindex[],
./pzsymbfact_distdata.c:1853:	    if ( myrow == PROW( gb, grid ) ) {
./pzsymbfact_distdata.c:1872:			krow = PROW( jb, grid );
./pzsymbfact_distdata.c:1951:  /* exchange information about bsendx_plist in between column of processors */
./pzsymbfact_distdata.c:1952:  k = SUPERLU_MAX( grid->nprow, grid->npcol);
./pzsymbfact_distdata.c:1954:    fprintf (stderr, "Malloc fails for recvBuf[].");
./pzsymbfact_distdata.c:1957:  if ( !(nnzToRecv = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pzsymbfact_distdata.c:1958:    fprintf (stderr, "Malloc fails for nnzToRecv[].");
./pzsymbfact_distdata.c:1961:  if ( !(ptrToRecv = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pzsymbfact_distdata.c:1962:    fprintf (stderr, "Malloc fails for ptrToRecv[].");
./pzsymbfact_distdata.c:1965:  if ( !(nnzToSend = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pzsymbfact_distdata.c:1966:    fprintf (stderr, "Malloc fails for nnzToRecv[].");
./pzsymbfact_distdata.c:1969:  if ( !(ptrToSend = (int *) SUPERLU_MALLOC(nprocs*sizeof(int))) ) {
./pzsymbfact_distdata.c:1970:    fprintf (stderr, "Malloc fails for ptrToRecv[].");
./pzsymbfact_distdata.c:1974:  if (memDist < (nsupers*k*iword +4*nprocs * sizeof(int)))
./pzsymbfact_distdata.c:1975:    memDist = nsupers*k*iword +4*nprocs * sizeof(int);
./pzsymbfact_distdata.c:1977:  for (p = 0; p < nprocs; p++)
./pzsymbfact_distdata.c:1982:    jbrow = PROW( jb, grid );
./pzsymbfact_distdata.c:1987:  for (p = 0; p < nprocs; p++) {
./pzsymbfact_distdata.c:2000:    jbrow = PROW( jb, grid );
./pzsymbfact_distdata.c:2014:    jbrow = PROW( jb, grid );
./pzsymbfact_distdata.c:2040:  /* exchange information about bsendx_plist in between column of processors */
./pzsymbfact_distdata.c:2041:  MPI_Allreduce ((*bsendx_plist), recvBuf, nsupers_j * grid->nprow, mpi_int_t,
./pzsymbfact_distdata.c:2046:    jbrow = PROW( jb, grid);
./pzsymbfact_distdata.c:2050:	for (k = ljb_j * grid->nprow; k < (ljb_j+1) * grid->nprow; k++) {
./pzsymbfact_distdata.c:2057:	for (k = ljb_j * grid->nprow; k < (ljb_j+1) * grid->nprow; k++) 
./pzsymbfact_distdata.c:2076:		nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
./pzsymbfact_distdata.c:2136:			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
./pzsymbfact_distdata.c:2151:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2159:		if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pzsymbfact_distdata.c:2161:		if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pzsymbfact_distdata.c:2177:		if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pzsymbfact_distdata.c:2179:		for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
./pzsymbfact_distdata.c:2189:				pr = PROW( gb, grid );
./pzsymbfact_distdata.c:2190:				ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
./pzsymbfact_distdata.c:2196:		MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MIN,grid->cscp.comm);					  
./pzsymbfact_distdata.c:2206:			for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pzsymbfact_distdata.c:2207:			for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pzsymbfact_distdata.c:2208:			for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pzsymbfact_distdata.c:2212:			for (j=0;j<grid->nprow;++j){
./pzsymbfact_distdata.c:2215:				pr = PROW( gb, grid );
./pzsymbfact_distdata.c:2216:				if(gb==jb)Root=pr;
./pzsymbfact_distdata.c:2217:				if(myrow==pr)Iactive=1;		
./pzsymbfact_distdata.c:2222:			quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
./pzsymbfact_distdata.c:2225:				// printf("jb %5d damn\n",jb);
./pzsymbfact_distdata.c:2230:				for (j = 0; j < grid->nprow; ++j){
./pzsymbfact_distdata.c:2231:					if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pzsymbfact_distdata.c:2232:						ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pzsymbfact_distdata.c:2248:					// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pzsymbfact_distdata.c:2252:					// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb],'z'));
./pzsymbfact_distdata.c:2256:					// #if ( PRNTlevel>=1 )		
./pzsymbfact_distdata.c:2259:						for (j = 0; j < grid->nprow; ++j) {
./pzsymbfact_distdata.c:2266:						// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
./pzsymbfact_distdata.c:2268:						// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
./pzsymbfact_distdata.c:2269:						// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pzsymbfact_distdata.c:2270:						// // printf("\n");
./pzsymbfact_distdata.c:2285:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2287:	if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
./pzsymbfact_distdata.c:2291:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2296:		nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzsymbfact_distdata.c:2304:			pr = PROW( k, grid );
./pzsymbfact_distdata.c:2305:			if ( myrow == pr ) {
./pzsymbfact_distdata.c:2312:		/* Every process receives the count, but it is only useful on the
./pzsymbfact_distdata.c:2313:		   diagonal processes.  */
./pzsymbfact_distdata.c:2318:		k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzsymbfact_distdata.c:2360:					pr = PROW( ib, grid );
./pzsymbfact_distdata.c:2361:					if ( myrow == pr ) { /* Block row ib in my process row */
./pzsymbfact_distdata.c:2372:			ib = myrow+lib*grid->nprow;  /* not sure */
./pzsymbfact_distdata.c:2374:				pr = PROW( ib, grid );
./pzsymbfact_distdata.c:2406:							ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pzsymbfact_distdata.c:2418:						// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
./pzsymbfact_distdata.c:2422:						#if ( PRNTlevel>=1 )
./pzsymbfact_distdata.c:2425:						// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pzsymbfact_distdata.c:2426:						// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pzsymbfact_distdata.c:2427:						// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pzsymbfact_distdata.c:2428:						// printf("\n");
./pzsymbfact_distdata.c:2452:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2454:	if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
./pzsymbfact_distdata.c:2457:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2466:		if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
./pzsymbfact_distdata.c:2468:		if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
./pzsymbfact_distdata.c:2484:		if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
./pzsymbfact_distdata.c:2486:		for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
./pzsymbfact_distdata.c:2490:		for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
./pzsymbfact_distdata.c:2491:			ib = myrow+lib*grid->nprow;  /* not sure */
./pzsymbfact_distdata.c:2493:		// if(ib==0)printf("iam %5d ib %5d\n",iam,ib);
./pzsymbfact_distdata.c:2502:				  pr = PROW( ib, grid );
./pzsymbfact_distdata.c:2503:				  if ( mycol == pc ) { /* Block column ib in my process column */		
./pzsymbfact_distdata.c:2504:					ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);			  
./pzsymbfact_distdata.c:2507:				pr = PROW( ib, grid ); // take care of diagonal node stored as L
./pzsymbfact_distdata.c:2509:				if ( mycol == pc ) { /* Block column ib in my process column */					
./pzsymbfact_distdata.c:2511:					ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);					
./pzsymbfact_distdata.c:2512:					// if(pr+ljb*grid->nprow==0)printf("iam %5d ib %5d ActiveFlagAll %5d pr %5d ljb %5d\n",iam,ib,ActiveFlagAll[pr+ljb*grid->nprow],pr,ljb);
./pzsymbfact_distdata.c:2518:		// printf("iam %5d ActiveFlagAll %5d\n",iam,ActiveFlagAll[0]);
./pzsymbfact_distdata.c:2521:		MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MAX,grid->cscp.comm);					  
./pzsymbfact_distdata.c:2527:			// if ( mycol == pc ) { /* Block column jb in my process column */
./pzsymbfact_distdata.c:2529:			for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
./pzsymbfact_distdata.c:2530:			for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
./pzsymbfact_distdata.c:2531:			for (j=0;j<grid->nprow;++j)ranks[j]=-1;
./pzsymbfact_distdata.c:2535:			for (j=0;j<grid->nprow;++j){
./pzsymbfact_distdata.c:2538:				pr = PROW( gb, grid );
./pzsymbfact_distdata.c:2539:				if(gb==jb)Root=pr;
./pzsymbfact_distdata.c:2540:				if(myrow==pr)Iactive=1;		
./pzsymbfact_distdata.c:2544:			quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
./pzsymbfact_distdata.c:2545:		// printf("jb: %5d Iactive %5d\n",jb,Iactive);
./pzsymbfact_distdata.c:2548:				// if(jb==0)printf("root:%5d jb: %5d ActiveFlag %5d \n",Root,jb,ActiveFlag[0]);
./pzsymbfact_distdata.c:2553:				for (j = 0; j < grid->nprow; ++j){
./pzsymbfact_distdata.c:2554:					if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
./pzsymbfact_distdata.c:2555:						ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
./pzsymbfact_distdata.c:2559:		// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
./pzsymbfact_distdata.c:2571:					// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
./pzsymbfact_distdata.c:2576:					for (j = 0; j < grid->nprow; ++j) {
./pzsymbfact_distdata.c:2577:						// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
./pzsymbfact_distdata.c:2583:					// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
./pzsymbfact_distdata.c:2596:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2598:	if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
./pzsymbfact_distdata.c:2601:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2606:		nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzsymbfact_distdata.c:2614:			pr = PROW( k, grid );
./pzsymbfact_distdata.c:2615:			if ( myrow == pr ) {
./pzsymbfact_distdata.c:2622:		/* Every process receives the count, but it is only useful on the
./pzsymbfact_distdata.c:2623:		   diagonal processes.  */
./pzsymbfact_distdata.c:2628:		k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzsymbfact_distdata.c:2660:		for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
./pzsymbfact_distdata.c:2661:			ib = myrow+lib*grid->nprow;  /* not sure */
./pzsymbfact_distdata.c:2667:				  if ( mycol == pc ) { /* Block column ib in my process column */	
./pzsymbfact_distdata.c:2672:				if ( mycol == pc ) { /* Block column ib in my process column */						
./pzsymbfact_distdata.c:2681:			ib = myrow+lib*grid->nprow;  /* not sure */
./pzsymbfact_distdata.c:2683:				pr = PROW( ib, grid );
./pzsymbfact_distdata.c:2714:							ranks[ii] = PNUM( pr, ranks[ii], grid );		
./pzsymbfact_distdata.c:2726:						// #if ( PRNTlevel>=1 )
./pzsymbfact_distdata.c:2728:						// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
./pzsymbfact_distdata.c:2731:						// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
./pzsymbfact_distdata.c:2732:						// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
./pzsymbfact_distdata.c:2733:						// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
./pzsymbfact_distdata.c:2734:						// printf("\n");
./pzsymbfact_distdata.c:2756:#if ( PROFlevel>=1 )
./pzsymbfact_distdata.c:2758:	if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
./pzsymbfact_distdata.c:2807:#if ( PRNTlevel>=1 )
./pzsymbfact_distdata.c:2808:  if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
./pzsymbfact_distdata.c:2812:  k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./pzutil.c:4:approvals from U.S. Dept. of Energy) 
./pzutil.c:15: * <pre>
./pzutil.c:19: * </pre>
./pzutil.c:25:/*! \brief Gather A from the distributed compressed row format to global A in compressed column format.
./pzutil.c:27:int pzCompRow_loc_to_CompCol_global
./pzutil.c:55:    int   it, n_loc, procs;
./pzutil.c:58:    CHECK_MALLOC(grid->iam, "Enter pzCompRow_loc_to_CompCol_global");
./pzutil.c:73:       FIRST PHASE: TRANSFORM A INTO DISTRIBUTED COMPRESSED COLUMN.
./pzutil.c:75:    zCompRow_to_CompCol_dist(m_loc, n, nnz_loc, a, colind, rowptr, &a_loc,
./pzutil.c:81:    printf("Proc %d\n", grid->iam);
./pzutil.c:82:    PrintInt10("rowind_loc", nnz_loc, rowind_loc);
./pzutil.c:83:    PrintInt10("colptr_loc", n+1, colptr_loc);
./pzutil.c:86:    procs = grid->nprow * grid->npcol;
./pzutil.c:87:    if ( !(fst_rows = (int_t *) intMalloc_dist(2*procs)) )
./pzutil.c:89:    n_locs = fst_rows + procs;
./pzutil.c:92:    for (i = 0; i < procs-1; ++i) n_locs[i] = fst_rows[i+1] - fst_rows[i];
./pzutil.c:93:    n_locs[procs-1] = n - fst_rows[procs-1];
./pzutil.c:94:    if ( !(recvcnts = SUPERLU_MALLOC(5*procs * sizeof(int))) )
./pzutil.c:96:    sendcnts = recvcnts + procs;
./pzutil.c:97:    rdispls = sendcnts + procs;
./pzutil.c:98:    sdispls = rdispls + procs;
./pzutil.c:99:    itemp_32 = sdispls + procs;
./pzutil.c:103:    /* n column starts for each column, and procs column ends for each block */
./pzutil.c:104:    if ( !(colptr_send = intMalloc_dist(n + procs)) )
./pzutil.c:106:    if ( !(colptr_blk = intMalloc_dist( (((size_t) n_loc)+1)*procs)) )
./pzutil.c:108:    for (i = 0, j = 0; i < procs; ++i) {
./pzutil.c:127:    for (i = 0; i < procs; ++i) {
./pzutil.c:137:    /*assert(k == (n_loc+1)*procs);*/
./pzutil.c:139:    /* Now prepare to transfer row indices and values. */
./pzutil.c:141:    for (i = 0; i < procs-1; ++i) {
./pzutil.c:145:    sendcnts[procs-1] = colptr_loc[n] - colptr_loc[fst_rows[procs-1]];
./pzutil.c:146:    for (i = 0; i < procs; ++i) {
./pzutil.c:151:    for (i = 0; i < procs-1; ++i) rdispls[i+1] = rdispls[i] + recvcnts[i];
./pzutil.c:153:    k = rdispls[procs-1] + recvcnts[procs-1]; /* Total received */
./pzutil.c:173:	for (i = 0; i < procs; ++i) {
./pzutil.c:183:    for (i = 0; i < procs; ++i) {
./pzutil.c:195:        for (i = 0; i < procs; ++i) {
./pzutil.c:207:       SECOND PHASE: GATHER TO GLOBAL A IN COMPRESSED COLUMN FORMAT.
./pzutil.c:220:    for (i = 0, nnz = 0; i < procs; ++i) nnz += itemp[i];
./pzutil.c:230:    for (i = 0; i < procs-1; ++i) {
./pzutil.c:234:    itemp_32[procs-1] = itemp[procs-1];
./pzutil.c:247:    for (i = 0; i < procs-1; ++i) {
./pzutil.c:251:    itemp_32[procs-1] = n_locs[procs-1];
./pzutil.c:256:    for (i = 1; i < procs; ++i) {
./pzutil.c:259:	itemp[i] += itemp[i-1]; /* prefix sum */
./pzutil.c:265:        printf("After pdCompRow_loc_to_CompCol_global()\n");
./pzutil.c:266:	zPrint_CompCol_Matrix_dist(GA);
./pzutil.c:280:    if ( !grid->iam ) printf("sizeof(NCformat) %lu\n", sizeof(NCformat));
./pzutil.c:281:    CHECK_MALLOC(grid->iam, "Exit pzCompRow_loc_to_CompCol_global");
./pzutil.c:284:} /* pzCompRow_loc_to_CompCol_global */
./pzutil.c:293: int_t row_to_proc[],
./pzutil.c:302:    int p, procs;
./pzutil.c:313:    procs = grid->nprow * grid->npcol;
./pzutil.c:314:    if ( !(sendcnts = SUPERLU_MALLOC(10*procs * sizeof(int))) )
./pzutil.c:316:    sendcnts_nrhs = sendcnts + procs;
./pzutil.c:317:    recvcnts = sendcnts_nrhs + procs;
./pzutil.c:318:    recvcnts_nrhs = recvcnts + procs;
./pzutil.c:319:    sdispls = recvcnts_nrhs + procs;
./pzutil.c:320:    sdispls_nrhs = sdispls + procs;
./pzutil.c:321:    rdispls = sdispls_nrhs + procs;
./pzutil.c:322:    rdispls_nrhs = rdispls + procs;
./pzutil.c:323:    ptr_to_ibuf = rdispls_nrhs + procs;
./pzutil.c:324:    ptr_to_dbuf = ptr_to_ibuf + procs;
./pzutil.c:326:    for (i = 0; i < procs; ++i) sendcnts[i] = 0;
./pzutil.c:328:    /* Count the number of X entries to be sent to each process.*/
./pzutil.c:330:        p = row_to_proc[perm[i]];
./pzutil.c:338:    for (i = 1; i < procs; ++i) {
./pzutil.c:346:    k = sdispls[procs-1] + sendcnts[procs-1];/* Total number of sends */
./pzutil.c:347:    l = rdispls[procs-1] + recvcnts[procs-1];/* Total number of recvs */
./pzutil.c:357:    for (i = 0; i < procs; ++i) {
./pzutil.c:365:	p = row_to_proc[j];
./pzutil.c:407:    int_t *row_to_proc, *inv_perm_c, *itemp;
./pzutil.c:410:    int          procs;
./pzutil.c:415:    procs = grid->nprow * grid->npcol;
./pzutil.c:417:    if ( !(row_to_proc = intMalloc_dist(A->nrow)) )
./pzutil.c:418:	ABORT("Malloc fails for row_to_proc[]");
./pzutil.c:419:    SOLVEstruct->row_to_proc = row_to_proc;
./pzutil.c:426:       EVERY PROCESS NEEDS TO KNOW GLOBAL PARTITION.
./pzutil.c:427:       SET UP THE MAPPING BETWEEN ROWS AND PROCESSES.
./pzutil.c:429:       NOTE: For those processes that do not own any row, it must
./pzutil.c:432:    if ( !(itemp = intMalloc_dist(procs+1)) )
./pzutil.c:436:    itemp[procs] = A->nrow;
./pzutil.c:437:    for (p = 0; p < procs; ++p) {
./pzutil.c:438:        for (i = itemp[p] ; i < itemp[p+1]; ++i) row_to_proc[i] = p;
./pzutil.c:442:      printf("fst_row = %d\n", fst_row);
./pzutil.c:443:      PrintInt10("row_to_proc", A->nrow, row_to_proc);
./pzutil.c:444:      PrintInt10("inv_perm_c", A->ncol, inv_perm_c);
./pzutil.c:450:    /* Compute the mapping between rows and processes. */
./pzutil.c:451:    /* XSL NOTE: What happens if # of mapped processes is smaller
./pzutil.c:452:       than total Procs?  For the processes without any row, let
./pzutil.c:456:    itemp[procs] = n;
./pzutil.c:457:    for (p = 0; p < procs; ++p) {
./pzutil.c:462:	    for (i = j ; i < k; ++i) row_to_proc[i] = p;
./pzutil.c:467:    get_diag_procs(A->ncol, LUstruct->Glu_persist, grid,
./pzutil.c:468:		   &SOLVEstruct->num_diag_procs,
./pzutil.c:469:		   &SOLVEstruct->diag_procs,
./pzutil.c:501:    SUPERLU_FREE(SOLVEstruct->row_to_proc);
./pzutil.c:503:    SUPERLU_FREE(SOLVEstruct->diag_procs);
./pzutil.c:536:      if ( !iam ) printf("\tSol %2d: ||X-Xtrue||/||X|| = %e\n", j, err);
./pzutil.c:568: 	nb = CEILING(nsupers, grid->nprow);
./smach_dist.c:4:approvals from U.S. Dept. of Energy) 
./smach_dist.c:27:    SMACH returns single precision machine parameters.   
./smach_dist.c:47:            eps   = relative machine precision   
./smach_dist.c:50:            prec  = eps*base   
./sp_colorder.c:4:approvals from U.S. Dept. of Energy) 
./sp_colorder.c:14: * <pre>
./sp_colorder.c:19: * </pre>
./sp_colorder.c:28: * <pre>
./sp_colorder.c:42: *       (4) Overwrite perm_c[] with the product perm_c * post.
./sp_colorder.c:78: * </pre>
./sp_colorder.c:117:	PrintInt10("pre_order:", n, perm_c);
./sp_colorder.c:129:	/* In this case, perm_r[] may be changed, etree(Pr*A + (Pr*A)')
./sp_colorder.c:176:	if ( !iam ) PrintInt10("etree:", n, etree);
./sp_colorder.c:189:	if ( !iam ) PrintInt10("postorder etree:", n, etree);
./sp_colorder.c:199:	    iwork[i] = post[perm_c[i]];  /* product of perm_c and post */
./sp_colorder.c:204:	    PrintInt10("Pc*post:", n, perm_c);
./sp_colorder.c:232:	    printf("%s: Not a valid PERM[" IFMT "] = " IFMT "\n", 
./sp_ienv.c:4:approvals from U.S. Dept. of Energy) 
./sp_ienv.c:23:</pre>
./sp_ienv.c:30:    This version provides a set of parameters which should give good,   
./sp_ienv.c:34:    and problem size information in the arguments.   
./sp_ienv.c:42:	         columns of matrix A in the process of Gaussian elimination.
./sp_ienv.c:54:	    = 7: the minimum value of the product M*N*K for a GEMM call
./sp_ienv.c:62:</pre>
./sp_ienv.c:73:    // printf(" this function called\n");
./static_schedule.c:4:approvals from U.S. Dept. of Energy) 
./static_schedule.c:14: * <pre>
./static_schedule.c:23: * </pre>
./static_schedule.c:62:    int_t Pc, Pr;
./static_schedule.c:68:    int ncb, nrb, p, pr, pc, nblocks;
./static_schedule.c:96:    Pr = grid->nprow;
./static_schedule.c:103:    nrb = nsupers / Pr;
./static_schedule.c:106:    print_memorylog(stat, "before static schedule");
./static_schedule.c:124:#if ( PRNTlevel>=1 )
./static_schedule.c:126:                printf (" === using column e-tree ===\n");
./static_schedule.c:146:#if ( PRNTlevel>=1 )
./static_schedule.c:148:                printf (" === using supernodal e-tree ===\n");
./static_schedule.c:162:                    krow = PROW (jb, grid);
./static_schedule.c:188:                    krow = PROW (jb, grid);
./static_schedule.c:228:                /*printf( " == push leaf %d (%d) ==\n",i,nnodes ); */
./static_schedule.c:241:        /* process fifo queue, and compute the ordering */
./static_schedule.c:262:                    /*printf( "=== push %d ===\n",ptr->id ); */
./static_schedule.c:272:            /*printf( "\n" ); */
./static_schedule.c:280:        /* Need to process both L- and U-factors, use the symmetrically
./static_schedule.c:281:           pruned graph of L & U instead of tree (very naive implementation) */
./static_schedule.c:286:        if (! (sendcnts = SUPERLU_MALLOC ((4 + 2 * nrbp1) * Pr * Pc * sizeof (int))))
./static_schedule.c:288:	log_memory((4 + 2 * nrbp1) * Pr * Pc * sizeof (int), stat);
./static_schedule.c:290:        sdispls = &sendcnts[Pr * Pc];
./static_schedule.c:291:        recvcnts = &sdispls[Pr * Pc];
./static_schedule.c:292:        rdispls = &recvcnts[Pr * Pc];
./static_schedule.c:293:        srows = &rdispls[Pr * Pc];
./static_schedule.c:294:        rrows = &srows[Pr * Pc * nrbp1];
./static_schedule.c:297:#if ( PRNTlevel>=1 )
./static_schedule.c:299:            printf (" === using DAG ===\n");
./static_schedule.c:302:        /* send supno block of local U-factor to a processor *
./static_schedule.c:305:        /* srows   : # of block to send to a processor from each supno row */
./static_schedule.c:306:        /* sendcnts: total # of blocks to send to a processor              */
./static_schedule.c:307:        for (p = 0; p < Pr * Pc * nrbp1; p++) srows[p] = 0;
./static_schedule.c:308:        for (p = 0; p < Pr * Pc; p++) sendcnts[p] = 0;
./static_schedule.c:313:            jb = lb * Pr + myrow;
./static_schedule.c:322:                    pr = ib % Pr;
./static_schedule.c:323:                    p = pr * Pc + pc;
./static_schedule.c:332:        if (myrow < nsupers % grid->nprow) {
./static_schedule.c:333:            jb = nrb * Pr + myrow;
./static_schedule.c:342:                    pr = ib % Pr;
./static_schedule.c:343:                    p = pr * Pc + pc;
./static_schedule.c:353:        for (p = 1; p < Pr * Pc; p++) sdispls[p] = sdispls[p - 1] + sendcnts[p - 1];
./static_schedule.c:359:            jb = lb * Pr + myrow;
./static_schedule.c:367:                    pr = ib % Pr;
./static_schedule.c:368:                    p = pr * Pc + pc;
./static_schedule.c:377:        if (myrow < nsupers % grid->nprow) {
./static_schedule.c:378:            jb = nrb * Pr + myrow;
./static_schedule.c:386:                    pr = ib % Pr;
./static_schedule.c:387:                    p = pr * Pc + pc;
./static_schedule.c:404:        for (p = 1; p < Pr * Pc; p++) {
./static_schedule.c:431:            pr = j % Pr;
./static_schedule.c:432:            lb = j / Pr;
./static_schedule.c:436:                p = pr * Pc + pc; /* the processor owning this block of U-factor */
./static_schedule.c:457:            pr = j % Pr;
./static_schedule.c:458:            lb = j / Pr;
./static_schedule.c:462:                p = pr * Pc + pc;
./static_schedule.c:515:                krow = PROW (jb, grid);
./static_schedule.c:544:                krow = PROW (jb, grid);
./static_schedule.c:732:        /* form global DAG on each processor */
./static_schedule.c:737:        for (lb = 1; lb < Pc * Pr; lb++) {
./static_schedule.c:755:        for (p = 0; p < Pc * Pr; p++) {
./static_schedule.c:768:        for (p = 0; p < Pc * Pr; p++) {
./static_schedule.c:816:                printf (" # %d: nnodes[" IFMT "]=" IFMT "+" IFMT "\n",
./static_schedule.c:822:        /* form global DAG on each processor */  
./static_schedule.c:826:        for (lb = 1; lb < Pc * Pr; lb++) {
./static_schedule.c:845:        for (p = 0; p < Pc * Pr; p++) {
./static_schedule.c:864:        for (p = 0; p < Pc * Pr; p++) {
./static_schedule.c:903:                /*printf( " == push leaf %d (%d) ==\n",i,nnodes ); */
./static_schedule.c:916:        /* process fifo queue, and compute the ordering */
./static_schedule.c:920:            /*printf( "=== pop %d (%d) ===\n",head->id,i ); */
./static_schedule.c:939:                    /*printf( "=== push %d ===\n",ptr->id ); */
./static_schedule.c:949:            /*printf( "\n" ); */
./static_schedule.c:960:	log_memory(-(4 * nsupers + (4 + 2 * nrbp1)*Pr*Pc) * iword, stat);
./static_schedule.c:975:    print_memorylog(stat, "after static schedule");
./superlu_ddefs.h:4:approvals from U.S. Dept. of Energy) 
./superlu_ddefs.h:14: * \brief  Distributed SuperLU data types and function prototypes
./superlu_ddefs.h:16: * <pre>
./superlu_ddefs.h:20: * April 5, 2015
./superlu_ddefs.h:22: * </pre>
./superlu_ddefs.h:30: * Purpose:     Distributed SuperLU data types and function prototypes
./superlu_ddefs.h:43: * On each processor, the blocks in L are stored in compressed block
./superlu_ddefs.h:44: * column format, the blocks in U are stored in compressed block row format.
./superlu_ddefs.h:55:    int_t   **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
./superlu_ddefs.h:56:    double  **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
./superlu_ddefs.h:59:    RdTree  *LRtree_ptr;       /* size ceil(NSUPERS/Pr)                */
./superlu_ddefs.h:61:    RdTree  *URtree_ptr;       /* size ceil(NSUPERS/Pr)			*/
./superlu_ddefs.h:84:    int   **ToSendR;        /* List of processes to send right block col. */
./superlu_ddefs.h:88:    int_t   **fsendx_plist;   /* Column process list to send down Xk       */
./superlu_ddefs.h:89:    int_t   *frecv;           /* Modifications to be recv'd in proc row    */
./superlu_ddefs.h:93:    int_t   **bsendx_plist;   /* Column process list to send down Xk       */
./superlu_ddefs.h:94:    int_t   *brecv;           /* Modifications to be recv'd in proc row    */
./superlu_ddefs.h:117:    int_t **ut_sendx_plist;  /* Row process list to send down Xk            */
./superlu_ddefs.h:118:    int_t *utrecv;           /* Modifications to be recev'd in proc column. */
./superlu_ddefs.h:146:    int_t *ind_tosend;    /* X indeices to be sent to other processes */
./superlu_ddefs.h:147:    int_t *ind_torecv;    /* X indeices to be received from other processes */
./superlu_ddefs.h:148:    int_t *ptr_ind_tosend;/* Printers to ind_tosend[] (Size procs)
./superlu_ddefs.h:150:    int_t *ptr_ind_torecv;/* Printers to ind_torecv[] (Size procs)
./superlu_ddefs.h:156:    double *val_tosend;   /* X values to be sent to other processes */
./superlu_ddefs.h:157:    double *val_torecv;   /* X values to be received from other processes */
./superlu_ddefs.h:166:    int_t *row_to_proc;
./superlu_ddefs.h:168:    int_t num_diag_procs, *diag_procs, *diag_len;
./superlu_ddefs.h:176:    int_t *xrow_to_proc; /* used by PDSLin */
./superlu_ddefs.h:181: * Function prototypes
./superlu_ddefs.h:194:dCreate_CompRowLoc_Matrix_dist(SuperMatrix *, int_t, int_t, int_t, int_t,
./superlu_ddefs.h:198:dCompRow_to_CompCol_dist(int_t, int_t, int_t, double *, int_t *, int_t *,
./superlu_ddefs.h:201:pdCompRow_loc_to_CompCol_global(int_t, SuperMatrix *, gridinfo_t *,
./superlu_ddefs.h:283:/* #define GPU_PROF
./superlu_ddefs.h:284:#define IPM_PROF */
./superlu_ddefs.h:349:extern void dClone_CompRowLoc_Matrix_dist(SuperMatrix *, SuperMatrix *);
./superlu_ddefs.h:350:extern void dCopy_CompRowLoc_Matrix_dist(SuperMatrix *, SuperMatrix *);
./superlu_ddefs.h:351:extern void dZero_CompRowLoc_Matrix_dist(SuperMatrix *);
./superlu_ddefs.h:352:extern void dScaleAddId_CompRowLoc_Matrix_dist(SuperMatrix *, double);
./superlu_ddefs.h:353:extern void dScaleAdd_CompRowLoc_Matrix_dist(SuperMatrix *, SuperMatrix *, double);
./superlu_ddefs.h:380:extern void  dPrintLblocks(int, int_t, gridinfo_t *, Glu_persist_t *,
./superlu_ddefs.h:382:extern void  dPrintUblocks(int, int_t, gridinfo_t *, Glu_persist_t *,
./superlu_ddefs.h:384:extern void  dPrint_CompCol_Matrix_dist(SuperMatrix *);
./superlu_ddefs.h:385:extern void  dPrint_Dense_Matrix_dist(SuperMatrix *);
./superlu_ddefs.h:386:extern int   dPrint_CompRowLoc_Matrix_dist(SuperMatrix *);
./superlu_ddefs.h:387:extern int   file_dPrint_CompRowLoc_Matrix_dist(FILE *fp, SuperMatrix *A);																			   
./superlu_ddefs.h:388:extern int   file_PrintDouble5(FILE *, char *, int_t, double *);
./superlu_defs.h:4:approvals from U.S. Dept. of Energy) 
./superlu_defs.h:12: * \brief Definitions which are precision-neutral
./superlu_defs.h:14: * <pre>
./superlu_defs.h:23: * </pre>
./superlu_defs.h:31: * Purpose:     Definitions which are precision-neutral
./superlu_defs.h:177:#define XK_H     2  /* The header preceding each X block. */
./superlu_defs.h:178:#define LSUM_H   2  /* The header preceding each MOD block. */
./superlu_defs.h:219:#define LBi(bnum,grid)  ( (bnum)/grid->nprow )/* Global to local block rowwise */
./superlu_defs.h:221:#define PROW(bnum,grid) ( (bnum) % grid->nprow )
./superlu_defs.h:223:#define PNUM(i,j,grid)  ( (i)*grid->npcol + j ) /* Process number at coord(i,j) */
./superlu_defs.h:281: *   Define the 2D mapping of matrix blocks to process grid.
./superlu_defs.h:283: *   Process grid:
./superlu_defs.h:284: *     Processes are numbered (0 : P-1).
./superlu_defs.h:285: *     P = Pr x Pc, where Pr, Pc are the number of process rows and columns.
./superlu_defs.h:286: *     (pr,pc) is the coordinate of IAM; 0 <= pr < Pr, 0 <= pc < Pc.
./superlu_defs.h:300: *  Mapping of matrix block (I,J) to process grid (pr,pc):
./superlu_defs.h:301: *     (pr,pc) = ( MOD(I,NPROW), MOD(J,NPCOL) )
./superlu_defs.h:303: *  (xsup[nsupers],supno[n]) are replicated on all processors.
./superlu_defs.h:310:    int Np;               /* number of processes */
./superlu_defs.h:311:    int Iam;              /* my process number */
./superlu_defs.h:314:/*-- Process grid definition */
./superlu_defs.h:317:    superlu_scope_t rscp; /* process scope in rowwise, horizontal directon */
./superlu_defs.h:318:    superlu_scope_t cscp; /* process scope in columnwise, vertical direction */
./superlu_defs.h:319:    int iam;              /* my process number in this scope */
./superlu_defs.h:320:    int_t nprow;          /* number of process rows */
./superlu_defs.h:321:    int_t npcol;          /* number of process columns */
./superlu_defs.h:346: * (xlsub,lsub): lsub[*] contains the compressed subscript of
./superlu_defs.h:353: *	(xlsub,lsub) for the purpose of symmetric pruning. For each
./superlu_defs.h:362: *	It is for the purpose of symmetric pruning. Therefore, the
./superlu_defs.h:371: *	The last column structures (for pruning) will be removed
./superlu_defs.h:394:    int_t     *lsub;     /* compressed L subscripts */
./superlu_defs.h:396:    int_t     *usub;     /* compressed U subscripts */
./superlu_defs.h:400:    LU_space_t MemModel; /* 0 - system malloc'd; 1 - user provided */
./superlu_defs.h:419: *        = ROW:  Row equilibration, i.e., A was premultiplied by diag(R).
./superlu_defs.h:435: *        Row permutation vector which defines the permutation matrix Pr,
./superlu_defs.h:436: *        perm_r[i] = j means row i of A is in position j in Pr*A.
./superlu_defs.h:473: *-- This contains the options used to control the solution process.
./superlu_defs.h:483: *             pattern was performed prior to this one. Therefore, this
./superlu_defs.h:490: *             prior to this one. Therefore, this factorization will reuse
./superlu_defs.h:493: *             data structure set up from the previous symbolic factorization.
./superlu_defs.h:507: *        = COLAMD: use approximate minimum degree column ordering
./superlu_defs.h:519: *        = SINGLE: perform iterative refinement in single precision
./superlu_defs.h:520: *        = DOUBLE: perform iterative refinement in double precision
./superlu_defs.h:521: *        = EXTRA: perform iterative refinement in extra precision
./superlu_defs.h:529: *        preference to diagonal pivots, and uses an (A'+A)-based column
./superlu_defs.h:533: *        Specifies whether to compute the reciprocal pivot growth.
./superlu_defs.h:536: *        Specifies whether to compute the reciprocal condition number.
./superlu_defs.h:544: * ILU_DropRule (int)  (only for serial SuperLU)
./superlu_defs.h:547: *	  = DROP_PROWS:   Supernodal based ILUTP(p,tau), p = gamma * nnz(A)/n.
./superlu_defs.h:563: *   	  Note: DROP_PROWS, DROP_COLUMN and DROP_AREA are mutually exclusive.
./superlu_defs.h:622:    int 	  ILU_DropRule;
./superlu_defs.h:633:    yes_no_t      PrintStat;
./superlu_defs.h:681: * Function prototypes
./superlu_defs.h:693:extern void   print_options_dist(superlu_dist_options_t *);
./superlu_defs.h:694:extern void   print_sp_ienv_dist(superlu_dist_options_t *);
./superlu_defs.h:699:extern void   Destroy_CompRowLoc_Matrix_dist(SuperMatrix *);
./superlu_defs.h:700:extern void   Destroy_CompRow_Matrix_dist(SuperMatrix *);
./superlu_defs.h:748:extern void  get_diag_procs(int_t, Glu_persist_t *, gridinfo_t *, int_t *,
./superlu_defs.h:755:extern void  PStatPrint(superlu_dist_options_t *, SuperLUStat_t *, gridinfo_t *);
./superlu_defs.h:757:extern void  print_memorylog(SuperLUStat_t *, char *);
./superlu_defs.h:764:/* Prototypes for parallel symbolic factorization */
./superlu_defs.h:789:extern int_t psymbfact_prLUXpand
./superlu_defs.h:799:extern int get_thread_per_process();
./superlu_defs.h:807:extern void  print_panel_seg_dist(int_t, int_t, int_t, int_t, int_t *, int_t *);
./superlu_defs.h:810:extern void  PrintDouble5(char *, int_t, double *);
./superlu_defs.h:811:extern void  PrintInt10(char *, int_t, int_t *);
./superlu_defs.h:812:extern void  PrintInt32(char *, int, int *);
./superlu_defs.h:813:extern int   file_PrintInt10(FILE *, char *, int_t, int_t *);
./superlu_defs.h:814:extern int   file_PrintInt32(FILE *, char *, int, int *);
./superlu_defs.h:815:extern int   file_PrintLong10(FILE *, char *, int_t, int_t *);
./superlu_defs.h:828:extern RdTree   RdTree_Create(MPI_Comm comm, int* ranks, int rank_cnt, int msgSize, double rseed, char precision);  
./superlu_defs.h:829:extern void   	RdTree_Destroy(RdTree Tree, char precision);
./superlu_defs.h:830:extern void 	RdTree_SetTag(RdTree Tree, int tag, char precision);
./superlu_defs.h:831:extern yes_no_t RdTree_IsRoot(RdTree Tree, char precision);
./superlu_defs.h:832:extern void 	RdTree_forwardMessageSimple(RdTree Tree, void* localBuffer, int msgSize, char precision);
./superlu_defs.h:833:extern void 	RdTree_allocateRequest(RdTree Tree, char precision);
./superlu_defs.h:834:extern int  	RdTree_GetDestCount(RdTree Tree, char precision);
./superlu_defs.h:835:extern int  	RdTree_GetMsgSize(RdTree Tree, char precision);
./superlu_defs.h:836:extern void 	RdTree_waitSendRequest(RdTree Tree, char precision);
./superlu_defs.h:838:extern BcTree   BcTree_Create(MPI_Comm comm, int* ranks, int rank_cnt, int msgSize, double rseed, char precision);  
./superlu_defs.h:839:extern void   	BcTree_Destroy(BcTree Tree, char precision);
./superlu_defs.h:840:extern void 	BcTree_SetTag(BcTree Tree, int tag, char precision);
./superlu_defs.h:841:extern yes_no_t BcTree_IsRoot(BcTree Tree, char precision);
./superlu_defs.h:842:extern void 	BcTree_forwardMessageSimple(BcTree Tree, void* localBuffer, int msgSize, char precision);
./superlu_defs.h:843:extern void 	BcTree_allocateRequest(BcTree Tree, char precision);
./superlu_defs.h:844:extern int 		BcTree_getDestCount(BcTree Tree, char precision); 
./superlu_defs.h:845:extern int 		BcTree_GetMsgSize(BcTree Tree, char precision); 
./superlu_defs.h:846:extern void 	BcTree_waitSendRequest(BcTree Tree, char precision);
./superlu_dist_version.c:4:approvals from U.S. Dept. of Energy) 
./superlu_enum_consts.h:4:approvals from U.S. Dept. of Energy) 
./superlu_enum_consts.h:45:	      DROP_PROWS	= 0x0002, /* ILUTP: keep p maximum rows */
./superlu_enum_consts.h:51:	      DROP_SECONDARY	= 0x000E, /* PROWS | COLUMN | AREA */
./superlu_enum_consts.h:73:    COMM_DIAG, /* Bcast diagonal block to process column */
./superlu_enum_consts.h:80:    RCOND,   /* estimate reciprocal condition number */
./superlu_grid.c:4:approvals from U.S. Dept. of Energy) 
./superlu_grid.c:14: * <pre>
./superlu_grid.c:18: * </pre>
./superlu_grid.c:26:/*! \brief All processes in the MPI communicator must call this routine.
./superlu_grid.c:30:		      int_t nprow, int_t npcol, gridinfo_t *grid)
./superlu_grid.c:32:    int Np = nprow * npcol;
./superlu_grid.c:36:    /* Make a list of the processes in the new communicator. */
./superlu_grid.c:39:	for (i = 0; i < nprow; ++i) usermap[j*nprow+i] = i*npcol+j;
./superlu_grid.c:44:	ABORT("C main program must explicitly call MPI_Init()");
./superlu_grid.c:48:	ABORT("Number of processes is smaller than NPROW * NPCOL");
./superlu_grid.c:50:    superlu_gridmap(Bcomm, nprow, npcol, usermap, nprow, grid);
./superlu_grid.c:56:/*! \brief All processes in the MPI communicator must call this routine.
./superlu_grid.c:61:		     int_t nprow,
./superlu_grid.c:63:		     int_t usermap[], /* usermap(i,j) holds the process
./superlu_grid.c:65:					 the process grid.  */
./superlu_grid.c:71:    int Np = nprow * npcol, mycol, myrow;
./superlu_grid.c:72:    int *pranks;
./superlu_grid.c:84:	ABORT("C main program must explicitly call MPI_Init()");
./superlu_grid.c:86:    grid->nprow = nprow;
./superlu_grid.c:89:    /* Make a list of the processes in the new communicator. */
./superlu_grid.c:90:    pranks = (int *) SUPERLU_MALLOC(Np*sizeof(int));
./superlu_grid.c:92:	for (i = 0; i < nprow; ++i)
./superlu_grid.c:93:	    pranks[i*npcol+j] = usermap[j*ldumap+i];
./superlu_grid.c:101:    MPI_Group_incl( mpi_base_group, Np, pranks, &superlu_grp );
./superlu_grid.c:103:    /* NOTE: The call is to be executed by all processes in Bcomm,
./superlu_grid.c:113:	SUPERLU_FREE(pranks);
./superlu_grid.c:125:    for (i = 0; i < npcol; ++i) pranks[i] = myrow*npcol + i;
./superlu_grid.c:127:    MPI_Group_incl( superlu_grp, npcol, pranks, &grp );  /* Form new group */
./superlu_grid.c:137:    for (i = 0; i < nprow; ++i) pranks[i] = i*npcol + mycol;
./superlu_grid.c:138:    MPI_Group_incl( superlu_grp, nprow, pranks, &grp );  /* Form new group */
./superlu_grid.c:146:    grid->cscp.Np = nprow;
./superlu_grid.c:154:	    printf("MPI_TAG_UB %d\n", tag_ub);
./superlu_grid.c:162:    SUPERLU_FREE(pranks);
./superlu_timer.c:4:approvals from U.S. Dept. of Energy) 
./superlu_timer.c:12: * \brief Returns the time in seconds used by the process
./superlu_timer.c:14: * <pre>
./superlu_timer.c:17: *	Returns the time in seconds used by the process.
./superlu_timer.c:20: *       compilation to choose the appropriate function.
./superlu_timer.c:21: * </pre>
./superlu_zdefs.h:4:approvals from U.S. Dept. of Energy) 
./superlu_zdefs.h:13: * \brief  Distributed SuperLU data types and function prototypes
./superlu_zdefs.h:15: * <pre>
./superlu_zdefs.h:19: * April 5, 2015
./superlu_zdefs.h:21: * </pre>
./superlu_zdefs.h:29: * Purpose:     Distributed SuperLU data types and function prototypes
./superlu_zdefs.h:43: * On each processor, the blocks in L are stored in compressed block
./superlu_zdefs.h:44: * column format, the blocks in U are stored in compressed block row format.
./superlu_zdefs.h:55:    int_t   **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
./superlu_zdefs.h:56:    doublecomplex  **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
./superlu_zdefs.h:59:    RdTree  *LRtree_ptr;       /* size ceil(NSUPERS/Pr)                */
./superlu_zdefs.h:61:    RdTree  *URtree_ptr;       /* size ceil(NSUPERS/Pr)			*/
./superlu_zdefs.h:84:    int   **ToSendR;        /* List of processes to send right block col. */
./superlu_zdefs.h:88:    int_t   **fsendx_plist;   /* Column process list to send down Xk       */
./superlu_zdefs.h:89:    int_t   *frecv;           /* Modifications to be recv'd in proc row    */
./superlu_zdefs.h:93:    int_t   **bsendx_plist;   /* Column process list to send down Xk       */
./superlu_zdefs.h:94:    int_t   *brecv;           /* Modifications to be recv'd in proc row    */
./superlu_zdefs.h:117:    int_t **ut_sendx_plist;  /* Row process list to send down Xk            */
./superlu_zdefs.h:118:    int_t *utrecv;           /* Modifications to be recev'd in proc column. */
./superlu_zdefs.h:146:    int_t *ind_tosend;    /* X indeices to be sent to other processes */
./superlu_zdefs.h:147:    int_t *ind_torecv;    /* X indeices to be received from other processes */
./superlu_zdefs.h:148:    int_t *ptr_ind_tosend;/* Printers to ind_tosend[] (Size procs)
./superlu_zdefs.h:150:    int_t *ptr_ind_torecv;/* Printers to ind_torecv[] (Size procs)
./superlu_zdefs.h:156:    doublecomplex *val_tosend;   /* X values to be sent to other processes */
./superlu_zdefs.h:157:    doublecomplex *val_torecv;   /* X values to be received from other processes */
./superlu_zdefs.h:166:    int_t *row_to_proc;
./superlu_zdefs.h:168:    int_t num_diag_procs, *diag_procs, *diag_len;
./superlu_zdefs.h:176:    int_t *xrow_to_proc; /* used by PDSLin */
./superlu_zdefs.h:181: * Function prototypes
./superlu_zdefs.h:194:zCreate_CompRowLoc_Matrix_dist(SuperMatrix *, int_t, int_t, int_t, int_t,
./superlu_zdefs.h:198:zCompRow_to_CompCol_dist(int_t, int_t, int_t, doublecomplex *, int_t *, int_t *,
./superlu_zdefs.h:201:pzCompRow_loc_to_CompCol_global(int_t, SuperMatrix *, gridinfo_t *,
./superlu_zdefs.h:283:/* #define GPU_PROF
./superlu_zdefs.h:284:#define IPM_PROF */
./superlu_zdefs.h:351:extern void zClone_CompRowLoc_Matrix_dist(SuperMatrix *, SuperMatrix *);
./superlu_zdefs.h:352:extern void zCopy_CompRowLoc_Matrix_dist(SuperMatrix *, SuperMatrix *);
./superlu_zdefs.h:353:extern void zZero_CompRowLoc_Matrix_dist(SuperMatrix *);
./superlu_zdefs.h:354:extern void zScaleAddId_CompRowLoc_Matrix_dist(SuperMatrix *, doublecomplex);
./superlu_zdefs.h:355:extern void zScaleAdd_CompRowLoc_Matrix_dist(SuperMatrix *, SuperMatrix *, doublecomplex);
./superlu_zdefs.h:382:extern void  zPrintLblocks(int, int_t, gridinfo_t *, Glu_persist_t *,
./superlu_zdefs.h:384:extern void  zPrintUblocks(int, int_t, gridinfo_t *, Glu_persist_t *,
./superlu_zdefs.h:386:extern void  zPrint_CompCol_Matrix_dist(SuperMatrix *);
./superlu_zdefs.h:387:extern void  zPrint_Dense_Matrix_dist(SuperMatrix *);
./superlu_zdefs.h:388:extern int   zPrint_CompRowLoc_Matrix_dist(SuperMatrix *);
./superlu_zdefs.h:389:extern int   file_zPrint_CompRowLoc_Matrix_dist(FILE *fp, SuperMatrix *A);																			   
./superlu_zdefs.h:390:extern void  PrintDoublecomplex(char *, int_t, doublecomplex *);
./superlu_zdefs.h:391:extern int   file_PrintDoublecomplex(FILE *fp, char *, int_t, doublecomplex *);
./supermatrix.h:4:approvals from U.S. Dept. of Energy) 
./supermatrix.h:32:    SLU_NR_loc  /* distributed compressed row format  */ 
./supermatrix.h:55:	Stype_t Stype; /* Storage type: interprets the storage structure 
./supermatrix.h:58:	Mtype_t Mtype; /* Matrix type: describes the mathematical property of 
./supermatrix.h:101:  int_t *rowind;     /* pointer to array of compressed row indices of 
./supermatrix.h:129:  int_t  *rowind;      /* pointer to array of compressed row indices of 
./supermatrix.h:172:    void *nzval;  /* array of size lda*ncol to represent a dense matrix */
./supermatrix.h:175:/* Stype == SLU_NR_loc (Distributed Compressed Row Format) */
./supermatrix.h:178:    int_t m_loc;     /* number of rows local to this processor */
./symbfact.c:4:approvals from U.S. Dept. of Energy) 
./symbfact.c:14: * <pre>
./symbfact.c:21:  THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY
./symbfact.c:22:  EXPRESSED OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.
./symbfact.c:24:  Permission is hereby granted to use or copy this program for any
./symbfact.c:25:  purpose, provided the above notices are retained on all copies.
./symbfact.c:27:  granted, provided the above notices are retained, and a notice that
./symbfact.c:29: * </pre>
./symbfact.c:43: * Internal protypes
./symbfact.c:55:static void  pruneL(const int_t, const int_t *, const int_t, const int_t,
./symbfact.c:63: * <pre>
./symbfact.c:71: *        o symmetric structure pruning
./symbfact.c:78: * </pre>
./symbfact.c:84: int         pnum,     /* process number */
./symbfact.c:95:    int_t *xprune, *marker, *parent, *xplore;
./symbfact.c:120:    xprune = xplore + m;
./symbfact.c:121:    relax_end = xprune + n;
./symbfact.c:144:	    if ( (info = snode_dfs(A, j, k, xprune, marker,
./symbfact.c:156:				   xprune, marker, parent, xplore,
./symbfact.c:167:	    /* Prune columns [0:j-1] using column j. */
./symbfact.c:168:	    pruneL(j, perm_r, pivrow, nseg, segrep, repfnz, xprune,
./symbfact.c:171:	    /* Reset repfnz[*] to prepare for the next column. */
./symbfact.c:181:    countnz_dist(min_mn, xprune, &nnzL, &nnzU, Glu_persist, Glu_freeable);
./symbfact.c:183:    /* Apply perm_r to L; Compress LSUB array. */
./symbfact.c:186:    if ( !pnum && (options->PrintStat == YES)) {
./symbfact.c:188:	printf("\tNonzeros in L       " IFMT "\n", nnzL);
./symbfact.c:189:	printf("\tNonzeros in U       " IFMT "\n", nnzU);
./symbfact.c:190:	printf("\tnonzeros in L+U     " IFMT "\n", nnzLU);
./symbfact.c:191:	printf("\tnonzeros in LSUB    " IFMT "\n", nnzLSUB);
./symbfact.c:195:#if ( PRNTlevel>=3 )
./symbfact.c:196:    PrintInt10("lsub", Glu_freeable->xlsub[n], Glu_freeable->lsub);
./symbfact.c:197:    PrintInt10("xlsub", n+1, Glu_freeable->xlsub);
./symbfact.c:198:    PrintInt10("xprune", n, xprune);
./symbfact.c:199:    PrintInt10("usub", Glu_freeable->xusub[n], Glu_freeable->usub);
./symbfact.c:200:    PrintInt10("xusub", n+1, Glu_freeable->xusub);
./symbfact.c:201:    PrintInt10("supno", n, Glu_persist->supno);
./symbfact.c:202:    PrintInt10("xsup", (Glu_persist->supno[n])+2, Glu_persist->xsup);
./symbfact.c:217: * <pre>
./symbfact.c:222: * </pre>
./symbfact.c:266:    printf(".. No of relaxed snodes: " IFMT "\trelax: " IFMT "\n", nsuper, relax);
./symbfact.c:274: * <pre> 
./symbfact.c:286: * </pre>
./symbfact.c:294: int_t       *xprune,   /* pruned location in each adjacency list (output) */
./symbfact.c:343:    /* Supernode > 1, then make a copy of the subscripts for pruning */
./symbfact.c:361:    xprune[kcol]   = nextl;
./symbfact.c:363:#if ( PRNTlevel>=3 )
./symbfact.c:364:    printf(".. snode_dfs(): (%8d:%8d) nextl %d\n", jcol, kcol, nextl);
./symbfact.c:374: * <pre>
./symbfact.c:383: *   A supernode representative is the last column of a supernode.
./symbfact.c:385: *   representatives. The routine returns a list of such supernodal 
./symbfact.c:386: *   representatives ( segrep[*] ) in topological order of the DFS that 
./symbfact.c:393: *      lsub[*] contains the compressed subscripts of the supernodes;
./symbfact.c:399: *	(lsub, xlsub, xprune) for the purpose of symmetric pruning.
./symbfact.c:402: *      structures (for pruning) will be removed in the end.
./symbfact.c:408: *	    It is for the purpose of symmetric pruning. Therefore, the
./symbfact.c:413: *          are stored. Column t represents pruned adjacency structure.
./symbfact.c:421: *                                       :         xprune[t]
./symbfact.c:423: *                                   xprune[s]    
./symbfact.c:426: *          is used for both G(L) and pruned graph:
./symbfact.c:433: *                                  xprune[s]
./symbfact.c:436: *       pruned graph.
./symbfact.c:453: * </pre>
./symbfact.c:462: int_t       *segrep,   /* list of U-segment representatives (output) */
./symbfact.c:464: int_t       *xprune,   /* pruned location in each adjacency list (output) */
./symbfact.c:482:    int_t     ito, ifrom, istop;	/* used to compress row subscripts */
./symbfact.c:555:		maxdfs = xprune[krep];
./symbfact.c:598:				    maxdfs = xprune[krep];
./symbfact.c:615:		    maxdfs = xprune[krep];
./symbfact.c:626:	jptr = xlsub[jcol];	/* Not compressed yet */
./symbfact.c:637:	 * lsub[*] from the previous supernode. Note we only store
./symbfact.c:639:	 * a supernode. (first for G(L'), last for pruned graph)
./symbfact.c:643:#ifdef CHK_COMPRESS
./symbfact.c:644:		printf("  Compress lsub[] at super %d-%d\n",fsupc,jcolm1);
./symbfact.c:649:		xprune[jcolm1] = istop; /* Initialize xprune[jcol-1] */
./symbfact.c:664:    xprune[jcol]   = nextl; /* Initialize an upper bound for pruning. */
./symbfact.c:672: * <pre>
./symbfact.c:678: * </pre>
./symbfact.c:693:    int_t  nsupr;       /* number of rows in the supernode */
./symbfact.c:706:    nsupr    = xlsub[fsupc+1] - lptr;
./symbfact.c:713:    for (isub = nsupc; isub < nsupr; ++isub)
./symbfact.c:721:	printf("At column " IFMT ", ", jcol);
./symbfact.c:744: * <pre> 
./symbfact.c:755: *   representative.
./symbfact.c:757: * </pre>
./symbfact.c:765: int_t       *segrep, /* list of U-segment representatives (output) */
./symbfact.c:820:static void pruneL
./symbfact.c:829: int_t  *xprune,       /* out */
./symbfact.c:837: *   pruneL() prunes the L-structure of supernodes whose L-structure
./symbfact.c:843:    int_t  do_prune; /* logical variable */
./symbfact.c:859:	/* Do not prune with a zero U-segment */
./symbfact.c:863:	 * If irep has not been pruned & it has a nonzero in row L[pivrow,i]
./symbfact.c:865:	do_prune = FALSE;
./symbfact.c:867:	    if ( xprune[irep] >= xlsub[irep1] ) {
./symbfact.c:872:			do_prune = TRUE;
./symbfact.c:877:    	    if ( do_prune ) {
./symbfact.c:894:	        xprune[irep] = kmin; /* Pruning */
./symbfact.c:896:		printf(".. pruneL(): use col %d: xprune[%d] = %d\n",
./symbfact.c:899:	    } /* if do_prune */
./symbfact.c:902:} /* PRUNEL */
./tags:2:ASSERT	./colamd.c	/^#define ASSERT(expression) (mxAssert ((expression)/
./tags:12:COL_IS_DEAD_PRINCIPAL	./colamd.c	/^#define COL_IS_DEAD_PRINCIPAL(c)	(Col [c].start ==/
./tags:17:DEBUG0	./colamd.c	/^#define DEBUG0(params) { (void) PRINTF params ; }$/
./tags:25:Destroy_CompRowLoc_Matrix_dist	./util.c	/^Destroy_CompRowLoc_Matrix_dist(SuperMatrix *A)$/
./tags:26:Destroy_CompRow_Matrix_dist	./util.c	/^Destroy_CompRow_Matrix_dist(SuperMatrix *A)$/
./tags:45:KILL_NON_PRINCIPAL_COL	./colamd.c	/^#define KILL_NON_PRINCIPAL_COL(c)	{ Col [c].start /
./tags:46:KILL_PRINCIPAL_COL	./colamd.c	/^#define KILL_PRINCIPAL_COL(c)		{ Col [c].start = D/
./tags:48:LBi	./superlu_defs.h	/^#define LBi(bnum,grid)  ( (bnum)\/grid->nprow )\/*/
./tags:50:LOCAL_IND	./psymbfact.h	/^#define LOCAL_IND(x)  ((x) % maxNvtcsPProc)$/
./tags:73:OWNER	./psymbfact.h	/^#define OWNER(x)      ((x) \/ maxNvtcsPProc)$/
./tags:76:PROW	./superlu_defs.h	/^#define PROW(bnum,grid) ( (bnum) % grid->nprow )$/
./tags:79:PStatPrint	./util.c	/^PStatPrint(superlu_dist_options_t *options, SuperL/
./tags:83:PrintDouble5	./dutil_dist.c	/^void PrintDouble5(char *name, int_t len, double *x/
./tags:84:PrintDoublecomplex	./zutil_dist.c	/^void PrintDoublecomplex(char *name, int_t len, dou/
./tags:85:PrintInt10	./util.c	/^void PrintInt10(char *name, int_t len, int_t *x)$/
./tags:86:PrintInt32	./util.c	/^void PrintInt32(char *name, int len, int *x)$/
./tags:139:a_plus_at_CompRow_loc	./get_perm_c_parmetis.c	/^($/
./tags:141:allocPrune_domain	./psymbfact.c	/^($/
./tags:142:allocPrune_lvl	./psymbfact.c	/^($/
./tags:160:countnz_dist	./util.c	/^countnz_dist(const int_t n, int_t *xprune,$/
./tags:164:dClone_CompRowLoc_Matrix_dist	./dutil_dist.c	/^void dClone_CompRowLoc_Matrix_dist(SuperMatrix *A,/
./tags:165:dCompRow_to_CompCol_dist	./dutil_dist.c	/^dCompRow_to_CompCol_dist(int_t m, int_t n, int_t n/
./tags:167:dCopy_CompRowLoc_Matrix_dist	./dutil_dist.c	/^void dCopy_CompRowLoc_Matrix_dist(SuperMatrix *A, /
./tags:170:dCreate_CompRowLoc_Matrix_dist	./dutil_dist.c	/^dCreate_CompRowLoc_Matrix_dist(SuperMatrix *A, int/
./tags:177:dPrintLblocks	./dutil_dist.c	/^void dPrintLblocks(int iam, int_t nsupers, gridinf/
./tags:178:dPrintMSRmatrix	./pdgsmv_AXglobal.c	/^($/
./tags:179:dPrintUblocks	./dutil_dist.c	/^void dPrintUblocks(int iam, int_t nsupers, gridinf/
./tags:180:dPrint_CompCol_Matrix_dist	./dutil_dist.c	/^void dPrint_CompCol_Matrix_dist(SuperMatrix *A)$/
./tags:181:dPrint_CompRowLoc_Matrix_dist	./dutil_dist.c	/^int dPrint_CompRowLoc_Matrix_dist(SuperMatrix *A)$/
./tags:182:dPrint_Dense_Matrix_dist	./dutil_dist.c	/^void dPrint_Dense_Matrix_dist(SuperMatrix *A)$/
./tags:186:dScaleAddId_CompRowLoc_Matrix_dist	./dutil_dist.c	/^void dScaleAddId_CompRowLoc_Matrix_dist(SuperMatri/
./tags:187:dScaleAdd_CompRowLoc_Matrix_dist	./dutil_dist.c	/^void dScaleAdd_CompRowLoc_Matrix_dist(SuperMatrix /
./tags:190:dZero_CompRowLoc_Matrix_dist	./dutil_dist.c	/^void dZero_CompRowLoc_Matrix_dist(SuperMatrix *A)$/
./tags:220:dprint_gsmv_comm	./dutil_dist.c	/^dprint_gsmv_comm(FILE *fp, int_t m_loc, pdgsmv_com/
./tags:242:file_PrintDouble5	./dutil_dist.c	/^int file_PrintDouble5(FILE *fp, char *name, int_t /
./tags:243:file_PrintDoublecomplex	./zutil_dist.c	/^int file_PrintDoublecomplex(FILE *fp, char *name, /
./tags:244:file_PrintInt10	./util.c	/^int file_PrintInt10(FILE *fp, char *name, int_t le/
./tags:245:file_PrintInt32	./util.c	/^int file_PrintInt32(FILE *fp, char *name, int len,/
./tags:246:file_dPrint_CompRowLoc_Matrix_dist	./dutil_dist.c	/^int file_dPrint_CompRowLoc_Matrix_dist(FILE *fp, S/
./tags:247:file_zPrint_CompRowLoc_Matrix_dist	./zutil_dist.c	/^int file_zPrint_CompRowLoc_Matrix_dist(FILE *fp, S/
./tags:258:gemm_profile	./pdgstrf.c	/^    } gemm_profile;$/
./tags:262:get_diag_procs	./util.c	/^get_diag_procs(int_t n, Glu_persist_t *Glu_persist/
./tags:265:get_min	./util.c	/^get_min (int_t * sums, int_t nprocs)$/
./tags:269:get_thread_per_process	./util.c	/^int get_thread_per_process()$/
./tags:319:pdCompRow_loc_to_CompCol_global	./pdutil.c	/^($/
./tags:346:print_memorylog	./util.c	/^void print_memorylog(SuperLUStat_t *stat, char *ms/
./tags:347:print_options_dist	./util.c	/^void print_options_dist(superlu_dist_options_t *op/
./tags:348:print_panel_seg_dist	./util.c	/^void print_panel_seg_dist(int_t n, int_t w, int_t /
./tags:349:print_report	./colamd.c	/^($/
./tags:350:print_sp_ienv_dist	./util.c	/^void print_sp_ienv_dist(superlu_dist_options_t *op/
./tags:351:probe_recv	./pdgstrf_X1.c	/^probe_recv(int iam, int source, int tag, MPI_Datat/
./tags:357:pzCompRow_loc_to_CompCol_global	./pzutil.c	/^($/
./tags:427:updateRcvd_prGraph	./psymbfact.c	/^($/
./tags:435:zClone_CompRowLoc_Matrix_dist	./zutil_dist.c	/^void zClone_CompRowLoc_Matrix_dist(SuperMatrix *A,/
./tags:436:zCompRow_to_CompCol_dist	./zutil_dist.c	/^zCompRow_to_CompCol_dist(int_t m, int_t n, int_t n/
./tags:438:zCopy_CompRowLoc_Matrix_dist	./zutil_dist.c	/^void zCopy_CompRowLoc_Matrix_dist(SuperMatrix *A, /
./tags:441:zCreate_CompRowLoc_Matrix_dist	./zutil_dist.c	/^zCreate_CompRowLoc_Matrix_dist(SuperMatrix *A, int/
./tags:448:zPrintLblocks	./zutil_dist.c	/^void zPrintLblocks(int iam, int_t nsupers, gridinf/
./tags:449:zPrintMSRmatrix	./pzgsmv_AXglobal.c	/^($/
./tags:450:zPrintUblocks	./zutil_dist.c	/^void zPrintUblocks(int iam, int_t nsupers, gridinf/
./tags:451:zPrint_CompCol_Matrix_dist	./zutil_dist.c	/^void zPrint_CompCol_Matrix_dist(SuperMatrix *A)$/
./tags:452:zPrint_CompRowLoc_Matrix_dist	./zutil_dist.c	/^int zPrint_CompRowLoc_Matrix_dist(SuperMatrix *A)$/
./tags:453:zPrint_Dense_Matrix_dist	./zutil_dist.c	/^void zPrint_Dense_Matrix_dist(SuperMatrix *A)$/
./tags:457:zScaleAddId_CompRowLoc_Matrix_dist	./zutil_dist.c	/^void zScaleAddId_CompRowLoc_Matrix_dist(SuperMatri/
./tags:458:zScaleAdd_CompRowLoc_Matrix_dist	./zutil_dist.c	/^void zScaleAdd_CompRowLoc_Matrix_dist(SuperMatrix /
./tags:461:zZero_CompRowLoc_Matrix_dist	./zutil_dist.c	/^void zZero_CompRowLoc_Matrix_dist(SuperMatrix *A)$/
./tags:480:zprint_gsmv_comm	./zutil_dist.c	/^zprint_gsmv_comm(FILE *fp, int_t m_loc, pzgsmv_com/
./util.c:4:approvals from U.S. Dept. of Energy) 
./util.c:14: * <pre>
./util.c:21: * </pre>
./util.c:46:Destroy_CompRowLoc_Matrix_dist(SuperMatrix *A)
./util.c:56:Destroy_CompRow_Matrix_dist(SuperMatrix *A)
./util.c:123:    nb = CEILING(nsupers, grid->nprow);
./util.c:254: * <pre>
./util.c:257: * </pre>
./util.c:260:countnz_dist(const int_t n, int_t *xprune,
./util.c:294:	nnzL0 += xprune[irep] - xlsub[irep];
./util.c:297:    /* printf("\tNo of nonzeros in symm-reduced L = %ld\n", nnzL0);*/
./util.c:312: * <pre>
./util.c:314: * sets for structural pruning,	and applies permuation to the remaining
./util.c:316: * </pre>
./util.c:373:    options->PrintStat         = YES;
./util.c:384:/*! \brief Print the options setting.
./util.c:386:void print_options_dist(superlu_dist_options_t *options)
./util.c:388:    if ( options->PrintStat == NO ) return;
./util.c:390:    printf("**************************************************\n");
./util.c:391:    printf(".. options:\n");
./util.c:392:    printf("**    Fact             : %4d\n", options->Fact);
./util.c:393:    printf("**    Equil            : %4d\n", options->Equil);
./util.c:394:    printf("**    ParSymbFact      : %4d\n", options->ParSymbFact);
./util.c:395:    printf("**    ColPerm          : %4d\n", options->ColPerm);
./util.c:396:    printf("**    RowPerm          : %4d\n", options->RowPerm);
./util.c:397:    printf("**    ReplaceTinyPivot : %4d\n", options->ReplaceTinyPivot);
./util.c:398:    printf("**    IterRefine       : %4d\n", options->IterRefine);
./util.c:399:    printf("**    Trans            : %4d\n", options->Trans);
./util.c:400:    printf("**    num_lookaheads   : %4d\n", options->num_lookaheads);
./util.c:401:    printf("**    SymPattern       : %4d\n", options->SymPattern);
./util.c:402:    printf("**    lookahead_etree  : %4d\n", options->lookahead_etree);
./util.c:403:    printf("**************************************************\n");
./util.c:406:/*! \brief Print the blocking parameters.
./util.c:408:void print_sp_ienv_dist(superlu_dist_options_t *options)
./util.c:410:    if ( options->PrintStat == NO ) return;
./util.c:412:    printf("**************************************************\n");
./util.c:413:    printf(".. blocking parameters from sp_ienv():\n");
./util.c:414:    printf("**    relaxation                 : " IFMT "\n", sp_ienv_dist(2));
./util.c:415:    printf("**    max supernode              : " IFMT "\n", sp_ienv_dist(3));
./util.c:416:    printf("**    estimated fill ratio       : " IFMT "\n", sp_ienv_dist(6));
./util.c:417:    printf("**    min GEMM dimension for GPU : " IFMT "\n", sp_ienv_dist(7));
./util.c:418:    printf("**************************************************\n");
./util.c:424: * <pre>
./util.c:452: *        The 2D process mesh.
./util.c:453: * </pre>
./util.c:464:    int_t *row_to_proc;
./util.c:465:    int_t i, gbi, k, l, num_diag_procs, *diag_procs;
./util.c:467:    int   iam, p, pkk, procs;
./util.c:470:    procs = grid->nprow * grid->npcol;
./util.c:476:    row_to_proc = SOLVEstruct->row_to_proc;
./util.c:481:    if ( !(itemp = SUPERLU_MALLOC(8*procs * sizeof(int))) )
./util.c:484:    SendCnt_nrhs = itemp +   procs;
./util.c:485:    RecvCnt      = itemp + 2*procs;
./util.c:486:    RecvCnt_nrhs = itemp + 3*procs;
./util.c:487:    sdispls      = itemp + 4*procs;
./util.c:488:    sdispls_nrhs = itemp + 5*procs;
./util.c:489:    rdispls      = itemp + 6*procs;
./util.c:490:    rdispls_nrhs = itemp + 7*procs;
./util.c:492:    /* Count the number of elements to be sent to each diagonal process.*/
./util.c:493:    for (p = 0; p < procs; ++p) SendCnt[p] = 0;
./util.c:495:        irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
./util.c:497:	p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
./util.c:504:    for (p = 1; p < procs; ++p) {
./util.c:508:    for (p = 0; p < procs; ++p) {
./util.c:522:    if ( !(itemp = SUPERLU_MALLOC(8*procs * sizeof(int))) )
./util.c:525:    SendCnt_nrhs = itemp +   procs;
./util.c:526:    RecvCnt      = itemp + 2*procs;
./util.c:527:    RecvCnt_nrhs = itemp + 3*procs;
./util.c:528:    sdispls      = itemp + 4*procs;
./util.c:529:    sdispls_nrhs = itemp + 5*procs;
./util.c:530:    rdispls      = itemp + 6*procs;
./util.c:531:    rdispls_nrhs = itemp + 7*procs;
./util.c:533:    /* Count the number of X entries to be sent to each process.*/
./util.c:534:    for (p = 0; p < procs; ++p) SendCnt[p] = 0;
./util.c:535:    num_diag_procs = SOLVEstruct->num_diag_procs;
./util.c:536:    diag_procs = SOLVEstruct->diag_procs;
./util.c:538:    for (p = 0; p < num_diag_procs; ++p) { /* for all diagonal processes */
./util.c:539:	pkk = diag_procs[p];
./util.c:541:	    for (k = p; k < nsupers; k += num_diag_procs) {
./util.c:546:		    q = row_to_proc[inv_perm_c[irow]];
./util.c:548:		    q = row_to_proc[irow];
./util.c:562:    for (p = 1; p < procs; ++p) {
./util.c:574:    if ( !(ptr_to_ibuf = SUPERLU_MALLOC(2*procs * sizeof(int))) )
./util.c:577:    gstrs_comm->ptr_to_dbuf = ptr_to_ibuf + procs;
./util.c:592:/*! \brief Diagnostic print of segment info after panel_dfs().
./util.c:594:void print_panel_seg_dist(int_t n, int_t w, int_t jcol, int_t nseg, 
./util.c:600:	printf("\tcol " IFMT ":\n", j);
./util.c:602:	    printf("\t\tseg " IFMT ", segrep " IFMT ", repfnz " IFMT "\n", k, 
./util.c:625:PStatPrint(superlu_dist_options_t *options, SuperLUStat_t *stat, gridinfo_t *grid)
./util.c:632:    if ( options->PrintStat == NO ) return;
./util.c:635:	printf("**************************************************\n");
./util.c:636:	printf("**** Time (seconds) ****\n");
./util.c:639:	    printf("\tEQUIL time         %8.2f\n", utime[EQUIL]);
./util.c:641:	    printf("\tROWPERM time       %8.2f\n", utime[ROWPERM]);
./util.c:643:	    printf("\tCOLPERM time       %8.2f\n", utime[COLPERM]);
./util.c:644:        printf("\tSYMBFACT time      %8.2f\n", utime[SYMBFAC]);
./util.c:645:	printf("\tDISTRIBUTE time    %8.2f\n", utime[DIST]);
./util.c:653:	printf("\tFACTOR time        %8.2f\n", utime[FACT]);
./util.c:655:	    printf("\tFactor flops\t%e\tMflops \t%8.2f\n",
./util.c:664:	printf("\tSOLVE time         %8.3f\n", utime[SOLVE]);
./util.c:666:	    printf("\tSolve flops\t%e\tMflops \t%8.2f\n",
./util.c:670:	    printf("\tREFINEMENT time    %8.3f\tSteps%8d\n\n",
./util.c:673:	printf("**************************************************\n");
./util.c:678:#if ( PROFlevel>=1 )
./util.c:683:	int_t i, P = grid->nprow*grid->npcol;
./util.c:695:	// if ( !iam ) printf("\n.. Tree max sizes:\tbtree\trtree\n");
./util.c:701:		// printf("\t\t%d %5d %5d\n", iam, stat->MaxActiveBTrees,stat->MaxActiveRTrees);
./util.c:712:	if ( !iam ) printf("\n.. FACT time breakdown:\tcomm\ttotal\n");
./util.c:718:		printf("\t\t(%d)%8.2f%8.2f\n", i, utime1[i], utime2[i]);
./util.c:723:	if ( !iam ) printf("\n.. FACT ops distribution:\n");
./util.c:728:		printf("\t\t(%d)\t%e\n", i, ops1[i]);
./util.c:737:	    printf("\tFACT load balance: %.2f\n", b);
./util.c:743:	if ( !iam ) printf("\n.. SOLVE time breakdown:\tcommL \tgemmL\ttrsmL\ttotal\n");
./util.c:751:		printf("\t\t\t%d%10.5f%10.5f%10.5f%10.5f\n", i,utime1[i],utime2[i],utime3[i], utime4[i]);
./util.c:756:	if ( !iam ) printf("\n.. SOLVE ops distribution:\n"); 
./util.c:760:		printf("\t\t%d\t%e\n", i, ops1[i]);
./util.c:765:	    printf("\tSOLVE load balance: %.2f\n", b);
./util.c:801:get_diag_procs(int_t n, Glu_persist_t *Glu_persist, gridinfo_t *grid,
./util.c:802:	       int_t *num_diag_procs, int_t **diag_procs, int_t **diag_len)
./util.c:804:    int_t i, j, k, knsupc, nprow, npcol, nsupers, pkk;
./util.c:807:    i = j = *num_diag_procs = pkk = 0;
./util.c:808:    nprow = grid->nprow;
./util.c:814:	++(*num_diag_procs);
./util.c:815:	i = (++i) % nprow;
./util.c:818:    } while ( pkk != 0 ); /* Until wrap back to process 0 */
./util.c:819:    if ( !(*diag_procs = intMalloc_dist(*num_diag_procs)) )
./util.c:820:	ABORT("Malloc fails for diag_procs[]");
./util.c:821:    if ( !(*diag_len = intCalloc_dist(*num_diag_procs)) )
./util.c:823:    for (i = j = k = 0; k < *num_diag_procs; ++k) {
./util.c:825:	(*diag_procs)[k] = pkk;
./util.c:826:	i = (++i) % nprow;
./util.c:831:	i = k % *num_diag_procs;
./util.c:856:    printf("    Supernode statistics:\n\tno of super = " IFMT "\n", nsuper+1);
./util.c:857:    printf("\tmax supernode size = " IFMT "\n", max_sup_size);
./util.c:858:    printf("\tno of size 1 supernodes = " IFMT "\n", nsup1);
./util.c:870:    printf("\tHistogram of supernode sizes:\n");
./util.c:874:        printf("\tsnode: " IFMT "-" IFMT "\t\t" IFMT "\n", bl+1, bh, bucket[i]);
./util.c:889:		fprintf(stderr, "col " IFMT ", repfnz_col[" IFMT "] = " IFMT "\n",
./util.c:895:void PrintInt10(char *name, int_t len, int_t *x)
./util.c:899:    printf("%10s:", name);
./util.c:901:	if ( i % 10 == 0 ) printf("\n\t[" IFMT "-" IFMT "]", i, i+9);
./util.c:902:	printf(IFMT, x[i]);
./util.c:904:    printf("\n");
./util.c:907:void PrintInt32(char *name, int len, int *x)
./util.c:911:    printf("%10s:", name);
./util.c:913:	if ( i % 10 == 0 ) printf("\n\t[%2d-%2d]", i, i+9);
./util.c:914:	printf("%6d", x[i]);
./util.c:916:    printf("\n");
./util.c:919:int file_PrintInt10(FILE *fp, char *name, int_t len, int_t *x)
./util.c:923:    fprintf(fp, "%10s:", name);
./util.c:925:	if ( i % 10 == 0 ) fprintf(fp, "\n\t[" IFMT "-" IFMT "]", i, i+9);
./util.c:926:	fprintf(fp, IFMT, x[i]);
./util.c:928:    fprintf(fp, "\n");
./util.c:932:int file_PrintInt32(FILE *fp, char *name, int len, int *x)
./util.c:936:    fprintf(fp, "%10s:", name);
./util.c:938:	if ( i % 10 == 0 ) fprintf(fp, "\n\t[%2d-%2d]", i, i+9);
./util.c:939:	fprintf(fp, "%6d", x[i]);
./util.c:941:    fprintf(fp, "\n");
./util.c:957:#if ( PRNTlevel>=2 )
./util.c:958:	    printf(".. Diagonal of column %d is zero.\n", j);
./util.c:1027: * ARRAY   (input/output) DOUBLE PRECISION ARRAY of LENGTH N
./util.c:1061:void print_memorylog(SuperLUStat_t *stat, char *msg) {
./util.c:1062:    printf("__ %s (MB):\n\tcurrent_buffer : %8.2f\tpeak_buffer : %8.2f\n",
./util.c:1071:int get_thread_per_process()
./util.c:1074:    ttemp = getenv("THREAD_PER_PROCESS");
./util.c:1114:get_min (int_t * sums, int_t nprocs)
./util.c:1119:    for (int i = 0; i < nprocs; i++)
./util.c:1133:		  int_t ldp, int_t * sums, int_t * counts, int nprocs)
./util.c:1136:    for (int i = 0; i < nprocs; ++i)
./util.c:1145:        int_t ind = get_min (sums, nprocs);
./util.c:1146:        // printf("ind %d\n",ind );
./util.c:1191:        // printf("iukp %d \n",*iukp );
./util.c:1193:        // printf("jb %d \n",*jb );
./util.c:1195:        // printf("nsupc %d \n",*nsupc );
./util.c:1211: * Count the maximum size of U(k,:) across all the MPI processes.
./util.c:1264:    int_t Pr = grid->nprow;
./util.c:1276:    for (int lk = myrow; lk < nsupers; lk += Pr ) {
./util.c:1287:#if ( PRNTlevel>=1 )
./util.c:1289:    printf("max_ncols " IFMT ", max_ldu " IFMT ", ldt " IFMT ", bigu_size " IFMT "\n",
./util.c:1345:	   	// printf("dims: %5d",dims);
./util_dist.h:4:approvals from U.S. Dept. of Energy) 
./util_dist.h:33:   sprintf(msg,"%s at line %d in file %s\n",err_msg,__LINE__, __FILE__);\
./util_dist.h:51:    printf("(%d) %s: superlu_malloc_total (MB) %.6f\n", \
./util_dist.h:135:#if ( PROFlevel>=1 )
./wingetopt.c:14:* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
./wingetopt.h:14:* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
./xerr_dist.c:4:approvals from U.S. Dept. of Energy) 
./xerr_dist.c:14:<pre>
./xerr_dist.c:21:</pre> 
./xerr_dist.c:29:    printf("** On entry to %6s, parameter number %2d had an illegal value\n",
./zSchCompUdt-2Ddynamic.c:4:approvals from U.S. Dept. of Energy) 
./zSchCompUdt-2Ddynamic.c:17: * <pre>
./zSchCompUdt-2Ddynamic.c:152:	 /* if ( iam==0 ) printf("--- k0 %d, k %d, jj0 %d, nub %d\n", k0, k, jj0, nub);*/
./zSchCompUdt-2Ddynamic.c:178:		 printf("j %d: Ublock_info[j].iukp %d, Ublock_info[j].rukp %d,"
./zSchCompUdt-2Ddynamic.c:183:	     /* Prepare to call GEMM. */
./zSchCompUdt-2Ddynamic.c:203:	 /* Now doing prefix sum on full_u_cols.
./zSchCompUdt-2Ddynamic.c:240:         /* Gather U(k,:) into buffer bigU[] to prepare for GEMM */
./zSchCompUdt-2Ddynamic.c:242:#pragma omp parallel for firstprivate(iukp, rukp) \
./zSchCompUdt-2Ddynamic.c:243:    private(j,tempu, jb, nsupc,ljb,segsize, lead_zero, jj, i) \
./zSchCompUdt-2Ddynamic.c:252:            /* == processing each of the remaining columns in parallel == */
./zSchCompUdt-2Ddynamic.c:271:#pragma omp simd
./zSchCompUdt-2Ddynamic.c:282:	if (ldu==0) printf("[%d] .. k0 %d, before updating: ldu %d, Lnbrow %d, Rnbrow %d, ncols %d\n",iam,k0,ldu,Lnbrow,Rnbrow, ncols);
./zSchCompUdt-2Ddynamic.c:301:#pragma omp parallel for private(j,jj,tempu,tempv) default (shared)
./zSchCompUdt-2Ddynamic.c:316:	 /* #pragma omp parallel for (gives slow down) */
./zSchCompUdt-2Ddynamic.c:325:	     tempv = &lusup[luptr+j*nsupr + StRowSource];
./zSchCompUdt-2Ddynamic.c:327:#pragma omp simd
./zSchCompUdt-2Ddynamic.c:333:		    &lusup[luptr+j*nsupr + StRowSource],
./zSchCompUdt-2Ddynamic.c:341:#pragma omp parallel for private(i,j,jj,tempu,tempv) default (shared) \
./zSchCompUdt-2Ddynamic.c:357:	 // #pragma omp parallel for (gives slow down)
./zSchCompUdt-2Ddynamic.c:360:	     // printf("StRowDest %d Rnbrow %d StRowSource %d \n", StRowDest,Rnbrow ,StRowSource);
./zSchCompUdt-2Ddynamic.c:366:	     tempv = &lusup[luptr + j*nsupr + StRowSource];
./zSchCompUdt-2Ddynamic.c:368:#pragma omp simd
./zSchCompUdt-2Ddynamic.c:374:		    &lusup[luptr+j*nsupr + StRowSource],
./zSchCompUdt-2Ddynamic.c:403:#pragma omp parallel default (shared) private(thread_id)
./zSchCompUdt-2Ddynamic.c:422:#pragma omp for \
./zSchCompUdt-2Ddynamic.c:423:    private (nsupc,ljb,lptr,ib,temp_nbrow,cum_nrow)	\
./zSchCompUdt-2Ddynamic.c:469:#if ( PRNTlevel>= 1)
./zSchCompUdt-2Ddynamic.c:488:#if (PRNTlevel>=1 )
./zSchCompUdt-2Ddynamic.c:528:#if ( PRNTlevel>=1 )
./zSchCompUdt-2Ddynamic.c:549:#if ( PRNTlevel>=1 )
./zSchCompUdt-2Ddynamic.c:555:	/* printf("[%d] .. k0 %d, before large GEMM: %d-%d-%d, RemainBlk %d\n",
./zSchCompUdt-2Ddynamic.c:574:#if ( PRNTlevel>=1 )
./zSchCompUdt-2Ddynamic.c:577:#if ( PROFlevel>=1 )
./zSchCompUdt-2Ddynamic.c:578:	//fprintf(fgemm, "%8d%8d%8d %16.8e\n", Rnbrow, ncols, ldu,
./zSchCompUdt-2Ddynamic.c:596:#pragma omp parallel default(shared) private(thread_id)
./zSchCompUdt-2Ddynamic.c:615:#pragma omp for \
./zSchCompUdt-2Ddynamic.c:616:    private (j,lb,rukp,iukp,jb,nsupc,ljb,lptr,ib,temp_nbrow,cum_nrow)	\
./zSchCompUdt-2Ddynamic.c:658:		// printf("[%d] .. before scatter: ib %d, jb %d, temp_nbrow %d, Rnbrow %d\n", iam, ib, jb, temp_nbrow, Rnbrow); fflush(stdout);
./zSchCompUdt-2Ddynamic.c:693:#if ( PRNTlevel>=1 )
./zSchCompUdt-cuda.c:4:approvals from U.S. Dept. of Energy) 
./zSchCompUdt-cuda.c:17: * <pre>
./zSchCompUdt-cuda.c:30:            fprintf(stderr, "Fatal cublas error: %d (at %s:%d)\n", \
./zSchCompUdt-cuda.c:33:            fprintf(stderr, "*** FAILED - ABORTING\n"); \
./zSchCompUdt-cuda.c:79:        // #pragma omp barrier 
./zSchCompUdt-cuda.c:83:#pragma omp single
./zSchCompUdt-cuda.c:89:                    /* prefix sum */
./zSchCompUdt-cuda.c:95:                    /* the number of columns that can be processed is limited by buffer size*/
./zSchCompUdt-cuda.c:113:		       full_u_cols + jjj_st, /*array containing prefix sum of work load*/
./zSchCompUdt-cuda.c:118:            } /* pragma omp single */
./zSchCompUdt-cuda.c:121:            // printf("thread_id %d, jjj %d \n",thread_id,jjj );
./zSchCompUdt-cuda.c:123:                printf("allocate more memory for buffer !!!!\n");
./zSchCompUdt-cuda.c:125:                    printf("%d buffer_size %d\n",nbrow*full_u_cols[jjj_st],buffer_size );
./zSchCompUdt-cuda.c:128:            // #pragma omp barrier 
./zSchCompUdt-cuda.c:134:#pragma omp for schedule( SCHEDULE_STRATEGY )
./zSchCompUdt-cuda.c:140:                /* == processing each of the remaining columns == */
./zSchCompUdt-cuda.c:164:		printf("nbrow %d *ldu %d  =%d < ldt %d * max_row_size %d =%d \n",nbrow,ldu,nbrow*ldu,ldt,max_row_size,ldt*max_row_size );
./zSchCompUdt-cuda.c:168:				  &lusup[luptr+(knsupc-ldu)*nsupr],
./zSchCompUdt-cuda.c:169:				  nsupr*sizeof(doublecomplex), nbrow*sizeof(doublecomplex),
./zSchCompUdt-cuda.c:221:			      &alpha, &lusup[luptr+(knsupc-ldu)*nsupr],
./zSchCompUdt-cuda.c:222:			      &nsupr, tempu+ldu*st_col, &ldu, &beta,
./zSchCompUdt-cuda.c:238:		  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr,
./zSchCompUdt-cuda.c:242:		  &lusup[luptr+(knsupc-ldu)*nsupr], &nsupr,
./zSchCompUdt-cuda.c:248:	    // printf("after zgemm \n");
./zSchCompUdt-cuda.c:257:#pragma omp parallel  \
./zSchCompUdt-cuda.c:258:    private(j,iukp,rukp, tempu, tempv, cum_nrow, jb, nsupc,ljb,	\
./zSchCompUdt-cuda.c:263:    firstprivate(luptr,lptr) default (shared)
./zSchCompUdt-cuda.c:278:                            printf("scattering %d  block column\n",j);
./zSchCompUdt-cuda.c:281:                        /* == processing each of the remaining columns == */
./zSchCompUdt-cuda.c:296:#pragma omp for schedule( SCHEDULE_STRATEGY ) nowait
./zSchCompUdt-cuda.c:321:                                    printf("cpu scatter \n");
./zSchCompUdt-cuda.c:322:                                    printf("A(%d,%d) goes to U block %d \n", ib,jb,ljb);
./zSchCompUdt-cuda.c:338:                                printf("cpu scatter \n");
./zSchCompUdt-cuda.c:339:                                printf("A(%d,%d) goes to L block %d \n", ib,jb,ljb);
./zSchCompUdt-cuda.c:364:#pragma omp for schedule(SCHEDULE_STRATEGY) nowait
./zSchCompUdt-cuda.c:369:                            printf("scattering %d  block column\n",j);
./zSchCompUdt-cuda.c:372:                        /* == processing each of the remaining columns == */
./zSchCompUdt-cuda.c:396:			    printf("%d %d %d \n",temp_nbrow, temp_ncol,ldu);
./zSchCompUdt-cuda.c:402:				printf("cpu scatter \n");
./zSchCompUdt-cuda.c:403:				printf("A(%d,%d) goes to U block %d \n", ib,jb,ljb);
./zSchCompUdt-cuda.c:419:                                printf("cpu scatter \n");
./zSchCompUdt-cuda.c:420:                                printf("A(%d,%d) goes to L block %d \n", ib,jb,ljb);
./zSchCompUdt-cuda.c:445:#pragma omp parallel							\
./zSchCompUdt-cuda.c:446:    private(j,iukp,rukp, tempu, tempv, cum_nrow, jb, nsupc,ljb,		\
./zSchCompUdt-cuda.c:451:    firstprivate(luptr,lptr) default (shared)
./zSchCompUdt-cuda.c:459:                for(i = 0; i < num_streams_used; i++) { /* i is private variable */
./zSchCompUdt-cuda.c:467:#pragma omp for schedule( SCHEDULE_STRATEGY ) nowait 
./zSchCompUdt-cuda.c:471:			printf("scattering %d  block column\n",j);
./zSchCompUdt-cuda.c:473:                        /* == processing each of the remaining columns == */
./zSchCompUdt-cuda.c:497:			    printf("%d %d %d \n",temp_nbrow, temp_ncol,ldu);
./zSchCompUdt-cuda.c:503:				printf("gpu scatter \n");
./zSchCompUdt-cuda.c:504:				printf("A(%d,%d) goes to U block %d \n", ib,jb,ljb);
./zSchCompUdt-cuda.c:519:                                printf("gpu scatter \n");
./zSchCompUdt-cuda.c:520:                                printf("A(%d,%d) goes to L block %d \n", ib,jb,ljb);
./zSchCompUdt-cuda.c:544:            } /* end pragma omp parallel */
./zbinary_io.c:11:    printf("fread n " IFMT "\tnnz " IFMT "\n", *n, *nnz);
./zbinary_io.c:19:    printf("# of doubles fread: %d\n", nnz_read);
./zbinary_io.c:36:    printf("n " IFMT ", # of doublecomplex: " IFMT "\n", n, nnz);
./zbinary_io.c:37:    printf("dump binary file ... # of doubles fwrite: %d\n", nnz_written);
./zdistribute.c:4:approvals from U.S. Dept. of Energy) 
./zdistribute.c:13: * \brief Distribute the matrix onto the 2D process mesh.
./zdistribute.c:15: * <pre>
./zdistribute.c:19: * </pre>
./zdistribute.c:25: * <pre>
./zdistribute.c:28: *   Distribute the matrix onto the 2D process mesh.
./zdistribute.c:51: *        The 2D process mesh.
./zdistribute.c:56: * </pre>
./zdistribute.c:72:    int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
./zdistribute.c:73:    int iam, jbrow, kcol, mycol, myrow, pc, pr;
./zdistribute.c:90:    doublecomplex **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
./zdistribute.c:91:    int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
./zdistribute.c:98:    int_t  **fsendx_plist; /* Column process list to send down Xk.   */
./zdistribute.c:105:    int_t  **bsendx_plist; /* Column process list to send down Xk.   */
./zdistribute.c:112:    int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./zdistribute.c:113:    int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
./zdistribute.c:114:    int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
./zdistribute.c:115:    int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
./zdistribute.c:117:    int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
./zdistribute.c:118:    int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
./zdistribute.c:119:    int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
./zdistribute.c:120:    int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
./zdistribute.c:127:#if ( PRNTlevel>=1 )
./zdistribute.c:130:#if ( PROFlevel>=1 ) 
./zdistribute.c:146:#if ( PRNTlevel>=1 )
./zdistribute.c:157:         * REUSE THE L AND U DATA STRUCTURES FROM A PREVIOUS FACTORIZATION.
./zdistribute.c:160:#if ( PROFlevel>=1 )
./zdistribute.c:163:	/* We can propagate the new values of A into the existing
./zdistribute.c:169:	nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
./zdistribute.c:178:#if ( PRNTlevel>=1 )
./zdistribute.c:181:#if ( PROFlevel>=1 )
./zdistribute.c:198:	    if ( mycol == pc ) { /* Block column jb in my process column */
./zdistribute.c:207:			if ( myrow == PROW( gb, grid ) ) {
./zdistribute.c:239:#if ( PROFlevel>=1 )
./zdistribute.c:270:#if ( PROFlevel>=1 )
./zdistribute.c:279:#if ( PROFlevel>=1 )
./zdistribute.c:280:	if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
./zdistribute.c:289:#if ( PROFlevel>=1 )
./zdistribute.c:293:	   We need to set up the L and U data structures and propagate
./zdistribute.c:295:	lsub = Glu_freeable->lsub;    /* compressed L subscripts */
./zdistribute.c:297:	usub = Glu_freeable->usub;    /* compressed U subscripts */
./zdistribute.c:310:#if ( PRNTlevel>=1 )
./zdistribute.c:315:	k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./zdistribute.c:342:#if ( PRNTlevel>=1 )	
./zdistribute.c:349:	    if ( myrow == PROW( gb, grid ) ) {
./zdistribute.c:360:	   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
./zdistribute.c:377:		    pr = PROW( gb, grid );
./zdistribute.c:380:			if  ( myrow == pr ) {
./zdistribute.c:389:#if ( PRNTlevel>=1 )
./zdistribute.c:401:	nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./zdistribute.c:430:#if ( PROFlevel>=1 )
./zdistribute.c:432:	if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
./zdistribute.c:434:#if ( PRNTlevel>=1 )
./zdistribute.c:457:#if ( PRNTlevel>=1 )	
./zdistribute.c:469:	/* These lists of processes will be used for triangular solves. */
./zdistribute.c:472:	len = k * grid->nprow;
./zdistribute.c:476:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./zdistribute.c:483:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./zdistribute.c:485:#if ( PRNTlevel>=1 )
./zdistribute.c:489:	  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
./zdistribute.c:490:	  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
./zdistribute.c:495:	    if ( mycol == pc ) { /* Block column jb in my process column */
./zdistribute.c:505:			if ( myrow == PROW( gb, grid ) ) {
./zdistribute.c:514:		jbrow = PROW( jb, grid );
./zdistribute.c:516:#if ( PROFlevel>=1 )
./zdistribute.c:532:			pr = PROW( gb, grid );
./zdistribute.c:533:			if ( pr != jbrow &&
./zdistribute.c:534:			     myrow == jbrow &&  /* diag. proc. owning jb */
./zdistribute.c:535:			     bsendx_plist[ljb][pr] == EMPTY ) {
./zdistribute.c:536:			    bsendx_plist[ljb][pr] = YES;
./zdistribute.c:539:			if ( myrow == pr ) {
./zdistribute.c:577:			} /* if myrow == pr ... */
./zdistribute.c:582:#if ( PROFlevel>=1 )
./zdistribute.c:599:		    pr = PROW( gb, grid ); /* Process row owning this block */
./zdistribute.c:600:		    if ( pr != jbrow &&
./zdistribute.c:601:			 myrow == jbrow &&  /* diag. proc. owning jb */
./zdistribute.c:602:			 fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
./zdistribute.c:603:			fsendx_plist[ljb][pr] = YES;
./zdistribute.c:606:		    if ( myrow == pr ) {
./zdistribute.c:618:#if ( PRNTlevel>=1 )
./zdistribute.c:637:			fprintf(stderr, "col block " IFMT " ", jb);
./zdistribute.c:659:		    /* Propagate the compressed row subscripts to Lindex[], and
./zdistribute.c:666:			if ( myrow == PROW( gb, grid ) ) {
./zdistribute.c:684:#if ( PROFlevel>=1 )
./zdistribute.c:709:#if ( PRNTlevel>=1 )
./zdistribute.c:710:	if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
./zdistribute.c:724:	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./zdistribute.c:732:#if ( PROFlevel>=1 )
./zdistribute.c:733:	if ( !iam ) printf(".. 1st distribute time:\n "
./zdistribute_mark.c:4:approvals from U.S. Dept. of Energy) 
./zdistribute_mark.c:12: * \brief Distribute the matrix onto the 2D process mesh
./zdistribute_mark.c:14: * <pre>
./zdistribute_mark.c:26: * Date: Apr 23 09:54:15 PDT 2001
./zdistribute_mark.c:27: * </pre>
./zdistribute_mark.c:33: * <pre>
./zdistribute_mark.c:41: *   Distribute the matrix onto the 2D process mesh.
./zdistribute_mark.c:64: *        The 2D process mesh.
./zdistribute_mark.c:65: * </pre>
./zdistribute_mark.c:78:    int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
./zdistribute_mark.c:79:    int iam, jbrow, kcol, mycol, myrow, pc, pr;
./zdistribute_mark.c:95:    doublecomplex **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
./zdistribute_mark.c:96:    int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
./zdistribute_mark.c:103:    int_t  **fsendx_plist; /* Column process list to send down Xk.   */
./zdistribute_mark.c:109:    int_t  **bsendx_plist; /* Column process list to send down Xk.   */
./zdistribute_mark.c:115:    int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
./zdistribute_mark.c:116:    int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
./zdistribute_mark.c:117:    int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
./zdistribute_mark.c:118:    int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
./zdistribute_mark.c:120:    int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
./zdistribute_mark.c:121:    int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
./zdistribute_mark.c:122:    int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
./zdistribute_mark.c:123:    int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
./zdistribute_mark.c:128:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:143:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:153:	/* We can propagate the new values of A into the existing
./zdistribute_mark.c:159:	nrbu = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./zdistribute_mark.c:170:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:175:	    if ( mycol == pc ) { /* Block column jb in my process column */
./zdistribute_mark.c:184:			if ( myrow == PROW( gb, grid ) ) {
./zdistribute_mark.c:199:			gb = lb * grid->nprow + myrow;/* Global block number */
./zdistribute_mark.c:250:	   We need to set up the L and U data structures and propagate
./zdistribute_mark.c:252:	lsub = Glu_freeable->lsub;    /* compressed L subscripts */
./zdistribute_mark.c:254:	usub = Glu_freeable->usub;    /* compressed U subscripts */
./zdistribute_mark.c:266:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:272:	k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
./zdistribute_mark.c:298:#if ( PRNTlevel>=1 )	
./zdistribute_mark.c:305:	    if ( myrow == PROW( gb, grid ) ) {
./zdistribute_mark.c:316:	   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
./zdistribute_mark.c:333:		    pr = PROW( gb, grid );
./zdistribute_mark.c:336:			if  ( myrow == pr ) {
./zdistribute_mark.c:345:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:357:	nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
./zdistribute_mark.c:386:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:408:#if ( PRNTlevel>=1 )	
./zdistribute_mark.c:421:	/* These lists of processes will be used for triangular solves. */
./zdistribute_mark.c:424:	len = k * grid->nprow;
./zdistribute_mark.c:428:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./zdistribute_mark.c:435:	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
./zdistribute_mark.c:437:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:442:	  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
./zdistribute_mark.c:443:	  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
./zdistribute_mark.c:448:	    if ( mycol == pc ) { /* Block column jb in my process column */
./zdistribute_mark.c:459:			if ( myrow == PROW( gb, grid ) ) {
./zdistribute_mark.c:468:		jbrow = PROW( jb, grid );
./zdistribute_mark.c:481:			pr = PROW( gb, grid );
./zdistribute_mark.c:482:			if ( pr != jbrow ) 
./zdistribute_mark.c:483:			    bsendx_plist[ljb][pr] = YES;
./zdistribute_mark.c:484:			if ( myrow == pr ) {
./zdistribute_mark.c:507:			} /* if myrow == pr ... */
./zdistribute_mark.c:519:			if ( myrow == PROW( gb, grid ) ) {
./zdistribute_mark.c:531:			    gb = lb*grid->nprow + myrow; /* Global block # */
./zdistribute_mark.c:550:			gb = lb * grid->nprow + myrow;/* Global block number */
./zdistribute_mark.c:580:		    pr = PROW( gb, grid ); /* Process row owning this block */
./zdistribute_mark.c:581:		    if ( pr != jbrow )
./zdistribute_mark.c:582:			fsendx_plist[ljb][pr] = YES;
./zdistribute_mark.c:583:		    if ( myrow == pr ) {
./zdistribute_mark.c:595:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:615:			fprintf(stderr, "col block %d ", jb);
./zdistribute_mark.c:637:		    /* Propagate the compressed row subscripts to Lindex[], and
./zdistribute_mark.c:644:			if ( myrow == PROW( gb, grid ) ) {
./zdistribute_mark.c:683:#if ( PRNTlevel>=1 )
./zdistribute_mark.c:684:	if ( !iam ) printf(".. # L blocks %d\t# U blocks %d\n",
./zgsequ_dist.c:4:approvals from U.S. Dept. of Energy) 
./zgsequ_dist.c:24:<pre>   
./zgsequ_dist.c:37:    works well in practice.   
./zgsequ_dist.c:80:</pre>
./zlangs_dist.c:4:approvals from U.S. Dept. of Energy) 
./zlangs_dist.c:24:<pre> 
./zlangs_dist.c:59:</pre>
./zlaqgs_dist.c:4:approvals from U.S. Dept. of Energy) 
./zlaqgs_dist.c:24:<pre>
./zlaqgs_dist.c:59:            = 'R':  Row equilibration, i.e., A has been premultiplied by  
./zlaqgs_dist.c:79:</pre>
./zlaqgs_dist.c:105:    small = dmach_dist("Safe minimum") / dmach_dist("Precision");
./zldperm_dist.c:4:approvals from U.S. Dept. of Energy) 
./zldperm_dist.c:15: * <pre>
./zldperm_dist.c:19: * </pre>
./zldperm_dist.c:30: * <pre>
./zldperm_dist.c:54: *        = 5 : Compute a row permutation of the matrix so that the product
./zldperm_dist.c:90: * </pre>
./zldperm_dist.c:115:    printf("LDPERM(): n %d, nnz %d\n", n, nnz);
./zldperm_dist.c:116:    PrintInt10("colptr", n+1, colptr);
./zldperm_dist.c:117:    PrintInt10("adjncy", nnz, adjncy);
./zldperm_dist.c:127:     * Since a symmetric permutation preserves the diagonal entries. Then
./zldperm_dist.c:137:    /* Suppress error and warning messages. */
./zldperm_dist.c:146:    PrintInt10("perm", n, perm);
./zldperm_dist.c:147:    printf(".. After MC64AD info %d\tsize of matching %d\n", info[0], num);
./zldperm_dist.c:150:        printf(".. The last " IFMT " permutations:\n", n-num);
./zldperm_dist.c:151:	PrintInt10("perm", n-num, &perm[num]);
./zlook_ahead_update.c:4:approvals from U.S. Dept. of Energy) 
./zlook_ahead_update.c:16: * <pre>
./zlook_ahead_update.c:88:    printf ("(%d) k=%d,jb=%d,ldu=%d,ncols=%d,nsupc=%d\n",
./zlook_ahead_update.c:121:       'firstprivate' ensures that the private variables are initialized
./zlook_ahead_update.c:123:#pragma omp parallel for \
./zlook_ahead_update.c:124:    firstprivate(lptr,luptr,ib,current_b) private(lb) \
./zlook_ahead_update.c:128:        int temp_nbrow; /* automatic variable is private */
./zlook_ahead_update.c:163:                   &lusup[luptr + (knsupc - ldu) * nsupr], &nsupr,
./zlook_ahead_update.c:167:                   &lusup[luptr + (knsupc - ldu) * nsupr], &nsupr,
./zlook_ahead_update.c:235:        /* Multicasts numeric values of L(:,kk) to process rows. */
./zlook_ahead_update.c:250:        scp = &grid->rscp;      /* The scope of process row. */
./zlook_ahead_update.c:253:#if ( PROFlevel>=1 )
./zlook_ahead_update.c:262:#if ( PROFlevel>=1 )
./zlook_ahead_update.c:269:                printf ("[%d] -2- Send L(:,%4d): #lsub %4d, #lusup %4d to Pj %2d, tags %d:%d \n",
./zmemory_dist.c:4:approvals from U.S. Dept. of Energy) 
./zmemory_dist.c:15: * <pre>
./zmemory_dist.c:19: * </pre>
./zmemory_dist.c:62: * <pre>
./zmemory_dist.c:70: * </pre>
./zmemory_dist.c:105:    nb = CEILING( nsupers, grid->nprow ); /* Number of local row blocks */
./zmemory_dist.c:107:	gb = k * grid->nprow + myrow; /* Global block number. */
./zmemory_dist.c:127:    k = CEILING( nsupers, grid->nprow );
./zmemory_dist.c:133:#if ( PRNTlevel>=1 )
./zmemory_dist.c:134:    if (iam==0) printf(".. zQuerySpace: peak_buffer %.2f (MB)\n",
./zmyblas2_dist.c:4:approvals from U.S. Dept. of Energy) 
./zmyblas2_dist.c:15: * <pre>
./zmyblas2_dist.c:20: * </pre>
./zmyblas2_dist.c:33: * <pre>
./zmyblas2_dist.c:37: * </pre>
./zmyblas2_dist.c:112: * <pre>
./zmyblas2_dist.c:116: * </pre>
./zmyblas2_dist.c:151: * <pre>
./zmyblas2_dist.c:153: * The input matrix is M(1:nrow,1:ncol); The product is returned in Mxvec[].
./zmyblas2_dist.c:154: * </pre> 
./zreadMM.c:4:approvals from U.S. Dept. of Energy) 
./zreadMM.c:26: * <pre>
./zreadMM.c:33: * </pre>
./zreadMM.c:62:       printf("Invalid header (first line does not contain 5 tokens)\n");
./zreadMM.c:67:       printf("Invalid header (first token is not \"%%%%MatrixMarket\")\n");
./zreadMM.c:72:       printf("Not a matrix; this driver cannot handle that.\n");
./zreadMM.c:77:       printf("Not in coordinate format; this driver cannot handle that.\n");
./zreadMM.c:83:         printf("Complex matrix; use dreadMM instead!\n");
./zreadMM.c:87:         printf("Pattern matrix; values are needed!\n");
./zreadMM.c:91:         printf("Unknown arithmetic\n");
./zreadMM.c:97:       printf("Symmetric matrix: will be expanded\n");
./zreadMM.c:116:      printf("Rectangular matrix!. Abort\n");
./zreadMM.c:126:    printf("m %lld, n %lld, nonz %lld\n", (long long) *m, (long long) *n, (long long) *nonz);
./zreadMM.c:153:		printf("triplet file: row/col indices are zero-based.\n");
./zreadMM.c:155:		printf("triplet file: row/col indices are one-based.\n");
./zreadMM.c:167:	    fprintf(stderr, "nz " IFMT ", (" IFMT ", " IFMT ") = {%e\t%e} out of bound, removed\n", 
./zreadMM.c:187:      printf("new_nonz after symmetric expansion:\t" IFMT "\n", *nonz);
./zreadMM.c:223:	printf("Col %d, xa %d\n", i, xa[i]);
./zreadMM.c:225:	    printf("%d\t%16.10f\n", asub[k], a[k]);
./zreadMM.c:238:        fprintf(stderr, "zreadrhs: file does not exist\n");
./zreadhb.c:4:approvals from U.S. Dept. of Energy) 
./zreadhb.c:13: * \brief Read a DOUBLE COMPLEX PRECISION matrix stored in Harwell-Boeing format
./zreadhb.c:15: * <pre>
./zreadhb.c:19: * </pre>
./zreadhb.c:27: * Prototypes
./zreadhb.c:37: * <pre>
./zreadhb.c:41: * Read a DOUBLE COMPLEX PRECISION matrix stored in Harwell-Boeing format 
./zreadhb.c:55: *		       if present) 
./zreadhb.c:56: *           	      (zero indicates no right-hand side data is present) 
./zreadhb.c:72: * Line 5 (A3, 11X, 2I14) Only present if there are right-hand sides present 
./zreadhb.c:101: * </pre>
./zreadhb.c:134:    if ( !iam ) printf("Matrix type %s\n", type);
./zreadhb.c:143:	if ( !iam ) printf("This is not an assembled matrix!\n");
./zreadhb.c:145:	if ( !iam ) printf("Matrix is not square.\n");
./zreadhb.c:166:	printf(IFMT " rows, " IFMT " nonzeros\n", *nrow, *nonz);
./zreadhb.c:167:	printf("colnum " IFMT ", colsize " IFMT "\n", colnum, colsize);
./zreadhb.c:168:	printf("rownum " IFMT ", rowsize " IFMT "\n", rownum, rowsize);
./zreadhb.c:169:	printf("valnum " IFMT ", valsize " IFMT "\n", valnum, valsize);
./zreadhb.c:175:    if ( !iam )	printf("read colptr[" IFMT "] = " IFMT "\n", *ncol, (*colptr)[*ncol]);
./zreadhb.c:179:    if ( !iam )	printf("read rowind[" IFMT "] = " IFMT "\n", *nonz-1, (*rowind)[*nonz-1]);
./zreadrb.c:4:approvals from U.S. Dept. of Energy) 
./zreadrb.c:15: * <pre>
./zreadrb.c:20: * </pre>
./zreadrb.c:25: * Read a DOUBLE COMPLEX PRECISION matrix stored in Rutherford-Boeing format 
./zreadrb.c:40: *      Col. 15 - 28  Compressed Column: Number of rows (NROW)
./zreadrb.c:42: *      Col. 30 - 42  Compressed Column: Number of columns (NCOL)
./zreadrb.c:44: *      Col. 44 - 56  Compressed Column: Number of entries (NNZERO)
./zreadrb.c:46: *      Col. 58 - 70  Compressed Column: Unused, explicitly zero
./zreadrb.c:77: *      A Compressed column form
./zreadrb.c:80: * </pre>
./zreadrb.c:191: * <pre>
./zreadrb.c:192: * On input, nonz/nzval/rowind/colptr represents lower part of a symmetric
./zreadrb.c:193: * matrix. On exit, it represents the full matrix with lower and upper parts.
./zreadrb.c:194: * </pre>
./zreadrb.c:266:    printf("FormFullA: new_nnz = " IFMT ", k = " IFMT "\n", new_nnz, k);
./zreadrb.c:308:    if ( !iam ) printf("Matrix type %s\n", type);
./zreadrb.c:317:        if ( !iam ) printf("This is not an assembled matrix!\n");
./zreadrb.c:319:        if ( !iam ) printf("Matrix is not square.\n");
./zreadrb.c:336:        printf(IFMT " rows, " IFMT " nonzeros\n", *nrow, *nonz);
./zreadrb.c:337:        printf("colnum " IFMT ", colsize " IFMT "\n", colnum, colsize);
./zreadrb.c:338:        printf("rownum " IFMT ", rowsize " IFMT "\n", rownum, rowsize);
./zreadrb.c:339:        printf("valnum " IFMT ", valsize " IFMT "\n", valnum, valsize);
./zreadtriple.c:4:approvals from U.S. Dept. of Energy) 
./zreadtriple.c:23: * <pre>
./zreadtriple.c:30: * </pre>
./zreadtriple.c:60:    printf("m %lld, n %lld, nonz %lld\n", (long long) *m, (long long) *n, (long long) *nonz);
./zreadtriple.c:86:		printf("triplet file: row/col indices are zero-based.\n");
./zreadtriple.c:88:		printf("triplet file: row/col indices are one-based.\n");
./zreadtriple.c:98:	    fprintf(stderr, "nz " IFMT ", (" IFMT ", " IFMT ") = {%e\t%e} out of bound, removed\n", 
./zreadtriple.c:118:    printf("new_nonz after symmetric expansion:\t%d\n", *nonz);
./zreadtriple.c:153:	printf("Col %d, xa %d\n", i, xa[i]);
./zreadtriple.c:155:	    printf("%d\t%16.10f\n", asub[k], a[k]);
./zreadtriple.c:168:        fprintf(stderr, "zreadrhs: file does not exist\n");
./zreadtriple_noheader.c:4:approvals from U.S. Dept. of Energy) 
./zreadtriple_noheader.c:23: * <pre>
./zreadtriple_noheader.c:30: * </pre>
./zreadtriple_noheader.c:73:	printf("triplet file: row/col indices are zero-based.\n");
./zreadtriple_noheader.c:75:	printf("triplet file: row/col indices are one-based.\n");
./zreadtriple_noheader.c:89:    printf("m %ld, n %ld, nonz %ld\n", *m, *n, *nonz);
./zreadtriple_noheader.c:120:	    fprintf(stderr, "nz %d, (%d, %d) = %e out of bound, removed\n", 
./zreadtriple_noheader.c:140:    printf("new_nonz after symmetric expansion:\t%d\n", *nonz);
./zreadtriple_noheader.c:174:	printf("Col %d, xa %d\n", i, xa[i]);
./zreadtriple_noheader.c:176:	    printf("%d\t%16.10f\n", asub[k], a[k]);
./zreadtriple_noheader.c:189:        fprintf(stderr, "zreadrhs: file does not exist\n");
./zscatter.c:4:approvals from U.S. Dept. of Energy) 
./zscatter.c:15: * <pre>
./zscatter.c:45:    // printf("hello\n");
./zscatter.c:80:        // printf("segsize %d \n",segsize);
./zscatter.c:82:            /*#pragma _CRI cache_bypass nzval,tempv */
./zscatter.c:87:                // printf("i (src) %d, perm (dest) %d  \n",i,indirect_thread[rel]);
./zscatter.c:91:                printf ("(%d %d, %0.3e, %0.3e, %3e ) ", ljb,
./zscatter.c:95:                //printing triplets (location??, old value, new value ) if none of them is zero
./zscatter.c:98:            // printf("\n");
./zscatter.c:101:            // printf("\n");
./zscatter.c:105:        // printf("%d\n",nzval );
./zscatter.c:154:#pragma omp simd
./zscatter.c:163:#pragma omp simd
./zscatter.c:165:    /* can be precalculated? */
./zscatter.c:173:#pragma ivdep
./zscatter.c:179:#pragma omp simd
./zscatter.c:209:    printf ("A(%d,%d) goes to U block \n", ib, jb);
./zscatter.c:232:        // printf("supersize[%ld] \t:%ld \n",ijb,SuperSize( ijb ) );
./zscatter.c:246:            // printf("========Entering loop=========\n");
./zscatter.c:248:#pragma omp simd
./zscatter.c:252:                // printf("%d %d %d %d %d \n",lptr,i,fnz,temp_nbrow,nbrow );
./zscatter.c:253:                // printf("hello   ucol[%d] %d %d : \n",rel,lsub[lptr + i],fnz);
./zscatter.c:259:                    printf ("(%d, %0.3e, %0.3e ) ", rel, ucol[rel] + tempv[i],
./zscatter.c:261:                //printing triplets (location??, old value, new value ) if none of them is zero
./zscatter.c:266:            // printf("\n");
./zscatter.c:274:    // printf("\n");
./zscatter.c:303:    int* full_u_cols,       /*array containing prefix sum of work load */
./zscatter.c:337:        printf ("full_u_cols[num_blks-1] %d  %d \n",
./zscatter.c:339:        printf ("Early return \n");
./zscatter.c:367:    printf ("Remaining cols %d num_blks %d cpu_blks %d \n", cols_remain,
./zscatter.c:375:        printf ("%d %d  %d %d \n", full_u_cols[num_blks - 1],
./zscatter.c:382:        printf ("cols_per_stream :\t%d\n", cols_per_stream);
./zscatter.c:398:                printf ("i %d, j %d, %d  %d ", i, j, full_u_cols[j + 1],
./zscatter.c:404:                    printf ("cutoff met \n");
./zscatter.c:413:                printf ("\n");
./zscatter.c:430:                   Ublock_info_t *Ublock_info,    /*array containing prefix sum of work load */
./zsp_blas2_dist.c:4:approvals from U.S. Dept. of Energy) 
./zsp_blas2_dist.c:14: * <pre>
./zsp_blas2_dist.c:18: * </pre>
./zsp_blas2_dist.c:30: * Function prototypes 
./zsp_blas2_dist.c:41: * <pre>
./zsp_blas2_dist.c:76: *	       The factor L from the factorization Pr*A*Pc=L*U. Use
./zsp_blas2_dist.c:77: *             compressed row subscripts storage for supernodes,
./zsp_blas2_dist.c:81: *	        The factor U from the factorization Pr*A*Pc=L*U.
./zsp_blas2_dist.c:91: * </pre>
./zsp_blas2_dist.c:110:    int fsupc, nsupr, nsupc, luptr, istart, irow;
./zsp_blas2_dist.c:149:		nsupr = SuperLU_L_SUB_START(fsupc+1) - istart;
./zsp_blas2_dist.c:152:		nrow = nsupr - nsupc;
./zsp_blas2_dist.c:167:		    CTRSV(ftcs1, ftcs2, ftcs3, &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:171:		       	&nsupr, &x[fsupc], &incx, &beta, &work[0], &incy);
./zsp_blas2_dist.c:173:		    ztrsv_("L", "N", "U", &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:177:		       	&nsupr, &x[fsupc], &incx, &beta, &work[0], &incy, 1);
./zsp_blas2_dist.c:180:		    zlsolve ( nsupr, nsupc, &Lval[luptr], &x[fsupc]);
./zsp_blas2_dist.c:182:		    zmatvec ( nsupr, nsupr-nsupc, nsupc, &Lval[luptr+nsupc],
./zsp_blas2_dist.c:203:	    	nsupr = SuperLU_L_SUB_START(fsupc+1) - SuperLU_L_SUB_START(fsupc);
./zsp_blas2_dist.c:219:		    CTRSV(ftcs3, ftcs2, ftcs2, &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:222:		    ztrsv_("U", "N", "N", &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:226:		    zusolve ( nsupr, nsupc, &Lval[luptr], &x[fsupc] );
./zsp_blas2_dist.c:251:	    	nsupr = SuperLU_L_SUB_START(fsupc+1) - istart;
./zsp_blas2_dist.c:255:		solve_ops += 8 * (nsupr - nsupc) * nsupc;
./zsp_blas2_dist.c:275:		    CTRSV(ftcs1, ftcs2, ftcs3, &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:278:		    ztrsv_("L", "T", "U", &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:282:		    ztrsv_("L", "T", "U", &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:293:	    	nsupr = SuperLU_L_SUB_START(fsupc+1) - SuperLU_L_SUB_START(fsupc);
./zsp_blas2_dist.c:316:		    CTRSV( ftcs1, ftcs2, ftcs3, &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:319:		    ztrsv_("U", "T", "N", &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:323:		    ztrsv_("U", "T", "N", &nsupc, &Lval[luptr], &nsupr,
./zsp_blas2_dist.c:340:<pre>
./zsp_blas2_dist.c:394:</pre>
./zsp_blas3_dist.c:4:approvals from U.S. Dept. of Energy) 
./zsp_blas3_dist.c:14: * <pre>
./zsp_blas3_dist.c:18: * </pre>
./zsp_blas3_dist.c:30:<pre>
./zsp_blas3_dist.c:91:    B      - DOUBLE COMPLEX PRECISION array of DIMENSION ( LDB, kb ), where kb is 
./zsp_blas3_dist.c:101:             in the calling (sub) program. LDB must be at least max( 1, n ).  
./zsp_blas3_dist.c:108:    C      - DOUBLE COMPLEX PRECISION array of DIMENSION ( LDC, n ).   
./zsp_blas3_dist.c:117:             in the calling (sub)program. LDC must be at least max(1,m).   
./zsp_blas3_dist.c:121:</pre> 
./zutil_dist.c:4:approvals from U.S. Dept. of Energy) 
./zutil_dist.c:15: * <pre>
./zutil_dist.c:47:zCreate_CompRowLoc_Matrix_dist(SuperMatrix *A, int_t m, int_t n,
./zutil_dist.c:70:/*! \brief Convert a row compressed storage into a column compressed storage.
./zutil_dist.c:73:zCompRow_to_CompCol_dist(int_t m, int_t n, int_t nnz, 
./zutil_dist.c:95:    /* Transfer the matrix into the compressed column storage. */
./zutil_dist.c:131:void zPrint_CompCol_Matrix_dist(SuperMatrix *A)
./zutil_dist.c:137:    printf("\nCompCol matrix: ");
./zutil_dist.c:138:    printf("Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./zutil_dist.c:140:    printf("nrow %lld, ncol %lld, nnz %lld\n", (long long) A->nrow,
./zutil_dist.c:143:        printf("nzval:\n");
./zutil_dist.c:144:        for (i = 0; i < Astore->nnz; ++i) printf("%f\t%f\n", dp[i].r, dp[i].i);
./zutil_dist.c:146:    printf("\nrowind:\n");
./zutil_dist.c:148:        printf("%lld  ", (long long) Astore->rowind[i]);
./zutil_dist.c:149:    printf("\ncolptr:\n");
./zutil_dist.c:151:        printf("%lld  ", (long long) Astore->colptr[i]);
./zutil_dist.c:152:    printf("\nend CompCol matrix.\n");
./zutil_dist.c:155:void zPrint_Dense_Matrix_dist(SuperMatrix *A)
./zutil_dist.c:161:    printf("\nDense matrix: ");
./zutil_dist.c:162:    printf("Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./zutil_dist.c:165:    printf("nrow %lld, ncol %lld, lda %lld\n", 
./zutil_dist.c:167:    printf("\nnzval: ");
./zutil_dist.c:168:    for (i = 0; i < A->nrow; ++i) printf("%f\t%f\n", dp[i].r, dp[i].i);
./zutil_dist.c:169:    printf("\nend Dense matrix.\n");
./zutil_dist.c:172:int zPrint_CompRowLoc_Matrix_dist(SuperMatrix *A)
./zutil_dist.c:178:    printf("\n==== CompRowLoc matrix: ");
./zutil_dist.c:179:    printf("Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./zutil_dist.c:181:    printf("nrow %ld, ncol %ld\n", 
./zutil_dist.c:184:    printf("nnz_loc %ld, m_loc %ld, fst_row %ld\n", (long int) nnz_loc, 
./zutil_dist.c:186:    PrintInt10("rowptr", m_loc+1, Astore->rowptr);
./zutil_dist.c:187:    PrintInt10("colind", nnz_loc, Astore->colind);
./zutil_dist.c:189:        PrintDoublecomplex("nzval", nnz_loc, dp);
./zutil_dist.c:190:    printf("==== end CompRowLoc matrix\n");
./zutil_dist.c:194:int file_zPrint_CompRowLoc_Matrix_dist(FILE *fp, SuperMatrix *A)
./zutil_dist.c:200:    fprintf(fp, "\n==== CompRowLoc matrix: ");
./zutil_dist.c:201:    fprintf(fp, "Stype %d, Dtype %d, Mtype %d\n", A->Stype,A->Dtype,A->Mtype);
./zutil_dist.c:203:    fprintf(fp, "nrow %ld, ncol %ld\n", (long int) A->nrow, (long int) A->ncol);
./zutil_dist.c:205:    fprintf(fp, "nnz_loc %ld, m_loc %ld, fst_row %ld\n", (long int) nnz_loc,
./zutil_dist.c:207:    file_PrintInt10(fp, "rowptr", m_loc+1, Astore->rowptr);
./zutil_dist.c:208:    file_PrintInt10(fp, "colind", nnz_loc, Astore->colind);
./zutil_dist.c:210:        file_PrintDoublecomplex(fp, "nzval", nnz_loc, dp);
./zutil_dist.c:211:    fprintf(fp, "==== end CompRowLoc matrix\n");
./zutil_dist.c:240: * <pre>
./zutil_dist.c:245: * </pre>
./zutil_dist.c:292:void zClone_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B)
./zutil_dist.c:323:void zCopy_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B)
./zutil_dist.c:327:    zClone_CompRowLoc_Matrix_dist(A, B);
./zutil_dist.c:340:void zZero_CompRowLoc_Matrix_dist(SuperMatrix *A)
./zutil_dist.c:357:void zScaleAddId_CompRowLoc_Matrix_dist(SuperMatrix *A, doublecomplex c)
./zutil_dist.c:383:void zScaleAdd_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B, doublecomplex c)
./zutil_dist.c:426:/*! \brief Fills a doublecomplex precision array with a given value.
./zutil_dist.c:458:      printf("\tRHS %2d: ||X-Xtrue||/||X|| = %e\n", j, err);
./zutil_dist.c:462:void PrintDoublecomplex(char *name, int_t len, doublecomplex *x)
./zutil_dist.c:466:    printf("%10s:\tReal\tImag\n", name);
./zutil_dist.c:468:	printf("\t" IFMT "\t%.4f\t%.4f\n", i, x[i].r, x[i].i);
./zutil_dist.c:471:int file_PrintDoublecomplex(FILE *fp, char *name, int_t len, doublecomplex *x)
./zutil_dist.c:475:    fprintf(fp, "%10s:\tReal\tImag\n", name);
./zutil_dist.c:477:	fprintf(fp, "\t" IFMT "\t%.4f\t%.4f\n", i, x[i].r, x[i].i);
./zutil_dist.c:481:/*! \brief Print the blocks in the factored matrix L.
./zutil_dist.c:483:void zPrintLblocks(int iam, int_t nsupers, gridinfo_t *grid,
./zutil_dist.c:486:    register int c, extra, gb, j, lb, nsupc, nsupr, len, nb, ncb;
./zutil_dist.c:492:    printf("\n[%d] L BLOCKS IN COLUMN-MAJOR ORDER -->\n", iam);
./zutil_dist.c:502:	    nsupr = index[1];
./zutil_dist.c:505:	    printf("[%d] block column %d (local # %d), nsupc %d, # row blocks %d\n",
./zutil_dist.c:509:		printf("[%d] row-block %d: block # " IFMT "\tlength %d\n", 
./zutil_dist.c:511:		PrintInt10("lsub", len, &index[k+LB_DESCRIPTOR]);
./zutil_dist.c:513:		    PrintDoublecomplex("nzval", len, &nzval[r + j*nsupr]);
./zutil_dist.c:519:	printf("(%d)", iam);
./zutil_dist.c:520: 	PrintInt32("ToSendR[]", grid->npcol, Llu->ToSendR[lb]);
./zutil_dist.c:521:	PrintInt10("fsendx_plist[]", grid->nprow, Llu->fsendx_plist[lb]);
./zutil_dist.c:523:    printf("nfrecvx " IFMT "\n", Llu->nfrecvx);
./zutil_dist.c:524:    k = CEILING( nsupers, grid->nprow );
./zutil_dist.c:525:    PrintInt10("fmod", k, Llu->fmod);
./zutil_dist.c:527:} /* ZPRINTLBLOCKS */
./zutil_dist.c:536:    register int c, extra, gb, j, i, lb, nsupc, nsupr, len, nb, ncb;
./zutil_dist.c:545:	// assert(grid->npcol*grid->nprow==1);
./zutil_dist.c:559:	    nsupr = index[1];
./zutil_dist.c:584:	snprintf(filename, sizeof(filename), "%s-%d", "L", iam);    
./zutil_dist.c:585:    printf("Dumping L factor to --> %s\n", filename);
./zutil_dist.c:591:		fprintf(fp, "%d %d %d\n", n,n,nnzL);
./zutil_dist.c:603:	    nsupr = index[1];
./zutil_dist.c:611:			fprintf(fp, IFMT IFMT " %e\n", index[k+LB_DESCRIPTOR+i]+1, xsup[gb]+j+1, (double)iam);
./zutil_dist.c:613:			fprintf(fp, IFMT IFMT " %e %e\n", index[k+LB_DESCRIPTOR+i]+1, xsup[gb]+j+1, nzval[r +i+ j*nsupr].r,nzval[r +i+ j*nsupr].i);
./zutil_dist.c:628:/*! \brief Print the blocks in the factored matrix U.
./zutil_dist.c:630:void zPrintUblocks(int iam, int_t nsupers, gridinfo_t *grid, 
./zutil_dist.c:639:    printf("\n[%d] U BLOCKS IN ROW-MAJOR ORDER -->\n", iam);
./zutil_dist.c:640:    nrb = nsupers / grid->nprow;
./zutil_dist.c:641:    extra = nsupers % grid->nprow;
./zutil_dist.c:649:	    printf("[%d] block row " IFMT " (local # %d), # column blocks %d\n",
./zutil_dist.c:650:		   iam, lb*grid->nprow+myrow, lb, nb);
./zutil_dist.c:655:		printf("[%d] col-block %d: block # %d\tlength " IFMT "\n", 
./zutil_dist.c:658:		PrintInt10("fstnz", nsupc, &index[k+UB_DESCRIPTOR]);
./zutil_dist.c:659:		PrintDoublecomplex("nzval", len, &nzval[r]);
./zutil_dist.c:664:	    printf("[%d] ToSendD[] %d\n", iam, Llu->ToSendD[lb]);
./zutil_dist.c:667:} /* ZPRINTUBLOCKS */
./zutil_dist.c:670:zprint_gsmv_comm(FILE *fp, int_t m_loc, pzgsmv_comm_t *gsmv_comm,
./zutil_dist.c:673:  int_t procs = grid->nprow*grid->npcol;
./zutil_dist.c:674:  fprintf(fp, "TotalIndSend " IFMT "\tTotalValSend " IFMT "\n", gsmv_comm->TotalIndSend,
./zutil_dist.c:676:  file_PrintInt10(fp, "extern_start", m_loc, gsmv_comm->extern_start);
./zutil_dist.c:677:  file_PrintInt10(fp, "ind_tosend", gsmv_comm->TotalIndSend, gsmv_comm->ind_tosend);
./zutil_dist.c:678:  file_PrintInt10(fp, "ind_torecv", gsmv_comm->TotalValSend, gsmv_comm->ind_torecv);
./zutil_dist.c:679:  file_PrintInt10(fp, "ptr_ind_tosend", procs+1, gsmv_comm->ptr_ind_tosend);
./zutil_dist.c:680:  file_PrintInt10(fp, "ptr_ind_torecv", procs+1, gsmv_comm->ptr_ind_torecv);
./zutil_dist.c:681:  file_PrintInt32(fp, "SendCounts", procs, gsmv_comm->SendCounts);
./zutil_dist.c:682:  file_PrintInt32(fp, "RecvCounts", procs, gsmv_comm->RecvCounts);
